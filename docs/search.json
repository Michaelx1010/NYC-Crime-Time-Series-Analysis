[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "about.html#contact-info",
    "href": "about.html#contact-info",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Michael",
    "section": "Background",
    "text": "Background\n\nHello, I’m Xinzhe Xie, and I prefer to be called Michael. Originally from Chongqing, China, I graduated in 2021 with a degree in Statistics and Actuarial Science from the University of Massachusetts Amherst. After graduation, I embarked on a career as a data analyst in China, where my work primarily focused on developing river water quality forecast models.\nIn my role, I applied mechanical modeling methods using R, built interactive data visualizations using R Shiny, and deployed applications using Shiny Proxy and Docker. I also had the opportunity to work with neural networks and SARIMA time series modeling. My professional interests lie in the fields of machine learning and time series modeling, particularly within the realms of environmental and healthcare applications.\nBeyond my professional pursuits, I have a strong passion for weightlifting, specifically weighted calisthenics and powerlifting. I also have a background as a competitive swimmer, which has contributed to my discipline and determination."
  },
  {
    "objectID": "about.html#academic-journey",
    "href": "about.html#academic-journey",
    "title": "About Michael",
    "section": "Academic Journey",
    "text": "Academic Journey\n\nMy academic journey has been marked by a deep fascination with time series analysis and machine learning. During my previous job, I gained extensive experience in handling large time series datasets. I applied various models to these datasets to discern underlying trends and patterns. This experience motivated me to enroll in DSAN 5600 this semester, where I hope to further develop my expertise in time series analysis.\nI’m particularly enthusiastic about machine learning, especially deep learning methods such as neural network modeling. These methods have broad applications, including forecasting in domains like stocks, healthcare, and weather. I believe that incorporating deep learning techniques into time series modeling can significantly enhance predictive accuracy."
  },
  {
    "objectID": "about.html#professional-interest",
    "href": "about.html#professional-interest",
    "title": "About Michael",
    "section": "Professional interest",
    "text": "Professional interest\nMy professional interests encompass the following areas:\n\nAdvanced programming in R and Python\nTime series forecasting models, including ARIMA, SARIMA, Bayesian structural time series, and more\nDeep learning methods, particularly neural networks, for forecasting in diverse fields such as stocks, healthcare, and weather\nSoftware development, including building and deploying applications\n\n\nEducation\n\nMaster of Science(MS) in Data Science and Analytics, Georgetown University(2025)\nBachelor of Science (BS) in Mathematics, University of Massachusetts Amherst (2021)"
  },
  {
    "objectID": "Introduction.html#time-series-analysis-and-forecasting-of-violent-crime-data-in-new-york",
    "href": "Introduction.html#time-series-analysis-and-forecasting-of-violent-crime-data-in-new-york",
    "title": "Introduction",
    "section": "Time series analysis and forecasting of violent crime data in New York",
    "text": "Time series analysis and forecasting of violent crime data in New York\n\nUnderstanding Crime Trends in NYC:\n\nSince the outbreak of Covid-19 in 2021, violent crime rates in New York City (NYC) have exhibited significant fluctuations. These criminal activities pose grave threats to the safety and well-being of NYC’s residents and communities. Notably, the city has witnessed changes in various aspects of violent crime, with some categories experiencing increases while others have decreased during the pandemic. For example, the number of murders and other serious violent crimes has raised concerns, while there has also been a decline in certain minor offenses. \nAnother perspective of crime activities during Covid-19 pandemic era is that according to this article Boman and Gallupe (2020), the study has found that crime decreased compared with 2019. But this decrease in crime rate was mostly contributed to minor crimes that happened when people aren’t staying home. More serious crimes like murders, has stayed the same and even got worse."
  },
  {
    "objectID": "Introduction.html#project-overview",
    "href": "Introduction.html#project-overview",
    "title": "Introduction",
    "section": "Project overview",
    "text": "Project overview\n\nViolent crime numbers in New York City is not a static measure and can vary over time. This project aims to provide an in-depth analysis of crime trends in NYC, utilizing statistical learning method including ARIMA/SARIMA models, ARIMAX/SARIMAX models, VAR models, and deep learning modelings incluing RNN models, GRU models, and LSTM models for modeling and forecasting, and at the same time, focusing specifically on the following aspects:\nIn this project, I will focus on different sides and perspectives listed below:\n\nGathering data on different categories of violent criminal activities, such as murder, rape, robbery, aggravated assault.\nCollecting time-series data on COVID-19 infection rates to examine how the pandemic has influenced violent crime rates over time in NYC.\nIncorporating relevant socioeconomic data to investigate the relationship between violent crime rates and societal factors, I will focus primarily on unemployment rates data in NYC\nAnalyzing geographical data with geographical data visualizaton, including violent crime rates across different neighborhoods, boroughs, and regions of NYC, to uncover patterns and correlations.\nConsidering weather and climate data to explore how different weather conditions may correlate with violent crime rate fluctuations over time.\nExamining historical violent crime data specific to New York City in the 21st century to understand the evolution of crime rates and improvements in data collection methods."
  },
  {
    "objectID": "Introduction.html#big-picture",
    "href": "Introduction.html#big-picture",
    "title": "Introduction",
    "section": "Big picture",
    "text": "Big picture"
  },
  {
    "objectID": "Introduction.html#important-questions-to-answer",
    "href": "Introduction.html#important-questions-to-answer",
    "title": "Introduction",
    "section": "Important questions to answer",
    "text": "Important questions to answer\n1. How have violent crimes in New York City evolved over the past decade?\n2. What are the variations in crime rates among different categories of violent criminal activities in NYC over time?\n3. How has the COVID-19 pandemic cases influenced violent crime rates in New York City? Are there specific trends related to the pandemic’s impact on different types of crimes?\n4. How has the unemployment rates in NYC influenced violent crime rates in New York City? Are there specific trends related to the unemployment’s rates impact on different types of crimes?\n5. How has the weather in NYC influenced violent crime rates in New York City? Are there specific trends related to the weather’s impact on different types of crimes?\n6. What are the differences in predictive performance between traditional time-series models and advanced techniques like deep learning modeling when forecasting violent crime trends in NYC?\n7. What insights can be derived from the forecasted models? How quickly do these models respond to changes in violent crime rates, and how far into the future can they predict?\n8. What are the key takeaways from this project in terms of understanding the factors contributing to crime rates in NYC, and how can this knowledge be used to mitigate crime and enhance public safety in the city?"
  },
  {
    "objectID": "Exploratory_Data_Analysis.html",
    "href": "Exploratory_Data_Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\n\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ngglagplot(v_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"Lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme_minimal()\n\n\n\n\n\n\nBased on the lag plot, there are obvious strong linear relationships at lag 7 and lag 14, this indicates that the data might have weekly seasonality.\n\nTry some weekly lags:\n\n\nCode\ngglagplot(v_ts, do.lines=FALSE, set.lags = c(7, 14, 21)) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"Weekly lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme_minimal()+\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\nThose weekly lag plots have proved my assumption that the data has weekly seasonality.\n\nTry longer lags with month:\n\n\nCode\n#Try some longer lags with month\ngglagplot(v_ts, do.lines=FALSE, set.lags = c(30, 60)) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"monthly lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme_minimal()+\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\nThis suggest that the data might not have a monthly seasonality"
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#lag-plots-for-total-violent-crimes",
    "href": "Exploratory_Data_Analysis.html#lag-plots-for-total-violent-crimes",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\n\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ngglagplot(v_ts, do.lines=FALSE) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"Lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme_minimal()\n\n\n\n\n\n\nBased on the lag plot, there are obvious strong linear relationships at lag 7 and lag 14, this indicates that the data might have weekly seasonality.\n\nTry some weekly lags:\n\n\nCode\ngglagplot(v_ts, do.lines=FALSE, set.lags = c(7, 14, 21)) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"Weekly lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme_minimal()+\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\nThose weekly lag plots have proved my assumption that the data has weekly seasonality.\n\nTry longer lags with month:\n\n\nCode\n#Try some longer lags with month\ngglagplot(v_ts, do.lines=FALSE, set.lags = c(30, 60)) +xlab(\"Lags\")+ylab(\"Yt\")+ggtitle(\"monthly lag Plot for violent crime time series from 2007 to 2022 in NYC \") +\n  theme_minimal()+\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n\n\n\n\n\n\nThis suggest that the data might not have a monthly seasonality"
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#decomposition",
    "href": "Exploratory_Data_Analysis.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\n\nDecompose data with both additive and multiplicative method\n\n\nDaily data\nAdditive:\n\n\nCode\n#additive\nautoplot(decompose(v_ts, \"additive\"))\n\n\n\n\n\nmultiplicative:\n\n\nCode\n#multiplicative\nautoplot(decompose(v_ts, \"multiplicative\"))\n\n\n\n\n\n\n\nWeekly data\n\n\nCode\n# Aggragate data to weekly\ndata_w &lt;- data_t %&gt;%\n  mutate(week = floor_date(as.Date(date_single), unit = \"week\")) %&gt;%\n  group_by(week, offense_type) %&gt;%\n  summarise(Total_Crimes = sum(Total_Crimes)) %&gt;%\n  ungroup()\n\n#write.csv(data_w, file = \"dataset/crimedata_week.csv\", row.names = FALSE)\n\n\nAdditive:\n\n\nCode\n#additive data\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\nautoplot(decompose(w_ts, \"additive\"))\n\n\n\n\n\nmultiplicative:\n\n\nCode\n#Multiplicative data\nautoplot(decompose(w_ts, \"multiplicative\"))\n\n\n\n\n\n\n\nInterpretation\n\nThe weekly data decomposition helps view the seasonality in a better way.\nBoth additive and multiplicative decomposition gives similar results, so it might indicate that the data does not exhibit strong multiplicative seasonality, and the seasonal component can be effectively captured using an additive approach.\nWe could see clear increasing trend from our decomposition, very clear yearly seasonality where crimes gradually increases until reaches peaks in May and June, and decrease to the lowest point at around December and January."
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#acf-and-pacf-plots",
    "href": "Exploratory_Data_Analysis.html#acf-and-pacf-plots",
    "title": "Exploratory Data Analysis",
    "section": "ACF and PACF plots",
    "text": "ACF and PACF plots\n\nDaily data\n\n\nCode\nggAcf(v_ts, 60)+ggtitle(\"ACF Plot for violent assault daily data in NYC\")\n\n\n\n\n\n\n\nCode\nggPacf(v_ts,60, main=\"ACF Plot for violent assault daily data in NYC\")\n\n\n\n\n\n\nAs we can see from this ACF and PACF plots of the data, we see strong correlations between the data and its lagged data, also, this data is definitely not stationary since we can clearly see yearly and weekly seasonality, we need to perform difference and seasonal difference to this data.\n\n\n\nWeekly data\n\n\nCode\nggAcf(w_ts)+ggtitle(\"ACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\n\n\nCode\nggPacf(w_ts)+ggtitle(\"PACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\n\nWe can see pretty high correlation with weekly data, also, quarterly correlations are also visible in the ACF plot."
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#stationarity",
    "href": "Exploratory_Data_Analysis.html#stationarity",
    "title": "Exploratory Data Analysis",
    "section": "Stationarity",
    "text": "Stationarity\n\nDaily data\nUse the Augmented Dickey-Fuller Test on data, first use it on the daily data\n\n\nCode\ntseries::adf.test(v_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  v_ts\nDickey-Fuller = -6.5887, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nBased on the test, p-value is smaller than 0.05, and we reject the null hypothesis, which shows that this data is stationary.\n\n\n\nWeekly data\n\n\nCode\ntseries::adf.test(w_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts\nDickey-Fuller = -6.0495, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nBased on the test, the p-value is greater than 0.05, suggests that this data is not stationary.\nThe result could imply that the Augmented Dickey-Fuller test is not reliable test for stationarity."
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#data-differencing",
    "href": "Exploratory_Data_Analysis.html#data-differencing",
    "title": "Exploratory Data Analysis",
    "section": "Data differencing",
    "text": "Data differencing\n\nDaily data\nFirst order differencing\n\n\nCode\nv_ts %&gt;% diff() %&gt;% ggAcf(50)\n\n\n\n\n\nCode\nv_ts %&gt;% diff() %&gt;% ggPacf(50)\n\n\n\n\n\n\nwe can see that the daily data still has seasonality after the first order differencing, need to apply seasonal difference.\n\nSeasonal differencing\n\n\nCode\nv_ts %&gt;% diff(lag=7) %&gt;% ggAcf(50)\n\n\n\n\n\nCode\nv_ts %&gt;% diff(lag=7) %&gt;% ggPacf(50)\n\n\n\n\n\n\nThis looks much more stationary, still can see some seasonality. Need to apply both difference and seasonal difference.\n\nBoth first order and seasonal differencing\n\n\nCode\nv_ts %&gt;% diff(lag=7) %&gt;% diff() %&gt;% ggAcf(50)\n\n\n\n\n\nCode\nv_ts %&gt;% diff(lag=7) %&gt;% diff() %&gt;% ggPacf(50)\n\n\n\n\n\n\n\nWeekly data\n\n\nCode\nw_ts %&gt;% diff() %&gt;% ggAcf(50)\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% ggPacf(50)\n\n\n\n\n\n\nFor the weekly data, only first order differencing is enough to make the data stationary.\n\n\n\nUsing ADF test to check for stationarity\n\nFor the daily data\n\n\n\nCode\nv_ts_test &lt;- v_ts %&gt;% diff(lag=7) %&gt;% diff()\ntseries::adf.test(v_ts_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  v_ts_test\nDickey-Fuller = -35.228, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nFor the weekly data\n\n\n\nCode\nw_ts_test &lt;- w_ts %&gt;% diff()\ntseries::adf.test(w_ts_test)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_test\nDickey-Fuller = -8.2691, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nBased on the results of both tests, both p-value is smaller than 0.05, this suggests that both differenced data is stationary."
  },
  {
    "objectID": "Exploratory_Data_Analysis.html#moving-average-smoothing",
    "href": "Exploratory_Data_Analysis.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving average smoothing",
    "text": "Moving average smoothing\n\nDaily data\n\n7-MA\n\n\n\nCode\nautoplot(v_ts, series=\"Data\") +\n  autolayer(ma(v_ts, 7), series=\"7-MA\") +\n  xlab(\"Year\") + ylab(\"Number of crimes\") +\n  ggtitle(\"Violent crime time series from 2007 to 2022 using SMA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"7-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"7-MA\"))\n\n\n\n\n\n\n14-MA\n\n\n\nCode\nautoplot(v_ts, series=\"Data\") +\n  autolayer(ma(v_ts, 14), series=\"14-MA\") +\n  xlab(\"Year\") + ylab(\"Number of crimes\") +\n  ggtitle(\"Violent crime time series from 2007 to 2023 using SMA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"14-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"14-MA\"))\n\n\n\n\n\n\n21-MA\n\n\n\nCode\nautoplot(v_ts, series=\"Data\") +\n  autolayer(ma(v_ts, 21), series=\"21-MA\") +\n  xlab(\"Year\") + ylab(\"Number of crimes\") +\n  ggtitle(\"Violent crime crime time series from 2007 to 2023 using SMA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"21-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"21-MA\"))\n\n\n\n\n\n\nFor the daily data, I chose the moving average window in weekly increments(7, 14, 21), as the window gets larger, the data seems to get more smoothed out, which suggest a process of removing the seasonality to reveal the trend, now we can see clearly that the data have stable trend.\n\n\n\nWeekly data\n\n13-MA\n\n\n\nCode\nautoplot(w_ts, series=\"Data\") +\n  autolayer(ma(w_ts, 13), series=\"13-MA\") +\n  xlab(\"Year\") + ylab(\"Number of crimes\") +\n  ggtitle(\"Violent crime time series from 2007 to 2022 using SMA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"13-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"13-MA\"))\n\n\n\n\n\n\n26-MA\n\n\n\nCode\nautoplot(w_ts, series=\"Data\") +\n  autolayer(ma(w_ts, 26), series=\"26-MA\") +\n  xlab(\"Year\") + ylab(\"Number of crimes\") +\n  ggtitle(\"Violent crime time series from 2007 to 2022 using SMA\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey\",\"26-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"26-MA\"))\n\n\n\n\n\n\nFor the weekly data, I chose moving average windows of quarterly increments(13, 26), We can see that as the windows gets larger, the time series data gets more smoothed out as well, this suggests that the data might have quarterly seasonality."
  },
  {
    "objectID": "Deep_Learning_for_TS.html",
    "href": "Deep_Learning_for_TS.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Traditional statistical methods, such as ARIMA and SARIMA models, have been extensively used for time series forecasting. However, the complex and often nonlinear nature of crime data makes it challenging for these traditional models to capture the underlying patterns accurately. This is where deep learning models come into play, offering a robust alternative with their ability to learn complex patterns from large datasets. In this study, we focus on employing deep learning techniques to analyze and predict violent crime time series data in NYC. Specifically, we explore three different types of neural network architectures: Recurrent Neural Networks (RNN), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM) networks.\nRecurrent Neural Networks (RNN): RNNs are a class of neural networks that are particularly well-suited for sequential data, like time series. They can capture temporal dynamics, which is essential for understanding how crime trends evolve over time.\nGated Recurrent Units (GRU): GRUs are a variation of RNNs that aim to solve the vanishing gradient problem often encountered in traditional RNNs. They are designed to retain information over extended periods, making them suitable for analyzing time series data where long-term dependencies are crucial.\nLong Short-Term Memory (LSTM): LSTMs are another advanced variant of RNNs, known for their effectiveness in learning long-term dependencies. They are particularly adept at handling the complexity and variability inherent in crime data, making them a promising tool for predicting violent crime trends.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\nfrom math import sqrt\n\n\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#introduction",
    "href": "Deep_Learning_for_TS.html#introduction",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Traditional statistical methods, such as ARIMA and SARIMA models, have been extensively used for time series forecasting. However, the complex and often nonlinear nature of crime data makes it challenging for these traditional models to capture the underlying patterns accurately. This is where deep learning models come into play, offering a robust alternative with their ability to learn complex patterns from large datasets. In this study, we focus on employing deep learning techniques to analyze and predict violent crime time series data in NYC. Specifically, we explore three different types of neural network architectures: Recurrent Neural Networks (RNN), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM) networks.\nRecurrent Neural Networks (RNN): RNNs are a class of neural networks that are particularly well-suited for sequential data, like time series. They can capture temporal dynamics, which is essential for understanding how crime trends evolve over time.\nGated Recurrent Units (GRU): GRUs are a variation of RNNs that aim to solve the vanishing gradient problem often encountered in traditional RNNs. They are designed to retain information over extended periods, making them suitable for analyzing time series data where long-term dependencies are crucial.\nLong Short-Term Memory (LSTM): LSTMs are another advanced variant of RNNs, known for their effectiveness in learning long-term dependencies. They are particularly adept at handling the complexity and variability inherent in crime data, making them a promising tool for predicting violent crime trends.\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\nfrom math import sqrt\n\n\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#data-preparation",
    "href": "Deep_Learning_for_TS.html#data-preparation",
    "title": "Deep Learning for TS",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor the deep learning model, we will be using the total violent crimes data we used in ARIMA/SARIMA model section.\n\nVisualization of raw data\n\n\nCode\ndf = pd.read_csv(\"./dataset/crimedata_week.csv\")\ndf.drop('offense_type', axis = 1)\nprint(df.head())\n\ndf = df.rename(columns={\"week\": \"t\", \"Total_Crimes\": \"y\"})\ndf = df[[\"t\",\"y\"]]\n\nt=np.array([*range(0,df.shape[0])])\nx=np.array(df['y']).reshape(t.shape[0],1)\nfeature_columns=[0] # columns to use as features\ntarget_columns=[0]  # columns to use as targets\n\n\nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o',alpha = 0.5)\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n\n\n         week offense_type  Total_Crimes\n0  2006-12-31        total           692\n1  2007-01-07        total           704\n2  2007-01-14        total           692\n3  2007-01-21        total           614\n4  2007-01-28        total           658\n\n\n\n\n\n\n\nVisualization of normalized data\n\n\nCode\n# Normalize the data\nx=(x-np.mean(x,axis=0))/np.std(x,axis=0)\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(t, x[:,i],'o')\n    ax.plot(t, x[:,i],\"-\")\nax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \nplt.show()\n\n\n\n\n\n\n\nSplit data into train/test sets\n\n\nCode\nsplit_fraction=0.8\ncut=int(split_fraction*x.shape[0]) \ntt=t[0:cut]; xt=x[0:cut]\ntv=t[cut:]; xv=x[cut:]\n\n# visualize normalized data \nfig, ax = plt.subplots()\nfor i in range(0,x.shape[1]):\n    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n    ax.plot(tt, xt[:,i],\"g-\")\nfor i in range(0,x.shape[1]):\n    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n    ax.plot(tv, xv[:,i],\"g-\")\nplt.show()\n\n\n\n\n\n\n\nForm mini-batches for training/testing data\n\n\nCode\n# Time series mini-batch function\ndef form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n    # verbose=True --&gt; report and plot for debugging\n    # unique=True --&gt; don't re-sample: \n    # x1,x2,x3 --&gt; x4 then x4,x5,x6 --&gt; x7 instead of x2,x3,x4 --&gt; x5\n\n    # initialize \n    i_start=0; count=0; \n    \n    # initialize output arrays with samples \n    x_out=[]\n    y_out=[]\n    \n    # sequentially build mini-batch samples\n    while i_start+lookback+delay&lt; x.shape[0]:\n        \n        # define index bounds\n        i_stop=i_start+lookback\n        i_pred=i_stop+delay\n        \n        # report if desired \n        if verbose and count&lt;2: print(\"indice range:\",i_start,i_stop,\"--&gt;\",i_pred)\n\n        # define arrays: \n        # method-1: buggy due to indexing from left \n        # numpy's slicing --&gt; start:stop:step\n        # xtmp=x[i_start:i_stop+1:steps]\n        \n        # method-2: non-vectorized but cleaner\n        indices_to_keep=[]; j=i_stop\n        while  j&gt;=i_start:\n            indices_to_keep.append(j)\n            j=j-step\n\n        # create mini-batch sample\n        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n        xtmp=xtmp[:,feature_columns] # isolate desire features\n        ytmp=x[i_pred,target_columns]\n        x_out.append(xtmp); y_out.append(ytmp); \n        \n        # report if desired \n        if verbose and count&lt;2: print(xtmp, \"--&gt;\",ytmp)\n        if verbose and count&lt;2: print(\"shape:\",xtmp.shape, \"--&gt;\",ytmp.shape)\n\n        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n        if verbose and count&lt;2:\n            fig, ax = plt.subplots()\n            ax.plot(x,'b-')\n            ax.plot(x,'bx')\n            ax.plot(indices_to_keep,xtmp,'go')\n            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n            plt.show()\n            \n        # UPDATE START POINT \n        if unique: i_start+=lookback \n        i_start+=1; count+=1\n        \n    return np.array(x_out),np.array(y_out)\n\n# training\nL=25; S=1; D=1\nXt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\n# validation\nXv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n\n\nindice range: 0 25 --&gt; 26\n[[ 0.60089283]\n [ 0.83342596]\n [ 0.84399565]\n [ 0.23095375]\n [ 1.55216473]\n [ 0.47405657]\n [ 0.07240844]\n [ 0.24152344]\n [ 0.4529172 ]\n [ 0.16753563]\n [-0.38208814]\n [-1.43905692]\n [-1.32279036]\n [ 0.04069937]\n [-0.7625969 ]\n [-1.17481473]\n [-1.53418411]\n [-1.1113966 ]\n [-1.61874162]\n [-1.1113966 ]\n [-2.13665632]\n [-0.86829378]\n [-1.33336004]\n [-0.50892439]\n [-0.38208814]\n [-0.50892439]] --&gt; [1.35134066]\nshape: (26, 1) --&gt; (1,)\nindice range: 1 26 --&gt; 27\n[[ 1.35134066]\n [ 0.60089283]\n [ 0.83342596]\n [ 0.84399565]\n [ 0.23095375]\n [ 1.55216473]\n [ 0.47405657]\n [ 0.07240844]\n [ 0.24152344]\n [ 0.4529172 ]\n [ 0.16753563]\n [-0.38208814]\n [-1.43905692]\n [-1.32279036]\n [ 0.04069937]\n [-0.7625969 ]\n [-1.17481473]\n [-1.53418411]\n [-1.1113966 ]\n [-1.61874162]\n [-1.1113966 ]\n [-2.13665632]\n [-0.86829378]\n [-1.33336004]\n [-0.50892439]\n [-0.38208814]] --&gt; [0.23095375]\nshape: (26, 1) --&gt; (1,)\nindice range: 0 25 --&gt; 26\n[[ 0.86513502]\n [ 0.23095375]\n [-0.49835471]\n [ 0.23095375]\n [-0.31867001]\n [-1.65045068]\n [-1.51304474]\n [-2.54887414]\n [-2.39032883]\n [-2.34805008]\n [-2.93995259]\n [-2.77083759]\n [-2.88710415]\n [-2.65457102]\n [-1.39677817]\n [-0.18126407]\n [-0.46664564]\n [-0.6780394 ]\n [-0.86829378]\n [-0.62519096]\n [-0.13898532]\n [-0.14955501]\n [-0.43493658]\n [-0.77316659]\n [-0.81544534]\n [ 0.09354781]] --&gt; [0.40006876]\nshape: (26, 1) --&gt; (1,)\nindice range: 1 26 --&gt; 27\n[[ 0.40006876]\n [ 0.86513502]\n [ 0.23095375]\n [-0.49835471]\n [ 0.23095375]\n [-0.31867001]\n [-1.65045068]\n [-1.51304474]\n [-2.54887414]\n [-2.39032883]\n [-2.34805008]\n [-2.93995259]\n [-2.77083759]\n [-2.88710415]\n [-2.65457102]\n [-1.39677817]\n [-0.18126407]\n [-0.46664564]\n [-0.6780394 ]\n [-0.86829378]\n [-0.62519096]\n [-0.13898532]\n [-0.14955501]\n [-0.43493658]\n [-0.77316659]\n [-0.81544534]] --&gt; [0.9708319]\nshape: (26, 1) --&gt; (1,)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefine the plot function and utility function for accuracy metrics\n\n\nCode\ndef history_plot(history):\n    FS=18   #FONT SIZE\n    # PLOTTING THE TRAINING AND VALIDATION LOSS \n    history_dict = history.history\n    loss_values = history_dict[\"loss\"]\n    val_loss_values = history_dict[\"val_loss\"]\n    epochs = range(1, len(loss_values) + 1)\n    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n# UTILITY FUNCTION\ndef regression_report(yt,ytp,yv,yvp):\n    print(\"---------- Regression report ----------\")\n    \n    print(\"TRAINING:\")\n    print(\" MSE:\",mean_squared_error(yt,ytp))\n    print(\" MAE:\",mean_absolute_error(yt,ytp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT\n    fig, ax = plt.subplots()\n    ax.plot(yt,ytp,'ro')\n    ax.plot(yt,yt,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Training data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    frac_plot=1.0\n    upper=int(frac_plot*yt.shape[0]); \n    # print(int(0.5*yt.shape[0]))\n    fig, ax = plt.subplots()\n    ax.plot(yt[0:upper],'b-')\n    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n    plt.show()\n\n      \n    print(\"VALIDATION:\")\n    print(\" MSE:\",mean_squared_error(yv,yvp))\n    print(\" MAE:\",mean_absolute_error(yv,yvp))\n    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n    \n    # PARITY PLOT \n    fig, ax = plt.subplots()\n    ax.plot(yv,yvp,'ro')\n    ax.plot(yv,yv,'b-')\n    ax.set(xlabel='y_data', ylabel='y_predicted',\n        title='Validation data parity plot (line y=x represents a perfect fit)')\n    plt.show()\n    \n    # PLOT PART OF THE PREDICTED TIME-SERIES\n    upper=int(frac_plot*yv.shape[0])\n    fig, ax = plt.subplots()\n    ax.plot(yv[0:upper],'b-')\n    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n    plt.show()"
  },
  {
    "objectID": "Deep_Learning_for_TS.html#model-training",
    "href": "Deep_Learning_for_TS.html#model-training",
    "title": "Deep Learning for TS",
    "section": "Model Training",
    "text": "Model Training\n\nRNNGRUSLSTM\n\n\n\nWithout regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nRNN_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/19 [&gt;.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/19 [================&gt;.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 1s 5ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 14ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3221943405114027\n MAE: 0.4601097235394949\nVALIDATION:\n MSE: 0.9409870871953195\n MAE: 0.7678807567867144\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \n\nRNN_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/19 [======================&gt;.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 5ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.27274462523568493\n MAE: 0.42877798416638085\nVALIDATION:\n MSE: 0.9831253170700438\n MAE: 0.7687598691322759\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, RNN model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization.\n\n\n\n\n\nWithout regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nGRU_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.34853514589929246\n MAE: 0.4877344783026034\nVALIDATION:\n MSE: 0.8267418483520799\n MAE: 0.7569208587765953\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \nGRU_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/19 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/19 [===========================&gt;..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.38931004350898185\n MAE: 0.503548982223264\nVALIDATION:\n MSE: 0.800338450125644\n MAE: 0.7290572510808782\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, GRU model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization.\n\n\n\n\n\nWithout regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nLSTM_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3129530333588253\n MAE: 0.4556539071981138\nVALIDATION:\n MSE: 0.7866643314474138\n MAE: 0.7063246306956839\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \nLSTM_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_5 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3051213235449832\n MAE: 0.45503845089254413\nVALIDATION:\n MSE: 0.6233462286768205\n MAE: 0.6498043169338328\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, LSTM model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#without-regularization",
    "href": "Deep_Learning_for_TS.html#without-regularization",
    "title": "Deep Learning for TS",
    "section": "Without regularization",
    "text": "Without regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nRNN_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 32)                1088      \n                                                                 \n dense (Dense)               (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nWARNING:tensorflow:From C:\\Users\\22426\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/19 [&gt;.............................] - ETA: 9s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/19 [================&gt;.............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 1s 5ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 14ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3221943405114027\n MAE: 0.4601097235394949\nVALIDATION:\n MSE: 0.9409870871953195\n MAE: 0.7678807567867144"
  },
  {
    "objectID": "Deep_Learning_for_TS.html#with-regularization",
    "href": "Deep_Learning_for_TS.html#with-regularization",
    "title": "Deep Learning for TS",
    "section": "With regularization",
    "text": "With regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\n#model.add(GRU(\nmodel.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \n\nRNN_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n                                                                 \n dense_1 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1121 (4.38 KB)\nTrainable params: 1121 (4.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/19 [======================&gt;.......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 5ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 4ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.27274462523568493\n MAE: 0.42877798416638085\nVALIDATION:\n MSE: 0.9831253170700438\n MAE: 0.7687598691322759\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, RNN model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#without-regularization-1",
    "href": "Deep_Learning_for_TS.html#without-regularization-1",
    "title": "Deep Learning for TS",
    "section": "Without regularization",
    "text": "Without regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nGRU_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 32)                3360      \n                                                                 \n dense_2 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 9ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.34853514589929246\n MAE: 0.4877344783026034\nVALIDATION:\n MSE: 0.8267418483520799\n MAE: 0.7569208587765953"
  },
  {
    "objectID": "Deep_Learning_for_TS.html#with-regularization-1",
    "href": "Deep_Learning_for_TS.html#with-regularization-1",
    "title": "Deep Learning for TS",
    "section": "With regularization",
    "text": "With regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n#model.add(LSTM(\nmodel.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \nGRU_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 32)                3360      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 3393 (13.25 KB)\nTrainable params: 3393 (13.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/19 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/19 [===========================&gt;..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 6ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.38931004350898185\n MAE: 0.503548982223264\nVALIDATION:\n MSE: 0.800338450125644\n MAE: 0.7290572510808782\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, GRU model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#without-regularization-2",
    "href": "Deep_Learning_for_TS.html#without-regularization-2",
    "title": "Deep Learning for TS",
    "section": "Without regularization",
    "text": "Without regularization\n\n\nCode\n# RESHAPE INTO A DATA FRAME \nXt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\nXv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp)  \nLSTM_e = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 32)                4352      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 10ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3129530333588253\n MAE: 0.4556539071981138\nVALIDATION:\n MSE: 0.7866643314474138\n MAE: 0.7063246306956839"
  },
  {
    "objectID": "Deep_Learning_for_TS.html#with-regularization-2",
    "href": "Deep_Learning_for_TS.html#with-regularization-2",
    "title": "Deep Learning for TS",
    "section": "With regularization",
    "text": "With regularization\n\n\nCode\n# # HYPERPARAMETERS \noptimizer=\"rmsprop\"\nloss_function=\"MeanSquaredError\" \nlearning_rate=0.001\nnumbers_epochs=200 #100\nL2=0 #1e-4\ninput_shape=(Xt.shape[1],Xt.shape[2])\n\n# # batch_size=1                       # stocastic training\n# # batch_size=int(len(x_train)/2.)    # mini-batch training\nbatch_size=len(Xt1)              # batch training\n\n# BUILD MODEL\nrecurrent_hidden_units=32\n\n# CREATE MODEL\nmodel = keras.Sequential()\n\n# ADD RECURRENT LAYER\n\n# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\nmodel.add(LSTM(\n#model.add(GRU(\n#model.add(SimpleRNN(\nunits=recurrent_hidden_units,\nreturn_sequences=False,\ninput_shape=input_shape, \nrecurrent_regularizer=regularizers.L2(L2),\n# recurrent_dropout=0.8,\nactivation='relu')\n          ) \n     \n# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \nmodel.add(Dense(units=1, activation='linear'))\n\n# MODEL SUMMARY\nprint(model.summary()); #print(x_train.shape,y_train.shape)\n# # print(\"initial parameters:\", model.get_weights())\n\n# # COMPILING THE MODEL \nopt = keras.optimizers.RMSprop(learning_rate=learning_rate)\nmodel.compile(optimizer=opt, loss=loss_function)\n\n# TRAINING YOUR MODEL\nhistory = model.fit(Xt,\n                    Yt,\n                    epochs=numbers_epochs,\n                    batch_size=batch_size, verbose=False,\n                    validation_data=(Xv, Yv))\n# History plot\nhistory_plot(history)\n\n# Predictions \nYtp=model.predict(Xt)\nYvp=model.predict(Xv) \n\n# REPORT\nregression_report(Yt,Ytp,Yv,Yvp) \nLSTM_e_r = sqrt(mean_squared_error(Yt, Ytp))\n\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 32)                4352      \n                                                                 \n dense_5 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 4385 (17.13 KB)\nTrainable params: 4385 (17.13 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n 1/19 [&gt;.............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/19 [=============&gt;................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/19 [=========================&gt;....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/19 [==============================] - 0s 6ms/step\n1/5 [=====&gt;........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 5ms/step\n---------- Regression report ----------\nTRAINING:\n MSE: 0.3051213235449832\n MAE: 0.45503845089254413\nVALIDATION:\n MSE: 0.6233462286768205\n MAE: 0.6498043169338328\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the model results, LSTM model with regularization has larger MSE and MAE values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data “too well”, and enhance generalization."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#evaluation",
    "href": "Deep_Learning_for_TS.html#evaluation",
    "title": "Deep Learning for TS",
    "section": "Evaluation",
    "text": "Evaluation\n\nComparison of RMSE of all three models\n\n\n\nCode\nimport seaborn as sns\n# Creating a DataFrame to hold the data\ndata = {\n    'Model': ['RNN', 'RNN with Regularization', 'GRU', 'GRU with Regularization', 'LSTM', 'LSTM with Regularization'],\n    'RMSE': [RNN_e, RNN_e_r, GRU_e, GRU_e_r, LSTM_e, LSTM_e_r]\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n# Set the palette to have different colors for each bar\npalette = sns.color_palette(\"hsv\", len(df))\n\n# Plotting using seaborn\nplt.figure(figsize=(10,6))\nsns.barplot(y='RMSE', x='Model', data=df, palette=palette)\nplt.xlabel('RMSE')\nplt.ylabel('Model')\nplt.title('Comparison of RMSE for RNN, GRU and LSTM with and without regularization')\nplt.show()\n\n\n                      Model      RMSE\n0                       RNN  0.567622\n1   RNN with Regularization  0.522250\n2                       GRU  0.590369\n3   GRU with Regularization  0.623947\n4                      LSTM  0.559422\n5  LSTM with Regularization  0.552378\n\n\n\n\n\n\nBased on the model diagnostics with RMSE, we can see that the all three deep learning model have similar accuracy, with LSTM model outperforms the other two with a slight lower RMSE on both with and without regularization.\nAlso, from the forcasting plots, we can see all three deep learning models can accurately forcast up to approximately 80 index, for my data set, this indicates accurate predictions up to 80 weeks of violent crimes."
  },
  {
    "objectID": "Deep_Learning_for_TS.html#comparison-with-traditional-ts-models",
    "href": "Deep_Learning_for_TS.html#comparison-with-traditional-ts-models",
    "title": "Deep Learning for TS",
    "section": "Comparison with traditional TS models",
    "text": "Comparison with traditional TS models\n\nIn order to compare with the SARIMA(0,2,2)(0,1,0)52, we need to denormalize the RMSE result from deep learning models to the normal scale.\n\n\n\n\nForcast plots for SARIMA(0,2,2)(0,1,0)52\n\n\n\n\nCode\ndata = pd.read_csv(\"./dataset/crimedata_week.csv\")\ndata.drop('offense_type', axis = 1)\nstd = np.std(data['Total_Crimes'])\n\n# Denormalizing the RMSE for the single time series\ndenormalized_rmse = df['RMSE'] * std\n\n# Update the DataFrame with denormalized RMSE\ndf['Denormalized RMSE'] = denormalized_rmse\ndf\n\n\n\n\n\n\n\n\n\nModel\nRMSE\nDenormalized RMSE\n\n\n\n\n0\nRNN\n0.567622\n53.702783\n\n\n1\nRNN with Regularization\n0.522250\n49.410124\n\n\n2\nGRU\n0.590369\n55.854881\n\n\n3\nGRU with Regularization\n0.623947\n59.031748\n\n\n4\nLSTM\n0.559422\n52.927018\n\n\n5\nLSTM with Regularization\n0.552378\n52.260567\n\n\n\n\n\n\n\n\n\nCode\n# Add a row for SARIMA model for plotting\ns_rmse = 67.19183\nsa = {'Model': 'SARIMA', 'RMSE': s_rmse, 'Denormalized RMSE': s_rmse}\n\nsa = pd.DataFrame([sa])\n\n# Concatenate the new row to the existing DataFrame\ndf = pd.concat([df, sa], ignore_index=True)\n\n# Plotting using seaborn\nplt.figure(figsize=(10,6))\nsns.barplot(y='Denormalized RMSE', x='Model', data=df, palette=palette)\nplt.xlabel('RMSE')\nplt.ylabel('Model')\nplt.title('Comparison of RMSE for RNN, GRU, LSTM with and without regularization and SARIMA model')\nplt.show()\n\n\n\n\n\n\nWe can clearly see that the RMSE for SARIMA(0,2,2)(0,1,0)52 is significantly higher than that of the deep learning modes, indicating that deep learning models have a higher accuracy than traditional time series model for my data set."
  },
  {
    "objectID": "Data_sources.html",
    "href": "Data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "1. The Crime Open Database (CODE), as supported and maintained by the OSF platform, is a comprehensive database service that offers crime data from multiple cities in the United States for analysis in the field of crime research. The database, which is publicly accessible and contains roughly 34.1GB of data, and is maintained by the contributor Matthew Ashby. the whole database is free for all users to use.\n2. The function get_crime_data() returns a tidy data tibble crime data sets with each row representing a single crime. The data provided for each offense includes the offense type, approximate offense location and date/time.\n\n\nCode\n#install.packages(\"crimedata\")\n#devtools::install_github(\"mpjashby/crimedata\")\nlibrary(crimedata)\nlibrary(tidyverse)\nlibrary(DT)\n#Load the crime data of NYC from 2007 to 2022\n#data &lt;- get_crime_data(\n#  cities = \"New York\", \n#  years = 2007:2022, \n#  type = \"core\",\n#  output = \"sf\"\n#) \n#Save the data\n#write.csv(data, file = \"dataset/crimedata_NYC.csv\", row.names = FALSE)\n\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\ndatatable(head(data))"
  },
  {
    "objectID": "Data_sources.html#crime-open-database-code",
    "href": "Data_sources.html#crime-open-database-code",
    "title": "Data Sources",
    "section": "",
    "text": "1. The Crime Open Database (CODE), as supported and maintained by the OSF platform, is a comprehensive database service that offers crime data from multiple cities in the United States for analysis in the field of crime research. The database, which is publicly accessible and contains roughly 34.1GB of data, and is maintained by the contributor Matthew Ashby. the whole database is free for all users to use.\n2. The function get_crime_data() returns a tidy data tibble crime data sets with each row representing a single crime. The data provided for each offense includes the offense type, approximate offense location and date/time.\n\n\nCode\n#install.packages(\"crimedata\")\n#devtools::install_github(\"mpjashby/crimedata\")\nlibrary(crimedata)\nlibrary(tidyverse)\nlibrary(DT)\n#Load the crime data of NYC from 2007 to 2022\n#data &lt;- get_crime_data(\n#  cities = \"New York\", \n#  years = 2007:2022, \n#  type = \"core\",\n#  output = \"sf\"\n#) \n#Save the data\n#write.csv(data, file = \"dataset/crimedata_NYC.csv\", row.names = FALSE)\n\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\ndatatable(head(data))"
  },
  {
    "objectID": "Data_sources.html#covidcast-dataset",
    "href": "Data_sources.html#covidcast-dataset",
    "title": "Data Sources",
    "section": "COVIDCast dataset",
    "text": "COVIDCast dataset\n1. COVIDcast is the name for Delphi’s project of Carnegie Mellon University in support of the COVID-19 response in the United States.\n2. It provides R access to the COVIDcast Epidata API published by the Delphi group at Carnegie Mellon University. This API provides daily access to a range of COVID-related variables Delphi have built and maintained.\n3. The package supports convenient plotting and mapping tools, correlation analyses between variables, and various geographical aspects for working with counties, metropolitan areas, and other units.\nExample Visualizations\n\n\nCode\n#install.packages(\"covidcast\")\nlibrary(covidcast)\nlibrary(tidyverse)\nlibrary(plotly)\ndeaths &lt;- covidcast_signal(\"jhu-csse\", \"deaths_incidence_num\",\n                           \"2020-04-15\", \"2021-04-15\",\n                           geo_type = \"nation\")\na &lt;- plot(deaths, plot_type = \"line\")\nggplotly(a)"
  },
  {
    "objectID": "Data_sources.html#unemployment-rate-in-new-york-nyur",
    "href": "Data_sources.html#unemployment-rate-in-new-york-nyur",
    "title": "Data Sources",
    "section": "Unemployment Rate in New York (NYUR)",
    "text": "Unemployment Rate in New York (NYUR)\nThe “Unemployment Rate in New York (NYUR)” data set, is supported and maintained by the Federal Reserve Bank of St. Louis's FRED database, which provides a comprehensive look at the fluctuations in unemployment rates within New York City. The data are updated monthly and are seasonally adjusted to account for predictable aspects within the job market throughout the year. Additionally, this data set supports comparison of unemployment rates over time.\nThe data provided by FRED is sourced from the U.S. Bureau of Labor Statistics, under the “State Employment and Unemployment” release, ensuring that the statistics are collected and processed by a reliable government authority."
  },
  {
    "objectID": "Data_sources.html#national-climatic-data-center-ncdc",
    "href": "Data_sources.html#national-climatic-data-center-ncdc",
    "title": "Data Sources",
    "section": "National Climatic Data Center (NCDC)",
    "text": "National Climatic Data Center (NCDC)\nThe Climate Data Online (CDO) is supported by the National Climatic Data Center (NCDC), a resource that offers access to a variety of aspects of climate data sets. Users can explore different data sets by name, each contains a detailed description, also, related tools for analysis, and options for FTP access and sample downloads."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html",
    "href": "ARMA_ARIMA_SARIMA_Models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For this modeling part, we will be fitting time series models to Total violent crimes, Aggvated assault and Robbery. For a better model fitting process, I will only be using weekly data to following reasons:\n\nHigh Frequency: Daily data often exhibits high frequency, resulting in a large number of data points. ARIMA, SARIMA models may struggle with the increased complexity and noise in such high-frequency data.\nOverfitting: With daily data, there might be a risk of overfitting, the daily data exhibits irregular fluctuations and the model will tend to learn those daily flucuations which contradicts our goal of generalization.\nComputational Intensity: Estimating SARIMA parameters involves optimization algorithms, and with daily data, the optimization problem can become computationally intense.\n\n\nTotal violent crimesRobbery CrimesAggragated Assault\n\n\n\n\n\nDuring the EDA process, I already differenced the data and used ADF test to check for stationarity.\n\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n\n\n\n\n\n\nCode\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.2691, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis.\n\n\n\n\n\n\n\nParameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n8729.585\n8738.909\n8729.601\n\n\n0\n2\n0\n9547.703\n9552.363\n9547.708\n\n\n0\n1\n1\n8515.772\n8529.758\n8515.803\n\n\n0\n2\n1\n8726.088\n8735.410\n8726.104\n\n\n0\n1\n2\n8514.685\n8533.333\n8514.737\n\n\n0\n2\n2\n8514.059\n8528.041\n8514.090\n\n\n0\n1\n3\n8515.799\n8539.108\n8515.876\n\n\n0\n2\n3\n8512.881\n8531.524\n8512.933\n\n\n1\n1\n0\n8556.439\n8570.425\n8556.470\n\n\n1\n2\n0\n9089.609\n9098.930\n9089.624\n\n\n1\n1\n1\n8514.979\n8533.626\n8515.030\n\n\n1\n2\n1\n8553.905\n8567.887\n8553.936\n\n\n1\n1\n2\n8512.438\n8535.748\n8512.516\n\n\n1\n2\n2\n8513.192\n8531.834\n8513.243\n\n\n1\n1\n3\n8514.270\n8542.241\n8514.378\n\n\n1\n2\n3\n8511.273\n8534.575\n8511.350\n\n\n2\n1\n0\n8524.907\n8543.555\n8524.959\n\n\n2\n2\n0\n8899.697\n8913.678\n8899.727\n\n\n2\n1\n1\n8516.590\n8539.899\n8516.667\n\n\n2\n2\n1\n8522.788\n8541.430\n8522.840\n\n\n2\n1\n2\n8514.229\n8542.200\n8514.337\n\n\n2\n2\n2\n8514.784\n8538.087\n8514.862\n\n\n2\n1\n3\n8513.575\n8546.208\n8513.720\n\n\n2\n2\n3\n8517.058\n8545.021\n8517.166\n\n\n3\n1\n0\n8515.867\n8539.176\n8515.945\n\n\n3\n2\n0\n8783.665\n8802.308\n8783.717\n\n\n3\n1\n1\n8517.028\n8544.999\n8517.136\n\n\n3\n2\n1\n8513.985\n8537.288\n8514.063\n\n\n3\n1\n2\n8514.507\n8547.140\n8514.651\n\n\n3\n2\n2\n8515.211\n8543.174\n8515.319\n\n\n3\n1\n3\n8517.805\n8555.100\n8517.992\n\n\n3\n2\n3\n8512.837\n8545.462\n8512.982\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC    AICc\n16 1 2 3 8511.273 8534.575 8511.35\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n6 0 2 2 8514.059 8528.041 8514.09\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC    AICc\n16 1 2 3 8511.273 8534.575 8511.35\n\n\n\nBased on the results, the ARIMA(0,2,2) and ARIMA(1,2,3) are the best models.\n\n\n\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 4.692267 \niter   2 value 4.326493\niter   3 value 4.149657\niter   4 value 4.138935\niter   5 value 4.119923\niter   6 value 4.112664\niter   7 value 4.105158\niter   8 value 4.100774\niter   9 value 4.058093\niter  10 value 4.057665\niter  11 value 4.057054\niter  12 value 4.053023\niter  13 value 4.033841\niter  14 value 4.032642\niter  15 value 4.032457\niter  16 value 4.032449\niter  17 value 4.032426\niter  18 value 4.032421\niter  18 value 4.032421\niter  18 value 4.032421\nfinal  value 4.032421 \nconverged\ninitial  value 4.031723 \niter   2 value 4.030889\niter   3 value 4.028158\niter   4 value 4.028046\niter   5 value 4.027964\niter   6 value 4.027962\niter   6 value 4.027962\nfinal  value 4.027962 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5286  0.5286\ns.e.   0.0280  0.0278\n\nsigma^2 estimated as 3118:  log likelihood = -4254.03,  aic = 8514.06\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5286 0.0280 -54.5980       0\nma2   0.5286 0.0278  19.0082       0\n\n$AIC\n[1] 10.90148\n\n$AICc\n[1] 10.9015\n\n$BIC\n[1] 10.91939\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = c + \\varepsilon_t - 1.5286\\varepsilon_{t-1} + 0.5286\\varepsilon_{t-2}\\)\n\nARIMA(1,2,3):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 1, 2, 3)\n\n\ninitial  value 4.692876 \niter   2 value 4.276925\niter   3 value 4.190265\niter   4 value 4.132567\niter   5 value 4.114998\niter   6 value 4.104167\niter   7 value 4.088415\niter   8 value 4.087752\niter   9 value 4.057506\niter  10 value 4.030335\niter  11 value 4.028233\niter  12 value 4.027630\niter  13 value 4.026883\niter  14 value 4.026196\niter  15 value 4.026169\niter  16 value 4.026150\niter  16 value 4.026150\nfinal  value 4.026150 \nconverged\ninitial  value 4.027428 \niter   2 value 4.026744\niter   3 value 4.026699\niter   4 value 4.026557\niter   5 value 4.026557\niter   6 value 4.026556\niter   7 value 4.026554\niter   8 value 4.026550\niter   9 value 4.026538\niter  10 value 4.026509\niter  11 value 4.026495\niter  12 value 4.026481\niter  13 value 4.026476\niter  14 value 4.026476\niter  15 value 4.026475\niter  16 value 4.026473\niter  17 value 4.026469\niter  18 value 4.026458\niter  19 value 4.026437\niter  20 value 4.026436\niter  21 value 4.026414\niter  22 value 4.026410\niter  23 value 4.026409\niter  24 value 4.026409\niter  25 value 4.026405\niter  26 value 4.026398\niter  27 value 4.026374\niter  28 value 4.026305\niter  29 value 4.026305\niter  30 value 4.026252\niter  31 value 4.026243\niter  32 value 4.026243\niter  33 value 4.026242\niter  34 value 4.026239\niter  35 value 4.026233\niter  36 value 4.026214\niter  37 value 4.026169\niter  38 value 4.026169\niter  39 value 4.026123\niter  40 value 4.026119\niter  41 value 4.026115\niter  42 value 4.026114\niter  43 value 4.026101\niter  44 value 4.026077\niter  45 value 4.025990\niter  46 value 4.025689\niter  47 value 4.025687\niter  48 value 4.025338\niter  49 value 4.025236\niter  50 value 4.025232\niter  51 value 4.025229\niter  52 value 4.025207\niter  53 value 4.025156\niter  54 value 4.025010\niter  55 value 4.024804\niter  56 value 4.024803\niter  57 value 4.024652\niter  58 value 4.024476\niter  59 value 4.024385\niter  60 value 4.024380\niter  61 value 4.024308\niter  62 value 4.024174\niter  63 value 4.023858\niter  64 value 4.023618\niter  64 value 4.023618\nfinal  value 4.023618 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2      ma3\n      0.5829  -2.1588  1.5395  -0.3807\ns.e.  0.1871   0.1882  0.2956   0.1086\n\nsigma^2 estimated as 3092:  log likelihood = -4250.64,  aic = 8511.27\n\n$degrees_of_freedom\n[1] 777\n\n$ttable\n    Estimate     SE  t.value p.value\nar1   0.5829 0.1871   3.1147  0.0019\nma1  -2.1588 0.1882 -11.4688  0.0000\nma2   1.5395 0.2956   5.2075  0.0000\nma3  -0.3807 0.1086  -3.5043  0.0005\n\n$AIC\n[1] 10.89792\n\n$AICc\n[1] 10.89798\n\n$BIC\n[1] 10.92775\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.5829 \\cdot \\varepsilon_{t-1} - 2.1588 \\cdot \\varepsilon_t + 1.5395 \\cdot \\varepsilon_{t-2} - 0.3807 \\cdot \\varepsilon_{t-3} + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n# Cross-validation for ARIMA(1,2,3)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(1, 2, 3))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(0,2,2)\", \"ARIMA(1,2,3)\"),\n                       MSE = c(mse_arima_022, mse_arima_123))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(1, 2, 3)due to less MSE.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n# auto.arima(w_ts)\n\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 1,2,3)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 705.8832 701.0396 698.2153 696.5680 695.6067 695.0454 694.7172 694.5249\n [9] 694.4118 694.3449 694.3048 694.2805 694.2653 694.2554 694.2487 694.2437\n[17] 694.2398 694.2365 694.2336 694.2309 694.2283 694.2258 694.2233 694.2209\n[25] 694.2184 694.2160 694.2136 694.2112 694.2087 694.2063 694.2039 694.2015\n[33] 694.1991 694.1966 694.1942 694.1918 694.1894 694.1870 694.1845 694.1821\n[41] 694.1797 694.1773 694.1749 694.1725 694.1700 694.1676 694.1652 694.1628\n[49] 694.1604 694.1579 694.1555 694.1531\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  55.64267  60.46606  65.89823  71.48065  76.98217  82.30267  87.40874\n [8]  92.29840  96.98327 101.47984 105.80546 109.97653 114.00791 117.91273\n[15] 121.70249 125.38725 128.97579 132.47581 135.89406 139.23655 142.50857\n[22] 145.71485 148.85965 151.94677 154.97966 157.96144 160.89494 163.78277\n[29] 166.62730 169.43071 172.19501 174.92205 177.61355 180.27110 182.89618\n[36] 185.49016 188.05435 190.58993 193.09803 195.57972 198.03599 200.46776\n[43] 202.87593 205.26132 207.62471 209.96686 212.28846 214.59018 216.87265\n[50] 219.13647 221.38223 223.61045\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no AR process and also no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 8468.3, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 294.84, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 2274.1, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals.\n\n\n\n\n\n\n\nFor the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*15),nrow=15)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n9372.810\n9377.401\n9372.815\n\n\n0\n2\n0\n0\n1\n1\n9059.512\n9068.696\n9059.529\n\n\n0\n2\n0\n0\n1\n2\n9060.531\n9074.306\n9060.564\n\n\n0\n2\n0\n1\n1\n0\n9181.870\n9191.053\n9181.886\n\n\n0\n2\n0\n1\n1\n1\n9060.336\n9074.111\n9060.369\n\n\n0\n2\n0\n2\n1\n0\n9127.387\n9141.162\n9127.420\n\n\n0\n2\n1\n0\n1\n0\n8581.108\n8590.291\n8581.124\n\n\n0\n2\n1\n0\n1\n1\n8276.842\n8290.617\n8276.876\n\n\n0\n2\n1\n1\n1\n0\n8396.789\n8410.564\n8396.822\n\n\n0\n2\n2\n0\n1\n0\n8273.441\n8287.216\n8273.474\n\n\n1\n2\n0\n0\n1\n0\n8907.929\n8917.113\n8907.946\n\n\n1\n2\n0\n0\n1\n1\n8613.583\n8627.358\n8613.616\n\n\n1\n2\n0\n1\n1\n0\n8738.135\n8751.910\n8738.169\n\n\n1\n2\n1\n0\n1\n0\n8375.312\n8389.087\n8375.345\n\n\n2\n2\n0\n0\n1\n0\n8733.919\n8747.694\n8733.953\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 5.006551 \niter   2 value 4.622313\niter   3 value 4.453561\niter   4 value 4.433500\niter   5 value 4.389045\niter   6 value 4.315838\niter   7 value 4.278707\niter   8 value 4.276188\niter   9 value 4.274223\niter  10 value 4.273702\niter  11 value 4.273680\niter  12 value 4.273680\niter  13 value 4.273678\niter  14 value 4.273678\niter  14 value 4.273678\niter  15 value 4.273678\niter  15 value 4.273678\niter  15 value 4.273678\nfinal  value 4.273678 \nconverged\ninitial  value 4.266043 \niter   2 value 4.256737\niter   3 value 4.254624\niter   4 value 4.252599\niter   5 value 4.251541\niter   6 value 4.251472\niter   7 value 4.251461\niter   7 value 4.251461\niter   7 value 4.251461\nfinal  value 4.251461 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.7364  0.7364    1.4144\ns.e.   0.0307  0.0305  298.6289\n\nsigma^2 estimated as 4849:  log likelihood = -4133.72,  aic = 8275.44\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.7364   0.0307 -56.5543  0.0000\nma2        0.7364   0.0305  24.1261  0.0000\nconstant   1.4144 298.6289   0.0047  0.9962\n\n$AIC\n[1] 11.35177\n\n$AICc\n[1] 11.35182\n\n$BIC\n[1] 11.37697\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n$(1 - (-1.7364)B) (1 - 0B^{365})(1 - 1B^{365}) (1 - )(1 - ^{365}) y_t = 1.4144 + (1 + 0.7364)(1 + 0^{365}) a_t $\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  640.2968  649.1412  721.9856  674.8300  645.6744  820.5188  744.3632\n [8]  705.2076  754.0520  698.8964  774.7408  792.5852  715.4296  826.2741\n[15]  848.1185  693.9629  772.8073  825.6517  817.4961  882.3405  878.1849\n[22]  945.0293  980.8737  922.7181  966.5625  994.4070  985.2514 1010.0958\n[29]  982.9402  950.7846  988.6290  889.4734  827.3178  908.1622  892.0066\n[36]  911.8510  837.6954  872.5399  766.3843  753.2287  804.0731  782.9175\n[43]  842.7619  906.6063  854.4507  730.2951  757.1395  729.9839  714.8283\n[50]  693.6728  685.5172  637.3616\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  69.68378  72.08829  74.42140  76.68961  78.89854  81.05302  83.15729\n [8]  85.21507  87.22964  89.20394  91.14059  93.04193  94.91008  96.74698\n[15]  98.55436 100.33382 102.08683 103.81471 105.51872 107.19997 108.85954\n[22] 110.49839 112.11743 113.71752 115.29944 116.86392 118.41166 119.94331\n[29] 121.45947 122.96072 124.44760 125.92062 127.38025 128.82696 130.26117\n[36] 131.68329 133.09371 134.49280 135.88091 137.25837 138.62549 139.98259\n[43] 141.32994 142.66784 143.99653 145.31627 146.62731 147.92987 149.22418\n[50] 150.51044 151.78888 153.05967\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = TRUE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.02046036 64.07666 50.25831 -0.3954723 6.906063 0.7486566\n                   ACF1\nTraining set -0.4480382\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                  ME     RMSE      MAE        MPE     MAPE MASE      ACF1\nTraining set 6.21751 86.10674 67.13133 0.05411534 9.227552    1 0.4941184\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                       ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set 4.185113e-14 94.61017 75.80391 -1.709056 10.59072 1.129188\n                  ACF1\nTraining set 0.7704864\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 1.776053 67.19183 51.33838 -0.1232156 7.139174 0.7647456\n                    ACF1\nTraining set 0.004121855\n\n\nBased on the accuracy metrics, The SARIMA model outperforms all methods, as seen in lower RMSE, MAE, and MASE, except the Naive method.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE)+\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly total violent crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\nCode\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"robbery\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ndata_w &lt;- data_t %&gt;%\n  mutate(week = floor_date(as.Date(date_single), unit = \"week\")) %&gt;%\n  group_by(week, offense_type) %&gt;%\n  summarise(Total_Crimes = sum(Total_Crimes)) %&gt;%\n  ungroup()\n\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n\n\n\n\n\nCode\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for Robbery weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for Robbery assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.6069, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis.\n\n\n\n\n\n\n\nParameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n7624.574\n7633.898\n7624.589\n\n\n0\n2\n0\n8393.258\n8397.918\n8393.263\n\n\n0\n1\n1\n7461.431\n7475.417\n7461.462\n\n\n0\n2\n1\n7622.490\n7631.812\n7622.506\n\n\n0\n1\n2\n7461.493\n7480.140\n7461.544\n\n\n0\n2\n2\n7460.986\n7474.968\n7461.017\n\n\n0\n1\n3\n7460.423\n7483.733\n7460.501\n\n\n0\n2\n3\n7461.121\n7479.764\n7461.173\n\n\n1\n1\n0\n7518.654\n7532.640\n7518.685\n\n\n1\n2\n0\n8042.365\n8051.687\n8042.381\n\n\n1\n1\n1\n7461.893\n7480.541\n7461.945\n\n\n1\n2\n1\n7517.322\n7531.303\n7517.353\n\n\n1\n1\n2\n7462.607\n7485.916\n7462.684\n\n\n1\n2\n2\n7461.512\n7480.155\n7461.564\n\n\n1\n1\n3\n7458.304\n7486.275\n7458.413\n\n\n1\n2\n3\n7462.221\n7485.524\n7462.298\n\n\n2\n1\n0\n7475.489\n7494.136\n7475.540\n\n\n2\n2\n0\n7865.554\n7879.535\n7865.585\n\n\n2\n1\n1\n7460.521\n7483.830\n7460.598\n\n\n2\n2\n1\n7474.638\n7493.281\n7474.690\n\n\n2\n1\n2\n7457.705\n7485.676\n7457.813\n\n\n2\n2\n2\n7460.096\n7483.399\n7460.174\n\n\n2\n1\n3\n7459.709\n7492.342\n7459.853\n\n\n2\n2\n3\n7464.949\n7492.913\n7465.058\n\n\n3\n1\n0\n7460.004\n7483.313\n7460.081\n\n\n3\n2\n0\n7748.305\n7766.947\n7748.356\n\n\n3\n1\n1\n7460.069\n7488.040\n7460.177\n\n\n3\n2\n1\n7459.452\n7482.755\n7459.529\n\n\n3\n1\n2\n7459.705\n7492.338\n7459.850\n\n\n3\n2\n2\n7459.620\n7487.584\n7459.729\n\n\n3\n1\n3\n7461.674\n7498.969\n7461.861\n\n\n3\n2\n3\n7460.246\n7492.870\n7460.391\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n21 2 1 2 7457.705 7485.676 7457.813\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 2 2 7460.986 7474.968 7461.017\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n21 2 1 2 7457.705 7485.676 7457.813\n\n\n\nBased on the results, the ARIMA(0,2,2) and ARIMA(2,1,2) are the best models.\n\n\n\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 3.953185 \niter   2 value 3.647913\niter   3 value 3.598719\niter   4 value 3.507897\niter   5 value 3.491709\niter   6 value 3.474912\niter   7 value 3.452684\niter   8 value 3.421456\niter   9 value 3.420827\niter  10 value 3.420588\niter  11 value 3.420495\niter  12 value 3.420494\niter  13 value 3.420470\niter  14 value 3.420467\niter  14 value 3.420467\niter  14 value 3.420467\nfinal  value 3.420467 \nconverged\ninitial  value 3.385960 \niter   2 value 3.377893\niter   3 value 3.363438\niter   4 value 3.358597\niter   5 value 3.354549\niter   6 value 3.353925\niter   7 value 3.353822\niter   8 value 3.353782\niter   9 value 3.353780\niter   9 value 3.353780\niter   9 value 3.353780\nfinal  value 3.353780 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5092  0.5092\ns.e.   0.0310  0.0308\n\nsigma^2 estimated as 809.8:  log likelihood = -3727.49,  aic = 7460.99\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5092 0.0310 -48.7334       0\nma2   0.5092 0.0308  16.5328       0\n\n$AIC\n[1] 9.553119\n\n$AICc\n[1] 9.553139\n\n$BIC\n[1] 9.571021\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = \\varepsilon_t - 1.5092\\varepsilon_{t-1} + 0.5092\\varepsilon_{t-2}\\)\n\nARIMA(2,1,2):\n\n\nCode\n# Model diagnostics for ARIMA(2,1,2)\nsarima(w_ts, 2, 1, 2)\n\n\ninitial  value 3.452679 \niter   2 value 3.373309\niter   3 value 3.346679\niter   4 value 3.345842\niter   5 value 3.345566\niter   6 value 3.345486\niter   7 value 3.345462\niter   8 value 3.345323\niter   9 value 3.345223\niter  10 value 3.345005\niter  11 value 3.344525\niter  12 value 3.341900\niter  13 value 3.341567\niter  14 value 3.340183\niter  15 value 3.339339\niter  16 value 3.339226\niter  17 value 3.339191\niter  18 value 3.338602\niter  19 value 3.338058\niter  20 value 3.337507\niter  21 value 3.337080\niter  22 value 3.336923\niter  23 value 3.336898\niter  24 value 3.336897\niter  25 value 3.336896\niter  26 value 3.336895\niter  27 value 3.336894\niter  28 value 3.336893\niter  29 value 3.336892\niter  30 value 3.336892\niter  30 value 3.336892\niter  30 value 3.336892\nfinal  value 3.336892 \nconverged\ninitial  value 3.341959 \niter   2 value 3.341946\niter   3 value 3.341846\niter   4 value 3.341794\niter   5 value 3.341784\niter   6 value 3.341776\niter   7 value 3.341774\niter   8 value 3.341771\niter   9 value 3.341764\niter  10 value 3.341753\niter  11 value 3.341745\niter  12 value 3.341742\niter  12 value 3.341742\nfinal  value 3.341742 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2  constant\n      0.8624  -0.2071  -1.3525  0.5491   -0.1371\ns.e.  0.1601   0.0660   0.1552  0.0968    0.5768\n\nsigma^2 estimated as 798.7:  log likelihood = -3722.85,  aic = 7457.7\n\n$degrees_of_freedom\n[1] 777\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.8624 0.1601  5.3855  0.0000\nar2       -0.2071 0.0660 -3.1392  0.0018\nma1       -1.3525 0.1552 -8.7146  0.0000\nma2        0.5491 0.0968  5.6727  0.0000\nconstant  -0.1371 0.5768 -0.2376  0.8122\n\n$AIC\n[1] 9.536707\n\n$AICc\n[1] 9.536806\n\n$BIC\n[1] 9.572476\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.8624 \\cdot Y_{t-1} - 0.2071 \\cdot Y_{t-2} - 1.3525 \\cdot \\varepsilon_{t-1} + 0.5491 \\cdot \\varepsilon_t - 0.1371 + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n# Cross-validation for ARIMA(2,1,2)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(2, 1, 2))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(0,2,2)\", \"ARIMA(2,1,2)\"),\n                       MSE = c(mse_arima_022, mse_arima_123))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(2, 1, 2)due to significantly less MSE.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n# auto.arima(w_ts)\n\n\n\nI’m having a weird issue with auto.arima() for this data, it will run for a long time and crash.\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 2,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 235.8106 232.6806 229.3520 227.0825 225.7673 225.0558 224.6674 224.4324\n [9] 224.2630 224.1183 223.9814 223.8460 223.7104 223.5742 223.4376 223.3008\n[17] 223.1638 223.0267 222.8897 222.7526 222.6156 222.4785 222.3414 222.2044\n[25] 222.0673 221.9302 221.7932 221.6561 221.5190 221.3820 221.2449 221.1078\n[33] 220.9708 220.8337 220.6966 220.5596 220.4225 220.2854 220.1484 220.0113\n[41] 219.8742 219.7372 219.6001 219.4631 219.3260 219.1889 219.0519 218.9148\n[49] 218.7777 218.6407 218.5036 218.3665\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  28.26175  31.72350  33.96419  36.37983  39.08449  41.92824  44.76714\n [8]  47.52142  50.16110  52.68264  55.09352  57.40440  59.62587  61.76734\n[15]  63.83687  65.84129  67.78642  69.67725  71.51809  73.31272  75.06446\n[22]  76.77624  78.45068  80.09012  81.69666  83.27222  84.81851  86.33712\n[29]  87.82947  89.29689  90.74057  92.16165  93.56114  94.94001  96.29913\n[36]  97.63934  98.96140 100.26603 101.55390 102.82564 104.08184 105.32307\n[43] 106.54983 107.76263 108.96193 110.14817 111.32178 112.48313 113.63262\n[50] 114.77060 115.89741 117.01336\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 18752, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 275.48, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 3949.8, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals.\n\n\n\n\n\n\n\nFor the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*30),nrow=30)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n8277.657\n8282.249\n8277.663\n\n\n0\n2\n0\n0\n1\n1\n7982.248\n7991.431\n7982.265\n\n\n0\n2\n0\n0\n1\n2\n7982.172\n7995.947\n7982.205\n\n\n0\n2\n0\n1\n1\n0\n8114.764\n8123.947\n8114.781\n\n\n0\n2\n0\n1\n1\n1\n7982.000\n7995.775\n7982.033\n\n\n0\n2\n0\n2\n1\n0\n8055.161\n8068.936\n8055.194\n\n\n0\n2\n1\n0\n1\n0\n7509.593\n7518.777\n7509.610\n\n\n0\n2\n1\n0\n1\n1\n7237.758\n7251.533\n7237.791\n\n\n0\n2\n1\n1\n1\n0\n7351.205\n7364.980\n7351.238\n\n\n0\n2\n2\n0\n1\n0\n7232.957\n7246.732\n7232.990\n\n\n1\n2\n0\n0\n1\n0\n7882.372\n7891.556\n7882.389\n\n\n1\n2\n0\n0\n1\n1\n7625.037\n7638.812\n7625.070\n\n\n1\n2\n0\n1\n1\n0\n7734.299\n7748.074\n7734.332\n\n\n1\n2\n1\n0\n1\n0\n7346.625\n7360.400\n7346.658\n\n\n2\n2\n0\n0\n1\n0\n7707.024\n7720.799\n7707.057\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 4.255412 \niter   2 value 3.907034\niter   3 value 3.860431\niter   4 value 3.818601\niter   5 value 3.780280\niter   6 value 3.746067\niter   7 value 3.719049\niter   8 value 3.708119\niter   9 value 3.699872\niter  10 value 3.691693\niter  11 value 3.652339\niter  12 value 3.605043\niter  13 value 3.589995\niter  14 value 3.588368\niter  15 value 3.582469\niter  16 value 3.582329\niter  17 value 3.581877\niter  18 value 3.581858\niter  19 value 3.581858\niter  19 value 3.581858\niter  19 value 3.581858\nfinal  value 3.581858 \nconverged\ninitial  value 3.563704 \niter   2 value 3.555266\niter   3 value 3.546826\niter   4 value 3.545355\niter   5 value 3.543937\niter   6 value 3.542426\niter   7 value 3.538641\niter   8 value 3.537907\niter   9 value 3.537832\niter  10 value 3.537825\niter  11 value 3.537825\niter  12 value 3.537824\niter  13 value 3.537823\niter  13 value 3.537823\niter  13 value 3.537823\nfinal  value 3.537823 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.7044  0.7044    0.4988\ns.e.   0.0290  0.0288  161.4600\n\nsigma^2 estimated as 1164:  log likelihood = -3613.48,  aic = 7234.96\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.7044   0.0290 -58.6910  0.0000\nma2        0.7044   0.0288  24.4246  0.0000\nconstant   0.4988 161.4600   0.0031  0.9975\n\n$AIC\n[1] 9.924498\n\n$AICc\n[1] 9.924543\n\n$BIC\n[1] 9.949692\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n\\((1 - (-1.7044)B) (1 - 0B^{365})(1 - 1B^{365}) y_t = 0.4988 + (1 + 0.7044\\nabla)(1 + 0\\nabla^{365}) a_t\\)\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 201.3764 217.2888 246.2012 217.1136 190.0260 249.9383 265.8507 250.7631\n [9] 235.6755 213.5878 228.5002 229.4126 226.3250 263.2374 228.1497 224.0621\n[17] 224.9745 266.8869 239.7993 263.7116 286.6240 304.5364 322.4488 286.3612\n[25] 308.2735 331.1859 305.0983 345.0107 330.9230 331.8354 321.7478 307.6602\n[33] 301.5726 311.4849 288.3973 297.3097 293.2221 320.1345 267.0468 264.9592\n[41] 251.8716 265.7840 289.6964 283.6087 334.5211 250.4335 269.3459 243.2582\n[49] 223.1706 216.0830 210.9954 208.9078\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 34.14280 35.61674 37.03585 38.40621 39.73286 41.02004 42.27136 43.48991\n [9] 44.67838 45.83911 46.97414 48.08531 49.17423 50.24234 51.29095 52.32122\n[17] 53.33423 54.33093 55.31221 56.27888 57.23167 58.17127 59.09831 60.01336\n[25] 60.91697 61.80964 62.69184 63.56401 64.42654 65.27983 66.12423 66.96007\n[33] 67.78768 68.60735 69.41937 70.22399 71.02147 71.81205 72.59595 73.37339\n[41] 74.14457 74.90968 75.66891 76.42244 77.17042 77.91303 78.65040 79.38270\n[49] 80.11005 80.83259 81.55045 82.26375\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = FALSE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.08951407 31.61258 24.53964 -0.6673075 8.861099 0.6429793\n                   ACF1\nTraining set -0.3584976\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                    ME     RMSE      MAE       MPE     MAPE MASE      ACF1\nTraining set -4.157319 47.55967 38.16553 -2.903463 14.42754    1 0.6184068\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                        ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set -2.161022e-14 63.25961 51.22099 -5.375413 19.44024 1.342075\n                  ACF1\nTraining set 0.8748357\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.6029735 32.92209 25.10713 -0.3763132 9.483319 0.6578483\n                   ACF1\nTraining set 0.01864713\n\n\nBased on the accuracy metrics, The SARIMA model,outperforms all methods, as seen in lower RMSE, MAE, and MASE, except the Naive method.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE) +\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly robbery crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))\n\n\n\n\n\n\n\n\nCode\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"aggravated assault\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ndata_w &lt;- data_t %&gt;%\n  mutate(week = floor_date(as.Date(date_single), unit = \"week\")) %&gt;%\n  group_by(week, offense_type) %&gt;%\n  summarise(Total_Crimes = sum(Total_Crimes)) %&gt;%\n  ungroup()\n\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n\n\n\n\n\nCode\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for Aggragated Assault weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for Aggragated Assault assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.2398, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis.\n\n\n\n\n\n\n\nParameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n8084.924\n8094.248\n8084.939\n\n\n0\n2\n0\n8910.079\n8914.740\n8910.084\n\n\n0\n1\n1\n7834.385\n7848.370\n7834.415\n\n\n0\n2\n1\n8082.252\n8091.573\n8082.267\n\n\n0\n1\n2\n7832.778\n7851.426\n7832.830\n\n\n0\n2\n2\n7833.763\n7847.745\n7833.794\n\n\n0\n1\n3\n7831.200\n7854.509\n7831.277\n\n\n0\n2\n3\n7832.073\n7850.715\n7832.125\n\n\n1\n1\n0\n7900.949\n7914.934\n7900.980\n\n\n1\n2\n0\n8466.254\n8475.576\n8466.270\n\n\n1\n1\n1\n7833.431\n7852.079\n7833.483\n\n\n1\n2\n1\n7899.270\n7913.252\n7899.301\n\n\n1\n1\n2\n7825.765\n7849.075\n7825.843\n\n\n1\n2\n2\n7832.749\n7851.391\n7832.801\n\n\n1\n1\n3\n7826.952\n7854.923\n7827.060\n\n\n1\n2\n3\n7837.605\n7860.908\n7837.683\n\n\n2\n1\n0\n7847.823\n7866.471\n7847.875\n\n\n2\n2\n0\n8252.170\n8266.151\n8252.200\n\n\n2\n1\n1\n7833.108\n7856.418\n7833.186\n\n\n2\n2\n1\n7846.678\n7865.320\n7846.730\n\n\n2\n1\n2\n7826.858\n7854.829\n7826.967\n\n\n2\n2\n2\n7832.387\n7855.690\n7832.464\n\n\n2\n1\n3\n7829.143\n7861.776\n7829.288\n\n\n2\n2\n3\n7825.737\n7853.700\n7825.845\n\n\n3\n1\n0\n7834.207\n7857.517\n7834.285\n\n\n3\n2\n0\n8126.832\n8145.475\n8126.884\n\n\n3\n1\n1\n7833.883\n7861.854\n7833.991\n\n\n3\n2\n1\n7833.344\n7856.647\n7833.422\n\n\n3\n1\n2\n7828.243\n7860.876\n7828.387\n\n\n3\n2\n2\n7833.144\n7861.108\n7833.253\n\n\n3\n1\n3\n7828.712\n7866.007\n7828.899\n\n\n3\n2\n3\n7827.186\n7859.810\n7827.331\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC    BIC     AICc\n24 2 2 3 7825.737 7853.7 7825.845\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 2 2 7833.763 7847.745 7833.794\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n13 1 1 2 7825.765 7849.075 7825.843\n\n\n\nBased on the results, the ARIMA(2,2,3) and ARIMA(0,2,2), ARIMA(1,1,2) are the best models.\n\n\n\n\nARIMA(2,2,3):\n\n\nCode\n# Model diagnostics for ARIMA(2,2,3)\nsarima(w_ts, 2, 2, 3)\n\n\ninitial  value 4.285103 \niter   2 value 3.929403\niter   3 value 3.801131\niter   4 value 3.719204\niter   5 value 3.699748\niter   6 value 3.683616\niter   7 value 3.679915\niter   8 value 3.600624\niter   9 value 3.598777\niter  10 value 3.596603\niter  11 value 3.596014\niter  12 value 3.595351\niter  13 value 3.595307\niter  14 value 3.593381\niter  15 value 3.592583\niter  16 value 3.591682\niter  17 value 3.590865\niter  18 value 3.590420\niter  19 value 3.589676\niter  20 value 3.585557\niter  21 value 3.585318\niter  22 value 3.585297\niter  23 value 3.584762\niter  24 value 3.584302\niter  25 value 3.583853\niter  26 value 3.583131\niter  27 value 3.582996\niter  28 value 3.582781\niter  29 value 3.582434\niter  30 value 3.582175\niter  31 value 3.582074\niter  31 value 3.582074\nfinal  value 3.582074 \nconverged\ninitial  value 3.589659 \niter   2 value 3.589092\niter   3 value 3.589068\niter   4 value 3.589053\niter   5 value 3.589050\niter   6 value 3.589048\niter   7 value 3.589024\niter   8 value 3.588981\niter   9 value 3.588864\niter  10 value 3.588555\niter  11 value 3.588212\niter  12 value 3.588030\niter  13 value 3.587849\niter  14 value 3.587657\niter  15 value 3.587569\niter  16 value 3.587522\niter  17 value 3.587516\niter  18 value 3.587508\niter  19 value 3.587480\niter  20 value 3.587404\niter  21 value 3.587179\niter  22 value 3.586828\niter  23 value 3.586637\niter  24 value 3.586565\niter  25 value 3.585917\niter  26 value 3.585636\niter  27 value 3.585443\niter  28 value 3.585422\niter  29 value 3.585356\niter  30 value 3.585181\niter  31 value 3.584364\niter  32 value 3.584095\niter  33 value 3.583645\niter  34 value 3.583643\niter  35 value 3.583625\niter  36 value 3.583463\niter  37 value 3.583460\niter  37 value 3.583460\niter  38 value 3.583460\niter  39 value 3.583454\niter  39 value 3.583454\niter  39 value 3.583454\nfinal  value 3.583454 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3\n      0.7601  -0.0628  -2.3910  1.9506  -0.5596\ns.e.  0.1013   0.0613   0.0949  0.1539   0.0632\n\nsigma^2 estimated as 1282:  log likelihood = -3906.87,  aic = 7825.74\n\n$degrees_of_freedom\n[1] 776\n\n$ttable\n    Estimate     SE  t.value p.value\nar1   0.7601 0.1013   7.5056  0.0000\nar2  -0.0628 0.0613  -1.0236  0.3063\nma1  -2.3910 0.0949 -25.1964  0.0000\nma2   1.9506 0.1539  12.6758  0.0000\nma3  -0.5596 0.0632  -8.8480  0.0000\n\n$AIC\n[1] 10.02015\n\n$AICc\n[1] 10.02025\n\n$BIC\n[1] 10.05595\n\n\n\nThe model diagnostics tell us that the most parameters are statistically significant except for ar2, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = 0.7601 \\cdot Y_{t-1} - 0.0628 \\cdot Y_{t-2} - 2.3910 \\cdot \\varepsilon_{t-1} + 1.9506 \\cdot \\varepsilon_{t-2} - 0.5596 \\cdot \\varepsilon_{t-3} + \\varepsilon_t\\)\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 4.284057 \niter   2 value 3.918786\niter   3 value 3.715856\niter   4 value 3.709338\niter   5 value 3.703956\niter   6 value 3.633458\niter   7 value 3.631828\niter   8 value 3.614513\niter   9 value 3.612157\niter  10 value 3.611056\niter  11 value 3.610938\niter  12 value 3.610854\niter  13 value 3.610785\niter  14 value 3.610757\niter  15 value 3.610753\niter  15 value 3.610753\niter  15 value 3.610753\nfinal  value 3.610753 \nconverged\ninitial  value 3.603018 \niter   2 value 3.598993\niter   3 value 3.594341\niter   4 value 3.594119\niter   5 value 3.592452\niter   6 value 3.592438\niter   7 value 3.592434\niter   8 value 3.592434\niter   9 value 3.592434\niter   9 value 3.592434\niter   9 value 3.592434\nfinal  value 3.592434 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5777  0.5777\ns.e.   0.0263  0.0261\n\nsigma^2 estimated as 1305:  log likelihood = -3913.88,  aic = 7833.76\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5777 0.0263 -60.0933       0\nma2   0.5777 0.0261  22.1683       0\n\n$AIC\n[1] 10.03043\n\n$AICc\n[1] 10.03045\n\n$BIC\n[1] 10.04833\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = -1.5777 \\cdot \\varepsilon_{t-1} + 0.5777 \\cdot \\varepsilon_{t-2} + \\varepsilon_t\\)\nARIMA(1,1,2):\n\n\nCode\n# Model diagnostics for ARIMA(1,1,2)\nsarima(w_ts, 1, 1, 2)\n\n\ninitial  value 3.748440 \niter   2 value 3.632828\niter   3 value 3.586798\niter   4 value 3.586611\niter   5 value 3.585291\niter   6 value 3.585269\niter   7 value 3.585262\niter   8 value 3.585215\niter   9 value 3.585108\niter  10 value 3.584767\niter  11 value 3.584462\niter  12 value 3.584337\niter  13 value 3.584304\niter  14 value 3.584291\niter  15 value 3.584110\niter  16 value 3.583659\niter  17 value 3.582707\niter  18 value 3.582548\niter  19 value 3.581932\niter  20 value 3.581468\niter  21 value 3.581396\niter  22 value 3.581329\niter  23 value 3.581025\niter  24 value 3.580314\niter  25 value 3.580085\niter  26 value 3.579960\niter  27 value 3.579528\niter  28 value 3.579202\niter  29 value 3.578829\niter  30 value 3.578738\niter  31 value 3.578569\niter  32 value 3.578518\niter  33 value 3.578511\niter  34 value 3.578511\niter  34 value 3.578511\niter  34 value 3.578511\nfinal  value 3.578511 \nconverged\ninitial  value 3.578447 \niter   2 value 3.578444\niter   3 value 3.578431\niter   4 value 3.578425\niter   5 value 3.578413\niter   6 value 3.578392\niter   7 value 3.578369\niter   8 value 3.578357\niter   9 value 3.578354\niter  10 value 3.578354\niter  10 value 3.578354\nfinal  value 3.578354 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2  constant\n      0.7518  -1.3994  0.5381    0.1778\ns.e.  0.0893   0.0862  0.0533    0.7144\n\nsigma^2 estimated as 1282:  log likelihood = -3907.88,  aic = 7825.77\n\n$degrees_of_freedom\n[1] 778\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.7518 0.0893   8.4176  0.0000\nma1       -1.3994 0.0862 -16.2393  0.0000\nma2        0.5381 0.0533  10.0876  0.0000\nconstant   0.1778 0.7144   0.2488  0.8036\n\n$AIC\n[1] 10.00737\n\n$AICc\n[1] 10.00744\n\n$BIC\n[1] 10.03718\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.7518 \\cdot Y_{t-1} - 1.3994 \\cdot \\varepsilon_{t-1} + 0.5381 \\cdot \\varepsilon_{t-2} + 0.1778 + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(2,2,3)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(2, 2, 3))\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n\n# Cross-validation for ARIMA(1,1,2)\ncv_arima_112 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(1, 1, 2))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\nmse_arima_112 &lt;- mean(cv_arima_112^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(2,2,3)\", \"ARIMA(0,2,2)\", \"ARIMA(1,1,2)\"),\n                       MSE = c(mse_arima_022, mse_arima_123,mse_arima_112 ))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(1,1,2)due to significantly less MSE compared with the other 2.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n#auto.arima(w_ts)\n\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 1,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 427.9520 424.1782 421.3854 419.3300 417.8289 416.7446 415.9736 415.4381\n [9] 415.0796 414.8543 414.7290 414.6790 414.6855 414.7345 414.8155 414.9205\n[17] 415.0436 415.1802 415.3271 415.4816 415.6419 415.8065 415.9744 416.1448\n[25] 416.3170 416.4905 416.6651 416.8405 417.0165 417.1930 417.3697 417.5467\n[33] 417.7239 417.9013 418.0787 418.2562 418.4338 418.6114 418.7891 418.9668\n[41] 419.1445 419.3222 419.5000 419.6777 419.8555 420.0332 420.2110 420.3887\n[49] 420.5665 420.7442 420.9220 421.0998\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  35.80264  37.95971  40.61541  43.58851  46.73470  49.95144  53.17083\n [8]  56.35021  59.46430  62.49929  65.44879  68.31108  71.08731  73.78034\n[15]  76.39393  78.93228  81.39974  83.80060  86.13897  88.41876  90.64359\n[22]  92.81684  94.94165  97.02087  99.05715 101.05292 103.01041 104.93166\n[29] 106.81857 108.67288 110.49618 112.28997 114.05560 115.79437 117.50744\n[36] 119.19592 120.86083 122.50313 124.12371 125.72341 127.30302 128.86327\n[43] 130.40486 131.92843 133.43462 134.92399 136.39710 137.85447 139.29659\n[50] 140.72394 142.13695 143.53606\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 12007, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 292.38, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 1096.4, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals.\n\n\n\n\n\n\n\nFor the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*30),nrow=30)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n8799.569\n8804.161\n8799.574\n\n\n0\n2\n0\n0\n1\n1\n8465.215\n8474.398\n8465.231\n\n\n0\n2\n0\n0\n1\n2\n8467.093\n8480.868\n8467.126\n\n\n0\n2\n0\n1\n1\n0\n8600.064\n8609.248\n8600.081\n\n\n0\n2\n0\n1\n1\n1\n8467.078\n8480.853\n8467.111\n\n\n0\n2\n0\n2\n1\n0\n8533.393\n8547.168\n8533.426\n\n\n0\n2\n1\n0\n1\n0\n7997.299\n8006.483\n7997.316\n\n\n0\n2\n1\n0\n1\n1\n7677.132\n7690.907\n7677.166\n\n\n0\n2\n1\n1\n1\n0\n7806.460\n7820.235\n7806.493\n\n\n0\n2\n2\n0\n1\n0\n7630.988\n7644.763\n7631.021\n\n\n1\n2\n0\n0\n1\n0\n8334.333\n8343.516\n8334.349\n\n\n1\n2\n0\n0\n1\n1\n8030.936\n8044.711\n8030.969\n\n\n1\n2\n0\n1\n1\n0\n8152.432\n8166.207\n8152.465\n\n\n1\n2\n1\n0\n1\n0\n7769.329\n7783.104\n7769.362\n\n\n2\n2\n0\n0\n1\n0\n8129.766\n8143.541\n8129.799\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 4.613380 \niter   2 value 4.217159\niter   3 value 4.035586\niter   4 value 4.010065\niter   5 value 3.985332\niter   6 value 3.959958\niter   7 value 3.883472\niter   8 value 3.880224\niter   9 value 3.878306\niter  10 value 3.874143\niter  11 value 3.873777\niter  12 value 3.873763\niter  13 value 3.873763\niter  13 value 3.873763\niter  13 value 3.873763\nfinal  value 3.873763 \nconverged\ninitial  value 3.852327 \niter   2 value 3.839837\niter   3 value 3.830646\niter   4 value 3.828493\niter   5 value 3.826859\niter   6 value 3.825499\niter   7 value 3.821067\niter   8 value 3.811424\niter   9 value 3.810832\niter  10 value 3.810808\niter  10 value 3.810808\niter  10 value 3.810808\nfinal  value 3.810808 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.8179  0.8179    0.7798\ns.e.   0.0298  0.0296  195.0190\n\nsigma^2 estimated as 2006:  log likelihood = -3812.48,  aic = 7632.97\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.8179   0.0298 -61.0486  0.0000\nma2        0.8179   0.0296  27.6262  0.0000\nconstant   0.7798 195.0190   0.0040  0.9968\n\n$AIC\n[1] 10.47047\n\n$AICc\n[1] 10.47051\n\n$BIC\n[1] 10.49566\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 397.1870 384.1760 436.1650 417.1539 417.1429 494.1318 444.1208 404.1097\n [9] 467.0987 427.0877 492.0766 511.0656 446.0545 514.0435 568.0324 424.0214\n[17] 492.0104 502.9993 535.9883 561.9772 549.9662 586.9552 604.9441 594.9331\n[25] 598.9220 601.9110 615.8999 609.8889 601.8779 579.8668 606.8558 551.8447\n[33] 495.8337 541.8226 554.8116 573.8006 505.7895 512.7785 482.7674 441.7564\n[41] 510.7453 489.7343 511.7233 586.7122 490.7012 456.6901 467.6791 457.6680\n[49] 456.6570 447.6460 455.6349 412.6239\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 44.81725 45.56558 46.30380 47.03241 47.75183 48.46247 49.16473 49.85894\n [9] 50.54544 51.22454 51.89654 52.56170 53.22028 53.87253 54.51866 55.15890\n[17] 55.79344 56.42249 57.04622 57.66480 58.27839 58.88716 59.49126 60.09081\n[25] 60.68597 61.27684 61.86357 62.44626 63.02503 63.59999 64.17122 64.73885\n[33] 65.30295 65.86363 66.42096 66.97503 67.52592 68.07370 68.61846 69.16027\n[41] 69.69918 70.23528 70.76861 71.29925 71.82726 72.35269 72.87559 73.39603\n[49] 73.91405 74.42971 74.94305 75.45412\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = FALSE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.1483376 42.43183 33.99233 -0.6180055 9.17241 0.8663712\n                   ACF1\nTraining set -0.4599732\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                   ME     RMSE      MAE      MPE    MAPE MASE      ACF1\nTraining set 11.80164 51.23555 39.23529 2.018993 10.1256    1 0.3281593\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                       ME     RMSE      MAE      MPE     MAPE     MASE\nTraining set 2.345155e-14 69.96365 55.83841 -3.39109 15.14925 1.423168\n                  ACF1\nTraining set 0.8151155\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                  ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.91809 43.21687 32.99462 -0.2909962 8.705878 0.8409423\n                      ACF1\nTraining set -0.0007675607\n\n\nBased on the accuracy metrics, The SARIMA model outperforms all other methods in RMSE, MAE, MPE, MAPE, and MASE.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE) +\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly aggregated assault crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#rsults-from-eda-process",
    "href": "ARMA_ARIMA_SARIMA_Models.html#rsults-from-eda-process",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "During the EDA process, I already differenced the data and used ADF test to check for stationarity.\n\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))"
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots",
    "href": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for violent assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.2691, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#arima-model",
    "href": "ARMA_ARIMA_SARIMA_Models.html#arima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Parameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n8729.585\n8738.909\n8729.601\n\n\n0\n2\n0\n9547.703\n9552.363\n9547.708\n\n\n0\n1\n1\n8515.772\n8529.758\n8515.803\n\n\n0\n2\n1\n8726.088\n8735.410\n8726.104\n\n\n0\n1\n2\n8514.685\n8533.333\n8514.737\n\n\n0\n2\n2\n8514.059\n8528.041\n8514.090\n\n\n0\n1\n3\n8515.799\n8539.108\n8515.876\n\n\n0\n2\n3\n8512.881\n8531.524\n8512.933\n\n\n1\n1\n0\n8556.439\n8570.425\n8556.470\n\n\n1\n2\n0\n9089.609\n9098.930\n9089.624\n\n\n1\n1\n1\n8514.979\n8533.626\n8515.030\n\n\n1\n2\n1\n8553.905\n8567.887\n8553.936\n\n\n1\n1\n2\n8512.438\n8535.748\n8512.516\n\n\n1\n2\n2\n8513.192\n8531.834\n8513.243\n\n\n1\n1\n3\n8514.270\n8542.241\n8514.378\n\n\n1\n2\n3\n8511.273\n8534.575\n8511.350\n\n\n2\n1\n0\n8524.907\n8543.555\n8524.959\n\n\n2\n2\n0\n8899.697\n8913.678\n8899.727\n\n\n2\n1\n1\n8516.590\n8539.899\n8516.667\n\n\n2\n2\n1\n8522.788\n8541.430\n8522.840\n\n\n2\n1\n2\n8514.229\n8542.200\n8514.337\n\n\n2\n2\n2\n8514.784\n8538.087\n8514.862\n\n\n2\n1\n3\n8513.575\n8546.208\n8513.720\n\n\n2\n2\n3\n8517.058\n8545.021\n8517.166\n\n\n3\n1\n0\n8515.867\n8539.176\n8515.945\n\n\n3\n2\n0\n8783.665\n8802.308\n8783.717\n\n\n3\n1\n1\n8517.028\n8544.999\n8517.136\n\n\n3\n2\n1\n8513.985\n8537.288\n8514.063\n\n\n3\n1\n2\n8514.507\n8547.140\n8514.651\n\n\n3\n2\n2\n8515.211\n8543.174\n8515.319\n\n\n3\n1\n3\n8517.805\n8555.100\n8517.992\n\n\n3\n2\n3\n8512.837\n8545.462\n8512.982\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC    AICc\n16 1 2 3 8511.273 8534.575 8511.35\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n6 0 2 2 8514.059 8528.041 8514.09\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC    AICc\n16 1 2 3 8511.273 8534.575 8511.35\n\n\n\nBased on the results, the ARIMA(0,2,2) and ARIMA(1,2,3) are the best models.\n\n\n\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 4.692267 \niter   2 value 4.326493\niter   3 value 4.149657\niter   4 value 4.138935\niter   5 value 4.119923\niter   6 value 4.112664\niter   7 value 4.105158\niter   8 value 4.100774\niter   9 value 4.058093\niter  10 value 4.057665\niter  11 value 4.057054\niter  12 value 4.053023\niter  13 value 4.033841\niter  14 value 4.032642\niter  15 value 4.032457\niter  16 value 4.032449\niter  17 value 4.032426\niter  18 value 4.032421\niter  18 value 4.032421\niter  18 value 4.032421\nfinal  value 4.032421 \nconverged\ninitial  value 4.031723 \niter   2 value 4.030889\niter   3 value 4.028158\niter   4 value 4.028046\niter   5 value 4.027964\niter   6 value 4.027962\niter   6 value 4.027962\nfinal  value 4.027962 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5286  0.5286\ns.e.   0.0280  0.0278\n\nsigma^2 estimated as 3118:  log likelihood = -4254.03,  aic = 8514.06\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5286 0.0280 -54.5980       0\nma2   0.5286 0.0278  19.0082       0\n\n$AIC\n[1] 10.90148\n\n$AICc\n[1] 10.9015\n\n$BIC\n[1] 10.91939\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = c + \\varepsilon_t - 1.5286\\varepsilon_{t-1} + 0.5286\\varepsilon_{t-2}\\)\n\nARIMA(1,2,3):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 1, 2, 3)\n\n\ninitial  value 4.692876 \niter   2 value 4.276925\niter   3 value 4.190265\niter   4 value 4.132567\niter   5 value 4.114998\niter   6 value 4.104167\niter   7 value 4.088415\niter   8 value 4.087752\niter   9 value 4.057506\niter  10 value 4.030335\niter  11 value 4.028233\niter  12 value 4.027630\niter  13 value 4.026883\niter  14 value 4.026196\niter  15 value 4.026169\niter  16 value 4.026150\niter  16 value 4.026150\nfinal  value 4.026150 \nconverged\ninitial  value 4.027428 \niter   2 value 4.026744\niter   3 value 4.026699\niter   4 value 4.026557\niter   5 value 4.026557\niter   6 value 4.026556\niter   7 value 4.026554\niter   8 value 4.026550\niter   9 value 4.026538\niter  10 value 4.026509\niter  11 value 4.026495\niter  12 value 4.026481\niter  13 value 4.026476\niter  14 value 4.026476\niter  15 value 4.026475\niter  16 value 4.026473\niter  17 value 4.026469\niter  18 value 4.026458\niter  19 value 4.026437\niter  20 value 4.026436\niter  21 value 4.026414\niter  22 value 4.026410\niter  23 value 4.026409\niter  24 value 4.026409\niter  25 value 4.026405\niter  26 value 4.026398\niter  27 value 4.026374\niter  28 value 4.026305\niter  29 value 4.026305\niter  30 value 4.026252\niter  31 value 4.026243\niter  32 value 4.026243\niter  33 value 4.026242\niter  34 value 4.026239\niter  35 value 4.026233\niter  36 value 4.026214\niter  37 value 4.026169\niter  38 value 4.026169\niter  39 value 4.026123\niter  40 value 4.026119\niter  41 value 4.026115\niter  42 value 4.026114\niter  43 value 4.026101\niter  44 value 4.026077\niter  45 value 4.025990\niter  46 value 4.025689\niter  47 value 4.025687\niter  48 value 4.025338\niter  49 value 4.025236\niter  50 value 4.025232\niter  51 value 4.025229\niter  52 value 4.025207\niter  53 value 4.025156\niter  54 value 4.025010\niter  55 value 4.024804\niter  56 value 4.024803\niter  57 value 4.024652\niter  58 value 4.024476\niter  59 value 4.024385\niter  60 value 4.024380\niter  61 value 4.024308\niter  62 value 4.024174\niter  63 value 4.023858\niter  64 value 4.023618\niter  64 value 4.023618\nfinal  value 4.023618 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2      ma3\n      0.5829  -2.1588  1.5395  -0.3807\ns.e.  0.1871   0.1882  0.2956   0.1086\n\nsigma^2 estimated as 3092:  log likelihood = -4250.64,  aic = 8511.27\n\n$degrees_of_freedom\n[1] 777\n\n$ttable\n    Estimate     SE  t.value p.value\nar1   0.5829 0.1871   3.1147  0.0019\nma1  -2.1588 0.1882 -11.4688  0.0000\nma2   1.5395 0.2956   5.2075  0.0000\nma3  -0.3807 0.1086  -3.5043  0.0005\n\n$AIC\n[1] 10.89792\n\n$AICc\n[1] 10.89798\n\n$BIC\n[1] 10.92775\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.5829 \\cdot \\varepsilon_{t-1} - 2.1588 \\cdot \\varepsilon_t + 1.5395 \\cdot \\varepsilon_{t-2} - 0.3807 \\cdot \\varepsilon_{t-3} + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n# Cross-validation for ARIMA(1,2,3)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(1, 2, 3))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(0,2,2)\", \"ARIMA(1,2,3)\"),\n                       MSE = c(mse_arima_022, mse_arima_123))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(1, 2, 3)due to less MSE.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n# auto.arima(w_ts)\n\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 1,2,3)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 705.8832 701.0396 698.2153 696.5680 695.6067 695.0454 694.7172 694.5249\n [9] 694.4118 694.3449 694.3048 694.2805 694.2653 694.2554 694.2487 694.2437\n[17] 694.2398 694.2365 694.2336 694.2309 694.2283 694.2258 694.2233 694.2209\n[25] 694.2184 694.2160 694.2136 694.2112 694.2087 694.2063 694.2039 694.2015\n[33] 694.1991 694.1966 694.1942 694.1918 694.1894 694.1870 694.1845 694.1821\n[41] 694.1797 694.1773 694.1749 694.1725 694.1700 694.1676 694.1652 694.1628\n[49] 694.1604 694.1579 694.1555 694.1531\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  55.64267  60.46606  65.89823  71.48065  76.98217  82.30267  87.40874\n [8]  92.29840  96.98327 101.47984 105.80546 109.97653 114.00791 117.91273\n[15] 121.70249 125.38725 128.97579 132.47581 135.89406 139.23655 142.50857\n[22] 145.71485 148.85965 151.94677 154.97966 157.96144 160.89494 163.78277\n[29] 166.62730 169.43071 172.19501 174.92205 177.61355 180.27110 182.89618\n[36] 185.49016 188.05435 190.58993 193.09803 195.57972 198.03599 200.46776\n[43] 202.87593 205.26132 207.62471 209.96686 212.28846 214.59018 216.87265\n[50] 219.13647 221.38223 223.61045\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no AR process and also no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 8468.3, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 294.84, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 2274.1, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#sarima-model",
    "href": "ARMA_ARIMA_SARIMA_Models.html#sarima-model",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*15),nrow=15)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n9372.810\n9377.401\n9372.815\n\n\n0\n2\n0\n0\n1\n1\n9059.512\n9068.696\n9059.529\n\n\n0\n2\n0\n0\n1\n2\n9060.531\n9074.306\n9060.564\n\n\n0\n2\n0\n1\n1\n0\n9181.870\n9191.053\n9181.886\n\n\n0\n2\n0\n1\n1\n1\n9060.336\n9074.111\n9060.369\n\n\n0\n2\n0\n2\n1\n0\n9127.387\n9141.162\n9127.420\n\n\n0\n2\n1\n0\n1\n0\n8581.108\n8590.291\n8581.124\n\n\n0\n2\n1\n0\n1\n1\n8276.842\n8290.617\n8276.876\n\n\n0\n2\n1\n1\n1\n0\n8396.789\n8410.564\n8396.822\n\n\n0\n2\n2\n0\n1\n0\n8273.441\n8287.216\n8273.474\n\n\n1\n2\n0\n0\n1\n0\n8907.929\n8917.113\n8907.946\n\n\n1\n2\n0\n0\n1\n1\n8613.583\n8627.358\n8613.616\n\n\n1\n2\n0\n1\n1\n0\n8738.135\n8751.910\n8738.169\n\n\n1\n2\n1\n0\n1\n0\n8375.312\n8389.087\n8375.345\n\n\n2\n2\n0\n0\n1\n0\n8733.919\n8747.694\n8733.953\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 8273.441 8287.216 8273.474\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 5.006551 \niter   2 value 4.622313\niter   3 value 4.453561\niter   4 value 4.433500\niter   5 value 4.389045\niter   6 value 4.315838\niter   7 value 4.278707\niter   8 value 4.276188\niter   9 value 4.274223\niter  10 value 4.273702\niter  11 value 4.273680\niter  12 value 4.273680\niter  13 value 4.273678\niter  14 value 4.273678\niter  14 value 4.273678\niter  15 value 4.273678\niter  15 value 4.273678\niter  15 value 4.273678\nfinal  value 4.273678 \nconverged\ninitial  value 4.266043 \niter   2 value 4.256737\niter   3 value 4.254624\niter   4 value 4.252599\niter   5 value 4.251541\niter   6 value 4.251472\niter   7 value 4.251461\niter   7 value 4.251461\niter   7 value 4.251461\nfinal  value 4.251461 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.7364  0.7364    1.4144\ns.e.   0.0307  0.0305  298.6289\n\nsigma^2 estimated as 4849:  log likelihood = -4133.72,  aic = 8275.44\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.7364   0.0307 -56.5543  0.0000\nma2        0.7364   0.0305  24.1261  0.0000\nconstant   1.4144 298.6289   0.0047  0.9962\n\n$AIC\n[1] 11.35177\n\n$AICc\n[1] 11.35182\n\n$BIC\n[1] 11.37697\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n$(1 - (-1.7364)B) (1 - 0B^{365})(1 - 1B^{365}) (1 - )(1 - ^{365}) y_t = 1.4144 + (1 + 0.7364)(1 + 0^{365}) a_t $\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  640.2968  649.1412  721.9856  674.8300  645.6744  820.5188  744.3632\n [8]  705.2076  754.0520  698.8964  774.7408  792.5852  715.4296  826.2741\n[15]  848.1185  693.9629  772.8073  825.6517  817.4961  882.3405  878.1849\n[22]  945.0293  980.8737  922.7181  966.5625  994.4070  985.2514 1010.0958\n[29]  982.9402  950.7846  988.6290  889.4734  827.3178  908.1622  892.0066\n[36]  911.8510  837.6954  872.5399  766.3843  753.2287  804.0731  782.9175\n[43]  842.7619  906.6063  854.4507  730.2951  757.1395  729.9839  714.8283\n[50]  693.6728  685.5172  637.3616\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  69.68378  72.08829  74.42140  76.68961  78.89854  81.05302  83.15729\n [8]  85.21507  87.22964  89.20394  91.14059  93.04193  94.91008  96.74698\n[15]  98.55436 100.33382 102.08683 103.81471 105.51872 107.19997 108.85954\n[22] 110.49839 112.11743 113.71752 115.29944 116.86392 118.41166 119.94331\n[29] 121.45947 122.96072 124.44760 125.92062 127.38025 128.82696 130.26117\n[36] 131.68329 133.09371 134.49280 135.88091 137.25837 138.62549 139.98259\n[43] 141.32994 142.66784 143.99653 145.31627 146.62731 147.92987 149.22418\n[50] 150.51044 151.78888 153.05967\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = TRUE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.02046036 64.07666 50.25831 -0.3954723 6.906063 0.7486566\n                   ACF1\nTraining set -0.4480382\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                  ME     RMSE      MAE        MPE     MAPE MASE      ACF1\nTraining set 6.21751 86.10674 67.13133 0.05411534 9.227552    1 0.4941184\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                       ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set 4.185113e-14 94.61017 75.80391 -1.709056 10.59072 1.129188\n                  ACF1\nTraining set 0.7704864\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 1.776053 67.19183 51.33838 -0.1232156 7.139174 0.7647456\n                    ACF1\nTraining set 0.004121855\n\n\nBased on the accuracy metrics, The SARIMA model outperforms all methods, as seen in lower RMSE, MAE, and MASE, except the Naive method.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE)+\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly total violent crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots-1",
    "href": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for Robbery weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for Robbery assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.6069, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#arima-model-1",
    "href": "ARMA_ARIMA_SARIMA_Models.html#arima-model-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Parameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n7624.574\n7633.898\n7624.589\n\n\n0\n2\n0\n8393.258\n8397.918\n8393.263\n\n\n0\n1\n1\n7461.431\n7475.417\n7461.462\n\n\n0\n2\n1\n7622.490\n7631.812\n7622.506\n\n\n0\n1\n2\n7461.493\n7480.140\n7461.544\n\n\n0\n2\n2\n7460.986\n7474.968\n7461.017\n\n\n0\n1\n3\n7460.423\n7483.733\n7460.501\n\n\n0\n2\n3\n7461.121\n7479.764\n7461.173\n\n\n1\n1\n0\n7518.654\n7532.640\n7518.685\n\n\n1\n2\n0\n8042.365\n8051.687\n8042.381\n\n\n1\n1\n1\n7461.893\n7480.541\n7461.945\n\n\n1\n2\n1\n7517.322\n7531.303\n7517.353\n\n\n1\n1\n2\n7462.607\n7485.916\n7462.684\n\n\n1\n2\n2\n7461.512\n7480.155\n7461.564\n\n\n1\n1\n3\n7458.304\n7486.275\n7458.413\n\n\n1\n2\n3\n7462.221\n7485.524\n7462.298\n\n\n2\n1\n0\n7475.489\n7494.136\n7475.540\n\n\n2\n2\n0\n7865.554\n7879.535\n7865.585\n\n\n2\n1\n1\n7460.521\n7483.830\n7460.598\n\n\n2\n2\n1\n7474.638\n7493.281\n7474.690\n\n\n2\n1\n2\n7457.705\n7485.676\n7457.813\n\n\n2\n2\n2\n7460.096\n7483.399\n7460.174\n\n\n2\n1\n3\n7459.709\n7492.342\n7459.853\n\n\n2\n2\n3\n7464.949\n7492.913\n7465.058\n\n\n3\n1\n0\n7460.004\n7483.313\n7460.081\n\n\n3\n2\n0\n7748.305\n7766.947\n7748.356\n\n\n3\n1\n1\n7460.069\n7488.040\n7460.177\n\n\n3\n2\n1\n7459.452\n7482.755\n7459.529\n\n\n3\n1\n2\n7459.705\n7492.338\n7459.850\n\n\n3\n2\n2\n7459.620\n7487.584\n7459.729\n\n\n3\n1\n3\n7461.674\n7498.969\n7461.861\n\n\n3\n2\n3\n7460.246\n7492.870\n7460.391\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n21 2 1 2 7457.705 7485.676 7457.813\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 2 2 7460.986 7474.968 7461.017\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n21 2 1 2 7457.705 7485.676 7457.813\n\n\n\nBased on the results, the ARIMA(0,2,2) and ARIMA(2,1,2) are the best models.\n\n\n\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 3.953185 \niter   2 value 3.647913\niter   3 value 3.598719\niter   4 value 3.507897\niter   5 value 3.491709\niter   6 value 3.474912\niter   7 value 3.452684\niter   8 value 3.421456\niter   9 value 3.420827\niter  10 value 3.420588\niter  11 value 3.420495\niter  12 value 3.420494\niter  13 value 3.420470\niter  14 value 3.420467\niter  14 value 3.420467\niter  14 value 3.420467\nfinal  value 3.420467 \nconverged\ninitial  value 3.385960 \niter   2 value 3.377893\niter   3 value 3.363438\niter   4 value 3.358597\niter   5 value 3.354549\niter   6 value 3.353925\niter   7 value 3.353822\niter   8 value 3.353782\niter   9 value 3.353780\niter   9 value 3.353780\niter   9 value 3.353780\nfinal  value 3.353780 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5092  0.5092\ns.e.   0.0310  0.0308\n\nsigma^2 estimated as 809.8:  log likelihood = -3727.49,  aic = 7460.99\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5092 0.0310 -48.7334       0\nma2   0.5092 0.0308  16.5328       0\n\n$AIC\n[1] 9.553119\n\n$AICc\n[1] 9.553139\n\n$BIC\n[1] 9.571021\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = \\varepsilon_t - 1.5092\\varepsilon_{t-1} + 0.5092\\varepsilon_{t-2}\\)\n\nARIMA(2,1,2):\n\n\nCode\n# Model diagnostics for ARIMA(2,1,2)\nsarima(w_ts, 2, 1, 2)\n\n\ninitial  value 3.452679 \niter   2 value 3.373309\niter   3 value 3.346679\niter   4 value 3.345842\niter   5 value 3.345566\niter   6 value 3.345486\niter   7 value 3.345462\niter   8 value 3.345323\niter   9 value 3.345223\niter  10 value 3.345005\niter  11 value 3.344525\niter  12 value 3.341900\niter  13 value 3.341567\niter  14 value 3.340183\niter  15 value 3.339339\niter  16 value 3.339226\niter  17 value 3.339191\niter  18 value 3.338602\niter  19 value 3.338058\niter  20 value 3.337507\niter  21 value 3.337080\niter  22 value 3.336923\niter  23 value 3.336898\niter  24 value 3.336897\niter  25 value 3.336896\niter  26 value 3.336895\niter  27 value 3.336894\niter  28 value 3.336893\niter  29 value 3.336892\niter  30 value 3.336892\niter  30 value 3.336892\niter  30 value 3.336892\nfinal  value 3.336892 \nconverged\ninitial  value 3.341959 \niter   2 value 3.341946\niter   3 value 3.341846\niter   4 value 3.341794\niter   5 value 3.341784\niter   6 value 3.341776\niter   7 value 3.341774\niter   8 value 3.341771\niter   9 value 3.341764\niter  10 value 3.341753\niter  11 value 3.341745\niter  12 value 3.341742\niter  12 value 3.341742\nfinal  value 3.341742 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2  constant\n      0.8624  -0.2071  -1.3525  0.5491   -0.1371\ns.e.  0.1601   0.0660   0.1552  0.0968    0.5768\n\nsigma^2 estimated as 798.7:  log likelihood = -3722.85,  aic = 7457.7\n\n$degrees_of_freedom\n[1] 777\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.8624 0.1601  5.3855  0.0000\nar2       -0.2071 0.0660 -3.1392  0.0018\nma1       -1.3525 0.1552 -8.7146  0.0000\nma2        0.5491 0.0968  5.6727  0.0000\nconstant  -0.1371 0.5768 -0.2376  0.8122\n\n$AIC\n[1] 9.536707\n\n$AICc\n[1] 9.536806\n\n$BIC\n[1] 9.572476\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.8624 \\cdot Y_{t-1} - 0.2071 \\cdot Y_{t-2} - 1.3525 \\cdot \\varepsilon_{t-1} + 0.5491 \\cdot \\varepsilon_t - 0.1371 + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n# Cross-validation for ARIMA(2,1,2)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(2, 1, 2))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(0,2,2)\", \"ARIMA(2,1,2)\"),\n                       MSE = c(mse_arima_022, mse_arima_123))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(2, 1, 2)due to significantly less MSE.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n# auto.arima(w_ts)\n\n\n\nI’m having a weird issue with auto.arima() for this data, it will run for a long time and crash.\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 2,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 235.8106 232.6806 229.3520 227.0825 225.7673 225.0558 224.6674 224.4324\n [9] 224.2630 224.1183 223.9814 223.8460 223.7104 223.5742 223.4376 223.3008\n[17] 223.1638 223.0267 222.8897 222.7526 222.6156 222.4785 222.3414 222.2044\n[25] 222.0673 221.9302 221.7932 221.6561 221.5190 221.3820 221.2449 221.1078\n[33] 220.9708 220.8337 220.6966 220.5596 220.4225 220.2854 220.1484 220.0113\n[41] 219.8742 219.7372 219.6001 219.4631 219.3260 219.1889 219.0519 218.9148\n[49] 218.7777 218.6407 218.5036 218.3665\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  28.26175  31.72350  33.96419  36.37983  39.08449  41.92824  44.76714\n [8]  47.52142  50.16110  52.68264  55.09352  57.40440  59.62587  61.76734\n[15]  63.83687  65.84129  67.78642  69.67725  71.51809  73.31272  75.06446\n[22]  76.77624  78.45068  80.09012  81.69666  83.27222  84.81851  86.33712\n[29]  87.82947  89.29689  90.74057  92.16165  93.56114  94.94001  96.29913\n[36]  97.63934  98.96140 100.26603 101.55390 102.82564 104.08184 105.32307\n[43] 106.54983 107.76263 108.96193 110.14817 111.32178 112.48313 113.63262\n[50] 114.77060 115.89741 117.01336\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 18752, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 275.48, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 3949.8, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#sarima-model-1",
    "href": "ARMA_ARIMA_SARIMA_Models.html#sarima-model-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*30),nrow=30)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n8277.657\n8282.249\n8277.663\n\n\n0\n2\n0\n0\n1\n1\n7982.248\n7991.431\n7982.265\n\n\n0\n2\n0\n0\n1\n2\n7982.172\n7995.947\n7982.205\n\n\n0\n2\n0\n1\n1\n0\n8114.764\n8123.947\n8114.781\n\n\n0\n2\n0\n1\n1\n1\n7982.000\n7995.775\n7982.033\n\n\n0\n2\n0\n2\n1\n0\n8055.161\n8068.936\n8055.194\n\n\n0\n2\n1\n0\n1\n0\n7509.593\n7518.777\n7509.610\n\n\n0\n2\n1\n0\n1\n1\n7237.758\n7251.533\n7237.791\n\n\n0\n2\n1\n1\n1\n0\n7351.205\n7364.980\n7351.238\n\n\n0\n2\n2\n0\n1\n0\n7232.957\n7246.732\n7232.990\n\n\n1\n2\n0\n0\n1\n0\n7882.372\n7891.556\n7882.389\n\n\n1\n2\n0\n0\n1\n1\n7625.037\n7638.812\n7625.070\n\n\n1\n2\n0\n1\n1\n0\n7734.299\n7748.074\n7734.332\n\n\n1\n2\n1\n0\n1\n0\n7346.625\n7360.400\n7346.658\n\n\n2\n2\n0\n0\n1\n0\n7707.024\n7720.799\n7707.057\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC    AICc\n10 0 2 2 0 1 0 7232.957 7246.732 7232.99\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 4.255412 \niter   2 value 3.907034\niter   3 value 3.860431\niter   4 value 3.818601\niter   5 value 3.780280\niter   6 value 3.746067\niter   7 value 3.719049\niter   8 value 3.708119\niter   9 value 3.699872\niter  10 value 3.691693\niter  11 value 3.652339\niter  12 value 3.605043\niter  13 value 3.589995\niter  14 value 3.588368\niter  15 value 3.582469\niter  16 value 3.582329\niter  17 value 3.581877\niter  18 value 3.581858\niter  19 value 3.581858\niter  19 value 3.581858\niter  19 value 3.581858\nfinal  value 3.581858 \nconverged\ninitial  value 3.563704 \niter   2 value 3.555266\niter   3 value 3.546826\niter   4 value 3.545355\niter   5 value 3.543937\niter   6 value 3.542426\niter   7 value 3.538641\niter   8 value 3.537907\niter   9 value 3.537832\niter  10 value 3.537825\niter  11 value 3.537825\niter  12 value 3.537824\niter  13 value 3.537823\niter  13 value 3.537823\niter  13 value 3.537823\nfinal  value 3.537823 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.7044  0.7044    0.4988\ns.e.   0.0290  0.0288  161.4600\n\nsigma^2 estimated as 1164:  log likelihood = -3613.48,  aic = 7234.96\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.7044   0.0290 -58.6910  0.0000\nma2        0.7044   0.0288  24.4246  0.0000\nconstant   0.4988 161.4600   0.0031  0.9975\n\n$AIC\n[1] 9.924498\n\n$AICc\n[1] 9.924543\n\n$BIC\n[1] 9.949692\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n\\((1 - (-1.7044)B) (1 - 0B^{365})(1 - 1B^{365}) y_t = 0.4988 + (1 + 0.7044\\nabla)(1 + 0\\nabla^{365}) a_t\\)\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 201.3764 217.2888 246.2012 217.1136 190.0260 249.9383 265.8507 250.7631\n [9] 235.6755 213.5878 228.5002 229.4126 226.3250 263.2374 228.1497 224.0621\n[17] 224.9745 266.8869 239.7993 263.7116 286.6240 304.5364 322.4488 286.3612\n[25] 308.2735 331.1859 305.0983 345.0107 330.9230 331.8354 321.7478 307.6602\n[33] 301.5726 311.4849 288.3973 297.3097 293.2221 320.1345 267.0468 264.9592\n[41] 251.8716 265.7840 289.6964 283.6087 334.5211 250.4335 269.3459 243.2582\n[49] 223.1706 216.0830 210.9954 208.9078\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 34.14280 35.61674 37.03585 38.40621 39.73286 41.02004 42.27136 43.48991\n [9] 44.67838 45.83911 46.97414 48.08531 49.17423 50.24234 51.29095 52.32122\n[17] 53.33423 54.33093 55.31221 56.27888 57.23167 58.17127 59.09831 60.01336\n[25] 60.91697 61.80964 62.69184 63.56401 64.42654 65.27983 66.12423 66.96007\n[33] 67.78768 68.60735 69.41937 70.22399 71.02147 71.81205 72.59595 73.37339\n[41] 74.14457 74.90968 75.66891 76.42244 77.17042 77.91303 78.65040 79.38270\n[49] 80.11005 80.83259 81.55045 82.26375\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = FALSE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.08951407 31.61258 24.53964 -0.6673075 8.861099 0.6429793\n                   ACF1\nTraining set -0.3584976\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                    ME     RMSE      MAE       MPE     MAPE MASE      ACF1\nTraining set -4.157319 47.55967 38.16553 -2.903463 14.42754    1 0.6184068\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                        ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set -2.161022e-14 63.25961 51.22099 -5.375413 19.44024 1.342075\n                  ACF1\nTraining set 0.8748357\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.6029735 32.92209 25.10713 -0.3763132 9.483319 0.6578483\n                   ACF1\nTraining set 0.01864713\n\n\nBased on the accuracy metrics, The SARIMA model,outperforms all methods, as seen in lower RMSE, MAE, and MASE, except the Naive method.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE) +\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly robbery crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots-2",
    "href": "ARMA_ARIMA_SARIMA_Models.html#acfpacf-plots-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Code\nw_ts_d &lt;- w_ts %&gt;% diff()\nggAcf(w_ts_d, 50) + ggtitle(\"ACF Plot for Aggragated Assault weekly data in NYC\")\n\n\n\n\n\nCode\nggPacf(w_ts_d, 50) + ggtitle(\"PACF Plot for Aggragated Assault assault weekly data in NYC\")\n\n\n\n\n\nCode\nautoplot(w_ts_d)\n\n\n\n\n\nCode\ntseries::adf.test(w_ts_d)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  w_ts_d\nDickey-Fuller = -8.2398, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nP-value: The p-value is 0.01, which is less than the common significance level of 0.05. Therefore, you would reject the null hypothesis."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#arima-model-2",
    "href": "ARMA_ARIMA_SARIMA_Models.html#arima-model-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Parameters: Based on the ACF plot, q = 0, 1, 2, 3, from the PACF plot, p = 0, 1, 2, 3, d = 1, 2, now it is time to fit the model.\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4)# p=0,1,2,3 : 4\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1:2)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(w_ts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n8084.924\n8094.248\n8084.939\n\n\n0\n2\n0\n8910.079\n8914.740\n8910.084\n\n\n0\n1\n1\n7834.385\n7848.370\n7834.415\n\n\n0\n2\n1\n8082.252\n8091.573\n8082.267\n\n\n0\n1\n2\n7832.778\n7851.426\n7832.830\n\n\n0\n2\n2\n7833.763\n7847.745\n7833.794\n\n\n0\n1\n3\n7831.200\n7854.509\n7831.277\n\n\n0\n2\n3\n7832.073\n7850.715\n7832.125\n\n\n1\n1\n0\n7900.949\n7914.934\n7900.980\n\n\n1\n2\n0\n8466.254\n8475.576\n8466.270\n\n\n1\n1\n1\n7833.431\n7852.079\n7833.483\n\n\n1\n2\n1\n7899.270\n7913.252\n7899.301\n\n\n1\n1\n2\n7825.765\n7849.075\n7825.843\n\n\n1\n2\n2\n7832.749\n7851.391\n7832.801\n\n\n1\n1\n3\n7826.952\n7854.923\n7827.060\n\n\n1\n2\n3\n7837.605\n7860.908\n7837.683\n\n\n2\n1\n0\n7847.823\n7866.471\n7847.875\n\n\n2\n2\n0\n8252.170\n8266.151\n8252.200\n\n\n2\n1\n1\n7833.108\n7856.418\n7833.186\n\n\n2\n2\n1\n7846.678\n7865.320\n7846.730\n\n\n2\n1\n2\n7826.858\n7854.829\n7826.967\n\n\n2\n2\n2\n7832.387\n7855.690\n7832.464\n\n\n2\n1\n3\n7829.143\n7861.776\n7829.288\n\n\n2\n2\n3\n7825.737\n7853.700\n7825.845\n\n\n3\n1\n0\n7834.207\n7857.517\n7834.285\n\n\n3\n2\n0\n8126.832\n8145.475\n8126.884\n\n\n3\n1\n1\n7833.883\n7861.854\n7833.991\n\n\n3\n2\n1\n7833.344\n7856.647\n7833.422\n\n\n3\n1\n2\n7828.243\n7860.876\n7828.387\n\n\n3\n2\n2\n7833.144\n7861.108\n7833.253\n\n\n3\n1\n3\n7828.712\n7866.007\n7828.899\n\n\n3\n2\n3\n7827.186\n7859.810\n7827.331\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC    BIC     AICc\n24 2 2 3 7825.737 7853.7 7825.845\n\n\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 0 2 2 7833.763 7847.745 7833.794\n\n\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n13 1 1 2 7825.765 7849.075 7825.843\n\n\n\nBased on the results, the ARIMA(2,2,3) and ARIMA(0,2,2), ARIMA(1,1,2) are the best models.\n\n\n\n\nARIMA(2,2,3):\n\n\nCode\n# Model diagnostics for ARIMA(2,2,3)\nsarima(w_ts, 2, 2, 3)\n\n\ninitial  value 4.285103 \niter   2 value 3.929403\niter   3 value 3.801131\niter   4 value 3.719204\niter   5 value 3.699748\niter   6 value 3.683616\niter   7 value 3.679915\niter   8 value 3.600624\niter   9 value 3.598777\niter  10 value 3.596603\niter  11 value 3.596014\niter  12 value 3.595351\niter  13 value 3.595307\niter  14 value 3.593381\niter  15 value 3.592583\niter  16 value 3.591682\niter  17 value 3.590865\niter  18 value 3.590420\niter  19 value 3.589676\niter  20 value 3.585557\niter  21 value 3.585318\niter  22 value 3.585297\niter  23 value 3.584762\niter  24 value 3.584302\niter  25 value 3.583853\niter  26 value 3.583131\niter  27 value 3.582996\niter  28 value 3.582781\niter  29 value 3.582434\niter  30 value 3.582175\niter  31 value 3.582074\niter  31 value 3.582074\nfinal  value 3.582074 \nconverged\ninitial  value 3.589659 \niter   2 value 3.589092\niter   3 value 3.589068\niter   4 value 3.589053\niter   5 value 3.589050\niter   6 value 3.589048\niter   7 value 3.589024\niter   8 value 3.588981\niter   9 value 3.588864\niter  10 value 3.588555\niter  11 value 3.588212\niter  12 value 3.588030\niter  13 value 3.587849\niter  14 value 3.587657\niter  15 value 3.587569\niter  16 value 3.587522\niter  17 value 3.587516\niter  18 value 3.587508\niter  19 value 3.587480\niter  20 value 3.587404\niter  21 value 3.587179\niter  22 value 3.586828\niter  23 value 3.586637\niter  24 value 3.586565\niter  25 value 3.585917\niter  26 value 3.585636\niter  27 value 3.585443\niter  28 value 3.585422\niter  29 value 3.585356\niter  30 value 3.585181\niter  31 value 3.584364\niter  32 value 3.584095\niter  33 value 3.583645\niter  34 value 3.583643\niter  35 value 3.583625\niter  36 value 3.583463\niter  37 value 3.583460\niter  37 value 3.583460\niter  38 value 3.583460\niter  39 value 3.583454\niter  39 value 3.583454\niter  39 value 3.583454\nfinal  value 3.583454 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3\n      0.7601  -0.0628  -2.3910  1.9506  -0.5596\ns.e.  0.1013   0.0613   0.0949  0.1539   0.0632\n\nsigma^2 estimated as 1282:  log likelihood = -3906.87,  aic = 7825.74\n\n$degrees_of_freedom\n[1] 776\n\n$ttable\n    Estimate     SE  t.value p.value\nar1   0.7601 0.1013   7.5056  0.0000\nar2  -0.0628 0.0613  -1.0236  0.3063\nma1  -2.3910 0.0949 -25.1964  0.0000\nma2   1.9506 0.1539  12.6758  0.0000\nma3  -0.5596 0.0632  -8.8480  0.0000\n\n$AIC\n[1] 10.02015\n\n$AICc\n[1] 10.02025\n\n$BIC\n[1] 10.05595\n\n\n\nThe model diagnostics tell us that the most parameters are statistically significant except for ar2, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\\(Y_t = 0.7601 \\cdot Y_{t-1} - 0.0628 \\cdot Y_{t-2} - 2.3910 \\cdot \\varepsilon_{t-1} + 1.9506 \\cdot \\varepsilon_{t-2} - 0.5596 \\cdot \\varepsilon_{t-3} + \\varepsilon_t\\)\n\nARIMA(0,2,2):\n\n\nCode\n# Model diagnostics for ARIMA(0,2,2)\nsarima(w_ts, 0, 2, 2)\n\n\ninitial  value 4.284057 \niter   2 value 3.918786\niter   3 value 3.715856\niter   4 value 3.709338\niter   5 value 3.703956\niter   6 value 3.633458\niter   7 value 3.631828\niter   8 value 3.614513\niter   9 value 3.612157\niter  10 value 3.611056\niter  11 value 3.610938\niter  12 value 3.610854\niter  13 value 3.610785\niter  14 value 3.610757\niter  15 value 3.610753\niter  15 value 3.610753\niter  15 value 3.610753\nfinal  value 3.610753 \nconverged\ninitial  value 3.603018 \niter   2 value 3.598993\niter   3 value 3.594341\niter   4 value 3.594119\niter   5 value 3.592452\niter   6 value 3.592438\niter   7 value 3.592434\niter   8 value 3.592434\niter   9 value 3.592434\niter   9 value 3.592434\niter   9 value 3.592434\nfinal  value 3.592434 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2\n      -1.5777  0.5777\ns.e.   0.0263  0.0261\n\nsigma^2 estimated as 1305:  log likelihood = -3913.88,  aic = 7833.76\n\n$degrees_of_freedom\n[1] 779\n\n$ttable\n    Estimate     SE  t.value p.value\nma1  -1.5777 0.0263 -60.0933       0\nma2   0.5777 0.0261  22.1683       0\n\n$AIC\n[1] 10.03043\n\n$AICc\n[1] 10.03045\n\n$BIC\n[1] 10.04833\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = -1.5777 \\cdot \\varepsilon_{t-1} + 0.5777 \\cdot \\varepsilon_{t-2} + \\varepsilon_t\\)\nARIMA(1,1,2):\n\n\nCode\n# Model diagnostics for ARIMA(1,1,2)\nsarima(w_ts, 1, 1, 2)\n\n\ninitial  value 3.748440 \niter   2 value 3.632828\niter   3 value 3.586798\niter   4 value 3.586611\niter   5 value 3.585291\niter   6 value 3.585269\niter   7 value 3.585262\niter   8 value 3.585215\niter   9 value 3.585108\niter  10 value 3.584767\niter  11 value 3.584462\niter  12 value 3.584337\niter  13 value 3.584304\niter  14 value 3.584291\niter  15 value 3.584110\niter  16 value 3.583659\niter  17 value 3.582707\niter  18 value 3.582548\niter  19 value 3.581932\niter  20 value 3.581468\niter  21 value 3.581396\niter  22 value 3.581329\niter  23 value 3.581025\niter  24 value 3.580314\niter  25 value 3.580085\niter  26 value 3.579960\niter  27 value 3.579528\niter  28 value 3.579202\niter  29 value 3.578829\niter  30 value 3.578738\niter  31 value 3.578569\niter  32 value 3.578518\niter  33 value 3.578511\niter  34 value 3.578511\niter  34 value 3.578511\niter  34 value 3.578511\nfinal  value 3.578511 \nconverged\ninitial  value 3.578447 \niter   2 value 3.578444\niter   3 value 3.578431\niter   4 value 3.578425\niter   5 value 3.578413\niter   6 value 3.578392\niter   7 value 3.578369\niter   8 value 3.578357\niter   9 value 3.578354\niter  10 value 3.578354\niter  10 value 3.578354\nfinal  value 3.578354 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2  constant\n      0.7518  -1.3994  0.5381    0.1778\ns.e.  0.0893   0.0862  0.0533    0.7144\n\nsigma^2 estimated as 1282:  log likelihood = -3907.88,  aic = 7825.77\n\n$degrees_of_freedom\n[1] 778\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.7518 0.0893   8.4176  0.0000\nma1       -1.3994 0.0862 -16.2393  0.0000\nma2        0.5381 0.0533  10.0876  0.0000\nconstant   0.1778 0.7144   0.2488  0.8036\n\n$AIC\n[1] 10.00737\n\n$AICc\n[1] 10.00744\n\n$BIC\n[1] 10.03718\n\n\n\nThe model diagnostics tell us that the parameters are statistically significant since the p-value is smaller than 0.05, the ACF of residuals and Q-Q plot tells us the residuals are normally distributed and have constant variance, also p-values for Ljung-Box test has shown results that are above 0.05.\n\n\\(Y_t = 0.7518 \\cdot Y_{t-1} - 1.3994 \\cdot \\varepsilon_{t-1} + 0.5381 \\cdot \\varepsilon_{t-2} + 0.1778 + \\varepsilon_t\\)\n\n\n\n\n\nCode\narima_forecast &lt;- function(y, h, order) {\n  fit &lt;- Arima(y, order = order)\n  forecast(fit, h = h)\n}\nh = 52\n\n# Cross-validation for ARIMA(2,2,3)\ncv_arima_022 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(2, 2, 3))\n\n# Cross-validation for ARIMA(0,2,2)\ncv_arima_123 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(0, 2, 2))\n\n\n# Cross-validation for ARIMA(1,1,2)\ncv_arima_112 &lt;- tsCV(w_ts, arima_forecast, h = h, order = c(1, 1, 2))\n\n# Calculate MSE\nmse_arima_022 &lt;- mean(cv_arima_022^2, na.rm = TRUE)\nmse_arima_123 &lt;- mean(cv_arima_123^2, na.rm = TRUE)\nmse_arima_112 &lt;- mean(cv_arima_112^2, na.rm = TRUE)\n\n# Create a data frame for ggplot\nmse_data &lt;- data.frame(Model = c(\"ARIMA(2,2,3)\", \"ARIMA(0,2,2)\", \"ARIMA(1,1,2)\"),\n                       MSE = c(mse_arima_022, mse_arima_123,mse_arima_112 ))\n\n# Plotting with ggplot2\nggplot(mse_data, aes(x = Model, y = MSE, fill = Model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"MSE of ARIMA Models\", x = \"Model\", y = \"MSE\") +\n  theme_minimal()\n\n\n\n\n\n\nBased on the cross-validation, I will chose ARIMA(1,1,2)due to significantly less MSE compared with the other 2.\n\n\n\n\n\n\nCode\n# Use auto.arima() to generate model parameters\n#auto.arima(w_ts)\n\n\n\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using ARIMA\nsarima.for(w_ts, 52, 1,1,2)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 427.9520 424.1782 421.3854 419.3300 417.8289 416.7446 415.9736 415.4381\n [9] 415.0796 414.8543 414.7290 414.6790 414.6855 414.7345 414.8155 414.9205\n[17] 415.0436 415.1802 415.3271 415.4816 415.6419 415.8065 415.9744 416.1448\n[25] 416.3170 416.4905 416.6651 416.8405 417.0165 417.1930 417.3697 417.5467\n[33] 417.7239 417.9013 418.0787 418.2562 418.4338 418.6114 418.7891 418.9668\n[41] 419.1445 419.3222 419.5000 419.6777 419.8555 420.0332 420.2110 420.3887\n[49] 420.5665 420.7442 420.9220 421.0998\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1]  35.80264  37.95971  40.61541  43.58851  46.73470  49.95144  53.17083\n [8]  56.35021  59.46430  62.49929  65.44879  68.31108  71.08731  73.78034\n[15]  76.39393  78.93228  81.39974  83.80060  86.13897  88.41876  90.64359\n[22]  92.81684  94.94165  97.02087  99.05715 101.05292 103.01041 104.93166\n[29] 106.81857 108.67288 110.49618 112.28997 114.05560 115.79437 117.50744\n[36] 119.19592 120.86083 122.50313 124.12371 125.72341 127.30302 128.86327\n[43] 130.40486 131.92843 133.43462 134.92399 136.39710 137.85447 139.29659\n[50] 140.72394 142.13695 143.53606\n\n\n\nThis result forecasts the crimes after one year of 2022, it does not have the expected wave-like forecasts, I believe this has to do with no seasonal component.\n\n\n\n\n\nMean model\n\n\n\nCode\nf1&lt;-meanf(w_ts, h=13) #mean\ncheckresiduals(f1)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 12007, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nNative method\n\n\n\nCode\nf2&lt;-naive(w_ts, h=13) \ncheckresiduals(f2)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 292.38, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nSeasonal Naive\n\n\n\nCode\nf3&lt;-snaive(w_ts, h=13)\ncheckresiduals(f3)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 1096.4, df = 104, p-value &lt; 2.2e-16\n\nModel df: 0.   Total lags used: 104\n\n\n\nFrom the benchmark method, only the Naive model illustrates a constant variance from the residuals plots and the ACF plots, the other benchmark models all resemble a correlation among their residuals."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#sarima-model-2",
    "href": "ARMA_ARIMA_SARIMA_Models.html#sarima-model-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "For the SARIMA model, we will begin by examining various degrees of seasonal lags for the seasonal effects through an analysis of Both ACF and PACF plots.\n\n\nCode\n# Combinnation of both first order differencing and seasonal differencing.\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggAcf(50)\n\n\n\n\n\n\n\nCode\nw_ts %&gt;% diff() %&gt;% diff(lag=52) %&gt;% ggPacf(50)\n\n\n\n\n\nBased on the ACF/PACF plots\n\nHere we can choose p = 0, 1, 2, 3, 4, 5, q = 0, 1, 2, 3, d = 1, 2, P = 1, 2, 3, Q = 1, 2, 3, D = 1, 2.\nLooping through model parameters for SARIMA\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=2\n  D=1\n  s=52\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*30),nrow=30)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q&lt;=9)\n          {\n            \n            model&lt;- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput=SARIMA.c(p1=1,p2=6,q1=1,q2=4,P1=1,P2=4,Q1=1,Q2=4,data=w_ts)\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n0\n0\n1\n0\n8799.569\n8804.161\n8799.574\n\n\n0\n2\n0\n0\n1\n1\n8465.215\n8474.398\n8465.231\n\n\n0\n2\n0\n0\n1\n2\n8467.093\n8480.868\n8467.126\n\n\n0\n2\n0\n1\n1\n0\n8600.064\n8609.248\n8600.081\n\n\n0\n2\n0\n1\n1\n1\n8467.078\n8480.853\n8467.111\n\n\n0\n2\n0\n2\n1\n0\n8533.393\n8547.168\n8533.426\n\n\n0\n2\n1\n0\n1\n0\n7997.299\n8006.483\n7997.316\n\n\n0\n2\n1\n0\n1\n1\n7677.132\n7690.907\n7677.166\n\n\n0\n2\n1\n1\n1\n0\n7806.460\n7820.235\n7806.493\n\n\n0\n2\n2\n0\n1\n0\n7630.988\n7644.763\n7631.021\n\n\n1\n2\n0\n0\n1\n0\n8334.333\n8343.516\n8334.349\n\n\n1\n2\n0\n0\n1\n1\n8030.936\n8044.711\n8030.969\n\n\n1\n2\n0\n1\n1\n0\n8152.432\n8166.207\n8152.465\n\n\n1\n2\n1\n0\n1\n0\n7769.329\n7783.104\n7769.362\n\n\n2\n2\n0\n0\n1\n0\n8129.766\n8143.541\n8129.799\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nExtract the lowest AIC, BIC and AICc\n\n\n\nCode\n# Extract lowest AIC\noutput[which.min(output$AIC),] \n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\n\nCode\n# Extract lowest BIC\noutput[which.min(output$BIC),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\n\nCode\n# Extract lowest AICc\noutput[which.min(output$AICc),]\n\n\n   p d q P D Q      AIC      BIC     AICc\n10 0 2 2 0 1 0 7630.988 7644.763 7631.021\n\n\n\nThe ARIMA(0,2,2)(0,1,0)52 is the best model\n\n\n\n\n\n\nCode\n# Model diagnostics\nsarima(w_ts,0,2,2,0,1,0,52)\n\n\ninitial  value 4.613380 \niter   2 value 4.217159\niter   3 value 4.035586\niter   4 value 4.010065\niter   5 value 3.985332\niter   6 value 3.959958\niter   7 value 3.883472\niter   8 value 3.880224\niter   9 value 3.878306\niter  10 value 3.874143\niter  11 value 3.873777\niter  12 value 3.873763\niter  13 value 3.873763\niter  13 value 3.873763\niter  13 value 3.873763\nfinal  value 3.873763 \nconverged\ninitial  value 3.852327 \niter   2 value 3.839837\niter   3 value 3.830646\niter   4 value 3.828493\niter   5 value 3.826859\niter   6 value 3.825499\niter   7 value 3.821067\niter   8 value 3.811424\niter   9 value 3.810832\niter  10 value 3.810808\niter  10 value 3.810808\niter  10 value 3.810808\nfinal  value 3.810808 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     ma2  constant\n      -1.8179  0.8179    0.7798\ns.e.   0.0298  0.0296  195.0190\n\nsigma^2 estimated as 2006:  log likelihood = -3812.48,  aic = 7632.97\n\n$degrees_of_freedom\n[1] 726\n\n$ttable\n         Estimate       SE  t.value p.value\nma1       -1.8179   0.0298 -61.0486  0.0000\nma2        0.8179   0.0296  27.6262  0.0000\nconstant   0.7798 195.0190   0.0040  0.9968\n\n$AIC\n[1] 10.47047\n\n$AICc\n[1] 10.47051\n\n$BIC\n[1] 10.49566\n\n\n\nBased on the model diagnostics, all coefficients in the SARIMA(0,2,1)(0,1,1)52 model are statistically significant. Additionally, the ACF plots of the residuals, QQ plot, and Ljung-Box test indicate good normality of the residuals.\n\n\n\n\nForecast the assault weekly total crimes for the next year.\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nsarima.for(w_ts, 52, 0,2,2,0,1,0,52)\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 397.1870 384.1760 436.1650 417.1539 417.1429 494.1318 444.1208 404.1097\n [9] 467.0987 427.0877 492.0766 511.0656 446.0545 514.0435 568.0324 424.0214\n[17] 492.0104 502.9993 535.9883 561.9772 549.9662 586.9552 604.9441 594.9331\n[25] 598.9220 601.9110 615.8999 609.8889 601.8779 579.8668 606.8558 551.8447\n[33] 495.8337 541.8226 554.8116 573.8006 505.7895 512.7785 482.7674 441.7564\n[41] 510.7453 489.7343 511.7233 586.7122 490.7012 456.6901 467.6791 457.6680\n[49] 456.6570 447.6460 455.6349 412.6239\n\n$se\nTime Series:\nStart = c(2022, 4) \nEnd = c(2023, 3) \nFrequency = 52 \n [1] 44.81725 45.56558 46.30380 47.03241 47.75183 48.46247 49.16473 49.85894\n [9] 50.54544 51.22454 51.89654 52.56170 53.22028 53.87253 54.51866 55.15890\n[17] 55.79344 56.42249 57.04622 57.66480 58.27839 58.88716 59.49126 60.09081\n[25] 60.68597 61.27684 61.86357 62.44626 63.02503 63.59999 64.17122 64.73885\n[33] 65.30295 65.86363 66.42096 66.97503 67.52592 68.07370 68.61846 69.16027\n[41] 69.69918 70.23528 70.76861 71.29925 71.82726 72.35269 72.87559 73.39603\n[49] 73.91405 74.42971 74.94305 75.45412\n\n\n\n\nCode\n# Forecast the next year's weekly crimes using SARIMA\nfit &lt;- w_ts %&gt;%\n  Arima(order=c(0,2,2), seasonal=c(0,1,0),include.drift = FALSE)\n\nfit %&gt;% forecast(h=52) %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\n\nCode\n# Naive method\nnaive_model &lt;- naive(w_ts, h = 52)\n\n# Seasonal Naive method\nsnaive_model &lt;- snaive(w_ts, h = 52)\n\n# Average method\naverage_model &lt;- meanf(w_ts, h = 52)\n\n# SARIMA model\nsarima_forecast &lt;- forecast(fit, h = 52)\n\n# Print out accuracy metrics\ncat(\"Naive Method Accuracy:\\n\")\n\n\nNaive Method Accuracy:\n\n\nCode\nprint(accuracy(naive_model))\n\n\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.1483376 42.43183 33.99233 -0.6180055 9.17241 0.8663712\n                   ACF1\nTraining set -0.4599732\n\n\nCode\ncat(\"\\nSeasonal Naive Method Accuracy:\\n\")\n\n\n\nSeasonal Naive Method Accuracy:\n\n\nCode\nprint(accuracy(snaive_model))\n\n\n                   ME     RMSE      MAE      MPE    MAPE MASE      ACF1\nTraining set 11.80164 51.23555 39.23529 2.018993 10.1256    1 0.3281593\n\n\nCode\ncat(\"\\nAverage Method Accuracy:\\n\")\n\n\n\nAverage Method Accuracy:\n\n\nCode\nprint(accuracy(average_model))\n\n\n                       ME     RMSE      MAE      MPE     MAPE     MASE\nTraining set 2.345155e-14 69.96365 55.83841 -3.39109 15.14925 1.423168\n                  ACF1\nTraining set 0.8151155\n\n\nCode\ncat(\"\\nSARIMA Model Accuracy:\\n\")\n\n\n\nSARIMA Model Accuracy:\n\n\nCode\nprint(accuracy(sarima_forecast))\n\n\n                  ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.91809 43.21687 32.99462 -0.2909962 8.705878 0.8409423\n                      ACF1\nTraining set -0.0007675607\n\n\nBased on the accuracy metrics, The SARIMA model outperforms all other methods in RMSE, MAE, MPE, MAPE, and MASE.\n\n\nCode\n# Plot forecasts\nautoplot(w_ts) +\n  autolayer(meanf(w_ts, h = 52), series = \"Mean\", PI = FALSE) +\n  autolayer(naive(w_ts, h = 52), series = \"Naïve\", PI = FALSE) +\n  autolayer(snaive(w_ts, h = 52), series = \"Seasonal Naïve\", PI = FALSE) +\n  autolayer(forecast(fit, h = 52), series = \"SARIMA\", PI = FALSE) +\n  ggtitle(\"Forecasts for weekly aggregated assault crimes in NYC\") +\n  xlab(\"Year\") + ylab(\"Crimes\") +\n  guides(colour = guide_legend(title = \"Forecast\"))"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html",
    "href": "ARIMAX_SARIMAX_VAR.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In order to choose appropriate variables for ARIMAX (AutoRegressive Integrated Moving Average with exogenous variables), SARIMAX (Seasonal ARIMAX), and VAR (Vector Autoregression) models, it is essential to conduct a literature review and identify relevant factors that may influence the time series of interest. The articles provided earlier cover diverse aspects of crime and its determinants, and they can guide the selection of variables for modeling.\n\nUnemployment: The article edmark2005unemployment discussing the effects of unemployment on property crime rates in Swedish counties during the period 1988–1999 highlights the potential impact of economic factors on crime. Unemployment rates can be a crucial variable to consider in the models.\nDemographic Changes: The article Hipp (2007) exploring the effects of neighborhood inequality and heterogeneity on crime rates emphasizes the significance of racial/ethnic heterogeneity. Demographic variables, especially those related to the composition of the population, could be relevant.\nEducation: The study Groot and Brink (2007) investigating the effects of education on criminal behavior suggests a negative correlation between education and certain types of crime. Educational attainment might be an important variable to include in the models.\nIncome Inequality: The article Groot and Brink (2007) examining the effects of income inequality and heterogeneity on crime rates identifies income inequality as a significant factor. Including measures of income inequality may be crucial in understanding crime dynamics.\nSocial Norms: The article Groot and Brink (2007) studying the impact of education on norms and attitudes towards offenses and crime reveals that higher education levels are associated with more permissive attitudes. Social norms could be a valuable variable to consider.\nPandemic Influence: The article Boman and Gallupe (2020) explores the impact of the COVID-19 pandemic on crime rates in the United States. The unprecedented lockdowns and societal disruptions during the pandemic have likely introduced unique dynamics to criminal activities. Understanding the specific effects of the pandemic on crime becomes paramount in the analytical models."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#literature-review",
    "href": "ARIMAX_SARIMAX_VAR.html#literature-review",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "In order to choose appropriate variables for ARIMAX (AutoRegressive Integrated Moving Average with exogenous variables), SARIMAX (Seasonal ARIMAX), and VAR (Vector Autoregression) models, it is essential to conduct a literature review and identify relevant factors that may influence the time series of interest. The articles provided earlier cover diverse aspects of crime and its determinants, and they can guide the selection of variables for modeling.\n\nUnemployment: The article edmark2005unemployment discussing the effects of unemployment on property crime rates in Swedish counties during the period 1988–1999 highlights the potential impact of economic factors on crime. Unemployment rates can be a crucial variable to consider in the models.\nDemographic Changes: The article Hipp (2007) exploring the effects of neighborhood inequality and heterogeneity on crime rates emphasizes the significance of racial/ethnic heterogeneity. Demographic variables, especially those related to the composition of the population, could be relevant.\nEducation: The study Groot and Brink (2007) investigating the effects of education on criminal behavior suggests a negative correlation between education and certain types of crime. Educational attainment might be an important variable to include in the models.\nIncome Inequality: The article Groot and Brink (2007) examining the effects of income inequality and heterogeneity on crime rates identifies income inequality as a significant factor. Including measures of income inequality may be crucial in understanding crime dynamics.\nSocial Norms: The article Groot and Brink (2007) studying the impact of education on norms and attitudes towards offenses and crime reveals that higher education levels are associated with more permissive attitudes. Social norms could be a valuable variable to consider.\nPandemic Influence: The article Boman and Gallupe (2020) explores the impact of the COVID-19 pandemic on crime rates in the United States. The unprecedented lockdowns and societal disruptions during the pandemic have likely introduced unique dynamics to criminal activities. Understanding the specific effects of the pandemic on crime becomes paramount in the analytical models."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#variables-for-arimaxsarimax-models",
    "href": "ARIMAX_SARIMAX_VAR.html#variables-for-arimaxsarimax-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Variables for ARIMAX/SARIMAX Models",
    "text": "Variables for ARIMAX/SARIMAX Models\n\nDependent Variable: violent crimes.\nExogenous Variables: Unemployment Rates, Demographic Variables (e.g., Racial/Ethnic Composition), Educational Attainment, COVID-19 data.\nSeasonal Components (for SARIMAX):\n\nConsidering the seasonality of violent crimes in NYC, include seasonal components in SARIMAX models"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#variables-for-var-models",
    "href": "ARIMAX_SARIMAX_VAR.html#variables-for-var-models",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Variables for VAR Models",
    "text": "Variables for VAR Models\n\nMultiple Time Series Variables: Weekly violent Crimes, Unemployment Rates, Demographic Variables, Education Levels, climate data"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#models-overlook",
    "href": "ARIMAX_SARIMAX_VAR.html#models-overlook",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Models Overlook",
    "text": "Models Overlook\n\nModel 1 (ARIMAX/SARIMAX):\n\nDependent Variable: Violent crimes.\nIndependent Variables: COVID-19\n\nModel 2 (VAR):\n\nDependent Variable: Violent crimes.\nIndependent Variables: Education, Income Inequality.\n\nModel 3 (VAR):\n\nDependent Variable: Crimes.\nIndependent Variables: COVID-19.\n\nModel 4 (ARIMAX/SARIMAX):\n\nDependent Variable: assualt crimes.\nIndependent Variables: COVID-19\n\nModel 5 (VAR):\n\nDependent Variable: assault crimes.\nIndependent Variables: Education, Income Inequality.\n\nModel 6 (VAR):\n\nDependent Variable: assaut crimes.\nIndependent Variables: COVID-19.\n\n\n\nARIMAX with COVID-19 for total crimesVAR with Climate data and Unemployment rate for total crimesVAR with COVID-19 data for total crimesARIMAX with COVID-19 for aggravated assaultVAR model with unemploymnet rates and climate data with aggravated assaultVAR model with COVID-19 with aggravated assault\n\n\n\nOverview\nIn this model, we focus on analyzing the weekly total violent crimes in New York City using an ARIMAX model. The unprecedented global crisis brought on by the COVID-19 pandemic has had profound impacts on various facets of society, including crime patterns. Recognizing the potential influence of the pandemic on crime dynamics, our model integrates COVID-19 data as a critical exogenous component. This data is extracted using the covidcast package in R, which provides a comprehensive dataset of COVID-19 metrics.\n\n\nFitting Model With Auto.arima()\n\nLoad in violent crime data\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\nlibrary(covidcast)\nlibrary(zoo)\nlibrary(imputeTS)\nlibrary(vars)\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n\n\n\nCOVID-19 data extraction and cleaning\n\nExtract COVID-19 data using covid_cast package in R\n\n\n\nCode\n# Fetching COVID-19 cases data for New York City for the specified timeframe\ncases_nyc &lt;- covidcast_signal(\"jhu-csse\", \"confirmed_incidence_num\",\n                              start_day = \"2020-01-01\", end_day = \"2022-12-31\",\n                              geo_type = \"county\", geo_values = \"36061\") # 36061 is the FIPS code for New York County\n\n# Cleaning and preparing the data\ncases_nyc_cleaned &lt;- cases_nyc %&gt;%\n  dplyr::select(time_value, value) %&gt;%\n  dplyr::arrange(time_value)\n\n# Ensure the date column is in the Date format\ncases_nyc_cleaned$time_value &lt;- as.Date(cases_nyc_cleaned$time_value)\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \nhead(cases_nyc_weekly)\n\n\n# A tibble: 6 × 2\n  week       total_cases\n  &lt;date&gt;           &lt;dbl&gt;\n1 2020-01-19           0\n2 2020-01-26           0\n3 2020-02-02           0\n4 2020-02-09           0\n5 2020-02-16           0\n6 2020-02-23           0\n\n\nCode\n#sum(is.na(cases_nyc_weekly)) = 0 indicates no missing value\n\n\n\n\nCode\n# Visualization of Covid data of NYC\ncovid_plot &lt;- ggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Weekly COVID-19 Cases in New York City\",\n       x = \"Date\",\n       y = \"Cases\") +\n  theme_minimal()\n\n# Convert to interactive plot with plotly\nggplotly(covid_plot, tooltip = \"text\")\n\n\n\n\n\n\n\nFilter weekly crime data and COVID-19 data to have matching time frame\n\n\n\nCode\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\ndata_f &lt;- data_w %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Filter cases_nyc_weekly to the same time period\ncases_nyc_weekly_filtered &lt;- cases_nyc_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Merging the two datasets on the 'week' column\ncombined_data &lt;- merge(data_f, cases_nyc_weekly_filtered, by = \"week\")\n\ncombined_data &lt;- combined_data[, -2] \nhead(combined_data)\n\n\n        week Total_Crimes total_cases\n1 2020-01-19          699           0\n2 2020-01-26          726           0\n3 2020-02-02          727           0\n4 2020-02-09          681           0\n5 2020-02-16          658           0\n6 2020-02-23          676           0\n\n\nCode\n# Convert to time series\ncombined_ts &lt;- ts(combined_data, start = c(2020, 3), frequency = 52)\n\n\n\n\nCode\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nWe need a log transform on the Covid-19 data due to a dramatic increase towards the end of the time series.\n\n\n\nCode\n# Apply log plus 1 transformation to the COVID-19 cases data to accommodate for 0s\ncombined_ts[, 3] &lt;- log(combined_ts[, 3] + 1)\n\n\n\n\nCode\n# Plot the transformed data\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and transformed Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nThe transformed COVID-19 time series have more stability.\n\n\n\nFit the model\n\n\nCode\nxreg &lt;- combined_ts[, \"total_cases\"]\n\nfit &lt;- auto.arima(combined_ts[, \"Total_Crimes\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"Total_Crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.4809  -26.7319\ns.e.   0.0841    6.4813\n\nsigma^2 = 2839:  log likelihood = -543.98\nAIC=1093.96   AICc=1094.2   BIC=1101.8\n\nTraining set error measures:\n                   ME    RMSE      MAE       MPE     MAPE      MASE        ACF1\nTraining set 6.011042 52.4972 41.53063 0.4133057 5.926572 0.3888636 -0.04368721\n\n\n\nModel Residuals\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1) errors\nQ* = 14.12, df = 19, p-value = 0.7767\n\nModel df: 1.   Total lags used: 20\n\n\n\n\nModel coefficients\n\nThe model computed by auto.arima() is a ARIMAX model with ARIMA(0,1,1), the coefficient for the moving average term is -0.4809, which suggests a moderate negative effect from the previous error term.\nThe exogenous coefficient (representing the COVID-19 cases) is -26.7319, implying that as the COVID-19 cases increase, the number of total crimes decreases, holding other factors constant.\nThe negative exogenous coefficient might raise some questions about the relationship between COVID-19 cases and crime rates. It could suggest that an increase in COVID-19 cases leads to a reduction in reported violent crimes in NYC, possibly due to lockdowns or reduced public activity.\n\n\n\nResidual Analysis\n\nThe Ljung-Box test has a p-value of 0.7767, which is well above 0.05, suggesting that the residuals are independently distributed and that there is no significant autocorrelation at lags up to 20.\nThe residual plots do not show any obvious patterns or trends, which is good as it suggests that the model has captured the data’s structure adequately. The histogram of residuals, overlaid with a normal distribution, seems to show that residuals are approximately normally distributed.\n\n\n\n\nFitting Manually\n\nLinear Regression\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm( Total_Crimes ~ total_cases, data=combined_ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = Total_Crimes ~ total_cases, data = combined_ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-241.199  -88.522    8.395   86.683  228.474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  781.260     36.680  21.299   &lt;2e-16 ***\ntotal_cases   -9.739      5.179  -1.881   0.0629 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 109.4 on 100 degrees of freedom\nMultiple R-squared:  0.03416,   Adjusted R-squared:  0.0245 \nF-statistic: 3.537 on 1 and 100 DF,  p-value: 0.06292\n\n\n\nWhile the linear regression model does not yield significant coefficients for COVID-19 cases, indicating a weak linear relationship, the variable may still be influential in an ARIMAX model.\n\n\n\nExtract Residuals\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit&lt;-ts(residuals(fit.reg),start=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\n\n\n\n\nResiduals ACF/PACF\n\n\nCode\n############## Then look at the residuals ############\nggAcf(res.fit, 50)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit,50)\n\n\n\n\n\n\nThe ACF plot of regression models residual has clear correlation, differencing needs to be applied for stationary purpose.\n\n\n\nDifferencing\n\n\nCode\nres.fit %&gt;% diff() %&gt;% ggtsdisplay() \n\n\n\n\n\n\nPotential Model Parameters\n\np = 0, 1, 2\nd = 0, 1\nq = 0, 1, 2\n\n\n\n\nFitting Model Parameters\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=0,1,2 : 3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 0:1)# d=0,1 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1206.490\n1214.365\n1206.735\n\n\n0\n1\n0\n1116.052\n1121.282\n1116.175\n\n\n0\n0\n1\n1161.659\n1172.159\n1162.071\n\n\n0\n1\n1\n1099.666\n1107.511\n1099.913\n\n\n0\n0\n2\n1142.726\n1155.851\n1143.351\n\n\n0\n1\n2\n1100.686\n1111.146\n1101.102\n\n\n1\n0\n0\n1118.492\n1128.992\n1118.904\n\n\n1\n1\n0\n1100.204\n1108.050\n1100.452\n\n\n1\n0\n1\n1110.109\n1123.234\n1110.734\n\n\n1\n1\n1\n1100.844\n1111.304\n1101.261\n\n\n1\n0\n2\n1110.436\n1126.186\n1111.321\n\n\n1\n1\n2\n1102.641\n1115.716\n1103.272\n\n\n2\n0\n0\n1109.340\n1122.465\n1109.965\n\n\n2\n1\n0\n1100.485\n1110.945\n1100.901\n\n\n2\n0\n1\n1110.944\n1126.693\n1111.828\n\n\n2\n1\n1\n1101.798\n1114.873\n1102.429\n\n\n2\n0\n2\n1104.548\n1122.923\n1105.739\n\n\n2\n1\n2\n1103.774\n1119.465\n1104.668\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\nARIMAX(0,1,1) is the best model with lowest AIC, BIC, AICc.\n\n\n\nModel Diagnostics\n\n\nCode\n# Model diagnostics for ARIMA(0, 1, 1)\nsarima(res.fit, 0, 1, 1)\n\n\ninitial  value 4.086270 \niter   2 value 3.994504\niter   3 value 3.994491\niter   4 value 3.994453\niter   5 value 3.994437\niter   6 value 3.994430\niter   6 value 3.994430\nfinal  value 3.994430 \nconverged\ninitial  value 3.995257 \niter   2 value 3.995251\niter   3 value 3.995249\niter   4 value 3.995249\niter   5 value 3.995248\niter   5 value 3.995248\niter   5 value 3.995248\nfinal  value 3.995248 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.4086    1.3989\ns.e.   0.0824    3.2167\n\nsigma^2 estimated as 2947:  log likelihood = -546.83,  aic = 1099.67\n\n$degrees_of_freedom\n[1] 99\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.4086 0.0824 -4.9591  0.0000\nconstant   1.3989 3.2167  0.4349  0.6646\n\n$AIC\n[1] 10.88778\n\n$AICc\n[1] 10.88899\n\n$BIC\n[1] 10.96546\n\n\n\nThe model appears to perform well, with residuals that are normally distributed and Ljung-Box test p-values indicating no significant autocorrelation. Given that the residuals from both models meet these criteria.\n\n\n\nModel Equations\n\n\nCode\nfit &lt;- Arima(combined_ts[, \"Total_Crimes\"],order=c(0,1,1),xreg=xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"Total_Crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.4809  -26.7319\ns.e.   0.0841    6.4813\n\nsigma^2 = 2839:  log likelihood = -543.98\nAIC=1093.96   AICc=1094.2   BIC=1101.8\n\nTraining set error measures:\n                   ME    RMSE      MAE       MPE     MAPE      MASE        ACF1\nTraining set 6.011042 52.4972 41.53063 0.4133057 5.926572 0.3888636 -0.04368721\n\n\n\n\\((1 - 0B) (1 - 1B) y_t = -26.7319x_t + a_t - 0.4809a_{t-1}\\)\n\n\n\nFitting an ARIMA model to the COVID-19 data\n\n\nCode\nfit_c &lt;- auto.arima(combined_ts[, \"total_cases\"]) #fitting an ARIMA model to the Covid data\nsummary(fit_c) \n\n\nSeries: combined_ts[, \"total_cases\"] \nARIMA(0,1,3) \n\nCoefficients:\n         ma1     ma2     ma3\n      0.7728  0.6654  0.2595\ns.e.  0.0928  0.1023  0.0943\n\nsigma^2 = 0.1263:  log likelihood = -37.79\nAIC=83.57   AICc=83.99   BIC=94.04\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03812665 0.3483306 0.2105525 1.441596 3.946224 0.1072581\n                    ACF1\nTraining set 0.007409321\n\n\nCode\n# Obtain the forecasts\nfc &lt;- forecast(fit_c,12)\nfc &lt;- fc$mean\n\n\n\n\nForecasts\n\n\nCode\nfcast &lt;- forecast(fit, xreg=fc,12) \nfcast\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2022.000       734.1273 665.8376 802.4169 629.6873 838.5672\n2022.019       738.4248 661.4833 815.3664 620.7530 856.0967\n2022.038       739.8946 655.1803 824.6090 610.3352 869.4540\n2022.058       739.8946 648.0630 831.7262 599.4503 880.3389\n2022.077       739.8946 641.4590 838.3302 589.3504 890.4389\n2022.096       739.8946 635.2710 844.5182 579.8867 899.9026\n2022.115       739.8946 629.4292 850.3601 570.9523 908.8370\n2022.135       739.8946 623.8811 855.9082 562.4672 917.3220\n2022.154       739.8946 618.5865 861.2028 554.3698 925.4194\n2022.173       739.8946 613.5135 866.2757 546.6114 933.1779\n2022.192       739.8946 608.6364 871.1528 539.1525 940.6367\n2022.212       739.8946 603.9342 875.8550 531.9611 947.8282\n\n\nCode\nautoplot(fcast, main=\"Forecast of violent Crimes in NYC for the next quarter\") + xlab(\"Year\") +\n  ylab(\"GDP\")\n\n\n\n\n\n\n\nEvaluations\nThe results from the ARIMAX model were not as robust as anticipated, possibly because I did not account for seasonality in the data. Despite this, I opted not to use a SARIMAX model due to the limited amount of COVID-19 data available, which spans only approximately two years on a weekly basis. This duration may not be sufficient for effectively capturing and modeling seasonal patterns. To potentially enhance model performance, I plan to shift to daily data, which will provide a larger data set that could be more suitable for a SARIMAX model, potentially yielding better results.\n\n\n\n\n\nOverview\n\nIn this model, I employ a VAR model to analyze the interplay between monthly violent crimes in New York City and two critical variables: unemployment rates and weather data. The choice of a VAR model stems from its ability to capture the dynamic relationships between multiple time-series datasets. It is particularly suited to understand how changes in economic conditions (reflected by unemployment rates) and environmental factors (illustrated by weather data) collectively impact crime patterns.\nThe integration of unemployment rates is premised on the well-documented correlation between economic conditions and crime. Economic downturns, often signaled by rising unemployment, can influence crime rates through various socio-economic mechanisms. By including unemployment data, the model aims to quantify this relationship and examine how economic fluctuations correlate with changes in crime rates. The unemployment data is from NYUR DATA.\nWeather data, comprising variables such as temperature, humidity, and precipitation, is included based on preliminary exploratory data analysis findings. Our initial examination revealed a notable trend: hotter weather tends to correlate with higher crime rates. This finding aligns with existing criminological theories that suggest weather can significantly impact human behavior, including the propensity to commit crimes. By incorporating weather data, our model is positioned to explore this relationship further, examining how weather patterns influence crime rates over time. The weather data is from NOAA DATA.\n\n\n\nLoad in violent crime data\n\n\nCode\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ndata_m &lt;- data_t %&gt;%\n  mutate(month = floor_date(date_single, \"month\")) %&gt;%\n  group_by(month) %&gt;%\n  summarise(Total_Crimes = sum(Total_Crimes))\n\n# Loading unemployment rate and temperature data\ntemp &lt;- read_csv(\"./dataset/tempdataNYC.csv\")\nune &lt;- read_csv(\"./dataset/NYUR.csv\")\ncolnames(temp)[1] &lt;- \"month\"\ncolnames(une)[1] &lt;- \"month\"\ntemp &lt;- temp[-(1:3), ]\ntemp$month &lt;- paste0(substr(temp$month, 1, 4), \"-\", substr(temp$month, 5, 6))\nune$month &lt;- format(as.Date(une$month), \"%Y-%m\")\n\n# Filter all three data to the overlapping time period (2007-01 to 2022-12)\ntemp &lt;- temp %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\ncolnames(temp)[2] &lt;- \"temp\"\ntemp$temp&lt;- as.numeric(temp$temp)\n\nune &lt;- une %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\n\ndata_m$month &lt;- as.Date(paste0(data_m$month, \"-01\"), format=\"%Y-%m-%d\")\ntemp$month &lt;- as.Date(paste0(temp$month, \"-01\"), format=\"%Y-%m-%d\")\nune$month &lt;- as.Date(paste0(une$month, \"-01\"), format=\"%Y-%m-%d\")\n\n# Merging all 3 datasets by month\ncombined_data &lt;- merge(data_m, temp, by = \"month\")\ncombined_data &lt;- merge(combined_data, une, by = \"month\")\n\n\nknitr::kable(head(combined_data))\n\n\n\n\n\nmonth\nTotal_Crimes\ntemp\nNYUR\n\n\n\n\n2007-01-01\n3066\n37.7\n4.2\n\n\n2007-02-01\n2415\n28.5\n4.2\n\n\n2007-03-01\n2910\n42.5\n4.2\n\n\n2007-04-01\n2904\n50.6\n4.3\n\n\n2007-05-01\n3469\n65.5\n4.3\n\n\n2007-06-01\n3469\n71.6\n4.4\n\n\n\n\n\nCode\n# Convert to ts\ncombined.ts&lt;-ts(combined_data,start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\n\n\nData Visualization\n\n\nCode\nggplot(data_m, aes(x = month, y = Total_Crimes)) +\n  geom_line() +\n  labs(title = \"Monthly violent Crimes\", x = \"Month\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(temp, aes(x = month, y = temp)) +\n  geom_line() +\n  labs(title = \"Monthly Temperature\", x = \"Month\", y = \"Temperature\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(une, aes(x = month, y = NYUR)) +\n  geom_line() +\n  labs(title = \"Monthly Unemployment Rate\", x = \"Month\", y = \"Unemployment Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFitting Model With VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:4)], lag.max=12, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    12      5      5     12 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.493165e+01 1.392855e+01     13.61708     13.36394     13.09146\nHQ(n)  1.504485e+01 1.410967e+01     13.86612     13.68090     13.47634\nSC(n)  1.521057e+01 1.437483e+01     14.23071     14.14493     14.03981\nFPE(n) 3.053199e+06 1.119919e+06 820475.05442 637366.60205 485796.32954\n                  6            7            8            9           10\nAIC(n)     13.05646     13.14066     13.19271     13.26704     13.25517\nHQ(n)      13.50926     13.66139     13.78136     13.92361     13.97965\nSC(n)      14.17216     14.42372     14.64312     14.88481     15.04029\nFPE(n) 469704.17422 511882.37659 540491.91112 583918.50481 579137.70401\n                 11           12\nAIC(n)     13.17201     12.85490\nHQ(n)      13.96442     13.71523\nSC(n)      15.12448     14.97473\nFPE(n) 535296.35122 391906.49202\n\n\n\nBased on the results, we have VAR(5) and VAR(12).\n\n\n\nModel Diagnostics\n\nVAR(5)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, temp, NYUR \nDeterministic variables: both \nSample size: 175 \nLog Likelihood: -1834.341 \nRoots of the characteristic polynomial:\n0.9941 0.9941 0.9557 0.907 0.8199 0.8199 0.7798 0.7798 0.7629 0.7629 0.6902 0.5647 0.5647 0.5027 0.5027\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   0.30162    0.08415   3.584 0.000450 ***\ntemp.l1           6.97573    4.41584   1.580 0.116174    \nNYUR.l1          -9.82705   19.92792  -0.493 0.622606    \nTotal_Crimes.l2   0.35402    0.08741   4.050 8.01e-05 ***\ntemp.l2          -6.82080    5.00667  -1.362 0.175029    \nNYUR.l2          28.12641   28.63596   0.982 0.327500    \nTotal_Crimes.l3  -0.12689    0.09391  -1.351 0.178547    \ntemp.l3           7.83481    5.08958   1.539 0.125712    \nNYUR.l3         -19.45212   28.73162  -0.677 0.499378    \nTotal_Crimes.l4   0.02575    0.08685   0.297 0.767230    \ntemp.l4          -0.54410    4.91205  -0.111 0.911941    \nNYUR.l4          22.90749   28.47866   0.804 0.422390    \nTotal_Crimes.l5   0.32350    0.08304   3.896 0.000144 ***\ntemp.l5         -16.94442    4.45045  -3.807 0.000201 ***\nNYUR.l5         -13.17257   19.04017  -0.692 0.490059    \nconst           852.14899  275.51688   3.093 0.002344 ** \ntrend             0.24774    0.33042   0.750 0.454517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 209.9 on 158 degrees of freedom\nMultiple R-Squared: 0.7497, Adjusted R-squared: 0.7243 \nF-statistic: 29.57 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  7.831e-04  1.450e-03   0.540  0.58982    \ntemp.l1          5.464e-01  7.607e-02   7.183 2.53e-11 ***\nNYUR.l1         -2.664e-01  3.433e-01  -0.776  0.43896    \nTotal_Crimes.l2 -1.666e-03  1.506e-03  -1.106  0.27030    \ntemp.l2          1.113e-01  8.625e-02   1.290  0.19894    \nNYUR.l2          7.327e-01  4.933e-01   1.485  0.13946    \nTotal_Crimes.l3 -2.923e-05  1.618e-03  -0.018  0.98561    \ntemp.l3          3.510e-02  8.768e-02   0.400  0.68948    \nNYUR.l3         -1.311e-01  4.950e-01  -0.265  0.79150    \nTotal_Crimes.l4  3.193e-03  1.496e-03   2.134  0.03436 *  \ntemp.l4         -2.639e-01  8.462e-02  -3.119  0.00216 ** \nNYUR.l4          3.109e-03  4.906e-01   0.006  0.99495    \nTotal_Crimes.l5 -2.245e-03  1.431e-03  -1.569  0.11858    \ntemp.l5         -3.657e-01  7.667e-02  -4.770 4.16e-06 ***\nNYUR.l5         -3.122e-01  3.280e-01  -0.952  0.34265    \nconst            5.154e+01  4.746e+00  10.860  &lt; 2e-16 ***\ntrend            8.022e-03  5.692e-03   1.409  0.16072    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.615 on 158 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.9476 \nF-statistic: 197.8 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1 -5.548e-04  3.619e-04  -1.533   0.1272    \ntemp.l1          6.378e-03  1.899e-02   0.336   0.7374    \nNYUR.l1          1.052e+00  8.570e-02  12.271   &lt;2e-16 ***\nTotal_Crimes.l2 -4.145e-06  3.759e-04  -0.011   0.9912    \ntemp.l2         -6.465e-03  2.153e-02  -0.300   0.7644    \nNYUR.l2         -3.063e-01  1.231e-01  -2.488   0.0139 *  \nTotal_Crimes.l3  7.720e-04  4.038e-04   1.912   0.0577 .  \ntemp.l3          9.826e-04  2.189e-02   0.045   0.9642    \nNYUR.l3          2.590e-01  1.236e-01   2.096   0.0377 *  \nTotal_Crimes.l4 -1.037e-04  3.735e-04  -0.278   0.7817    \ntemp.l4         -7.706e-03  2.112e-02  -0.365   0.7157    \nNYUR.l4         -1.658e-01  1.225e-01  -1.354   0.1778    \nTotal_Crimes.l5 -1.182e-04  3.571e-04  -0.331   0.7410    \ntemp.l5         -1.903e-03  1.914e-02  -0.099   0.9209    \nNYUR.l5          7.056e-02  8.188e-02   0.862   0.3901    \nconst            1.228e+00  1.185e+00   1.037   0.3015    \ntrend           -1.489e-03  1.421e-03  -1.048   0.2964    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9025 on 158 degrees of freedom\nMultiple R-Squared: 0.8518, Adjusted R-squared: 0.8368 \nF-statistic: 56.75 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes     temp     NYUR\nTotal_Crimes     44044.64 260.1181 -75.6436\ntemp               260.12  13.0714  -0.5906\nNYUR               -75.64  -0.5906   0.8145\n\nCorrelation matrix of residuals:\n             Total_Crimes    temp    NYUR\nTotal_Crimes       1.0000  0.3428 -0.3994\ntemp               0.3428  1.0000 -0.1810\nNYUR              -0.3994 -0.1810  1.0000\n\n\n\nThe first lags of violent crimes are significant predictors, and the third and fifth lags also contribute meaningfully.\nTemperature and unemployment rate in some lags show significance, with the fifth lag of temperature being a strong predictor.\nR squared of the model is 0.8518indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR5 &lt;- VAR(combined_data[,c(2:4)], p=5, type=\"both\")\nserial.test(VAR5, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR5\nChi-squared = 109.43, df = 63, p-value = 0.0002616\n\n\nCode\nacf(residuals(VAR5))\n\n\n\n\n\n\nThe residuals are normally distributed.\n\n\n\nVAR(12)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=12, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, temp, NYUR \nDeterministic variables: both \nSample size: 168 \nLog Likelihood: -1680.956 \nRoots of the characteristic polynomial:\n0.9987 0.9987 0.9797 0.9797 0.9744 0.9744 0.9687 0.9504 0.9504 0.9411 0.9411 0.9037 0.9037 0.8919 0.8919 0.8848 0.8848 0.8497 0.8497 0.8443 0.8443 0.8337 0.8337 0.8192 0.8192 0.7893 0.7893 0.771 0.771 0.7665 0.7665 0.7632 0.7632 0.5073 0.5073 0.5013\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 12, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1    0.31996    0.08458   3.783 0.000236 ***\ntemp.l1            3.68792    4.63894   0.795 0.428068    \nNYUR.l1          -24.56941   17.15873  -1.432 0.154575    \nTotal_Crimes.l2    0.22375    0.08338   2.683 0.008234 ** \ntemp.l2           -6.72250    4.75246  -1.415 0.159596    \nNYUR.l2           37.50651   24.47937   1.532 0.127911    \nTotal_Crimes.l3   -0.05015    0.08739  -0.574 0.567011    \ntemp.l3            3.68732    4.77341   0.772 0.441237    \nNYUR.l3          -23.53034   24.85360  -0.947 0.345519    \nTotal_Crimes.l4    0.17063    0.08846   1.929 0.055930 .  \ntemp.l4            2.84428    4.78909   0.594 0.553605    \nNYUR.l4           43.58852   25.21719   1.729 0.086269 .  \nTotal_Crimes.l5    0.25512    0.08794   2.901 0.004370 ** \ntemp.l5           -9.03397    4.83404  -1.869 0.063898 .  \nNYUR.l5          -29.30841   25.30530  -1.158 0.248909    \nTotal_Crimes.l6    0.13990    0.09338   1.498 0.136539    \ntemp.l6           -8.01680    4.94133  -1.622 0.107141    \nNYUR.l6           29.08540   25.36725   1.147 0.253664    \nTotal_Crimes.l7   -0.10529    0.09529  -1.105 0.271223    \ntemp.l7            1.90565    4.94172   0.386 0.700404    \nNYUR.l7          -44.73522   25.46095  -1.757 0.081270 .  \nTotal_Crimes.l8   -0.16992    0.09077  -1.872 0.063450 .  \ntemp.l8            3.29643    4.88056   0.675 0.500608    \nNYUR.l8           18.74965   25.42074   0.738 0.462103    \nTotal_Crimes.l9   -0.08893    0.09222  -0.964 0.336676    \ntemp.l9           -7.00308    4.90625  -1.427 0.155868    \nNYUR.l9          -42.26845   25.24026  -1.675 0.096408 .  \nTotal_Crimes.l10  -0.08845    0.09290  -0.952 0.342791    \ntemp.l10          10.66517    4.92204   2.167 0.032072 *  \nNYUR.l10          -2.74946   24.74241  -0.111 0.911690    \nTotal_Crimes.l11  -0.34923    0.08698  -4.015 9.98e-05 ***\ntemp.l11          13.52799    4.68640   2.887 0.004561 ** \nNYUR.l11          32.44703   24.43351   1.328 0.186514    \nTotal_Crimes.l12   0.54092    0.08550   6.326 3.73e-09 ***\ntemp.l12          -9.85297    4.77695  -2.063 0.041141 *  \nNYUR.l12          29.48804   16.93136   1.742 0.083940 .  \nconst            532.50378  747.26313   0.713 0.477367    \ntrend              0.22749    0.35115   0.648 0.518240    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 172.3 on 130 degrees of freedom\nMultiple R-Squared: 0.8547, Adjusted R-squared: 0.8133 \nF-statistic: 20.67 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)   \nTotal_Crimes.l1   8.534e-04  1.626e-03   0.525  0.60049   \ntemp.l1           2.762e-01  8.915e-02   3.098  0.00239 **\nNYUR.l1          -3.332e-01  3.298e-01  -1.010  0.31424   \nTotal_Crimes.l2  -1.004e-03  1.602e-03  -0.627  0.53196   \ntemp.l2           4.590e-02  9.133e-02   0.503  0.61610   \nNYUR.l2           4.403e-01  4.705e-01   0.936  0.35106   \nTotal_Crimes.l3  -6.033e-04  1.679e-03  -0.359  0.72002   \ntemp.l3           8.691e-02  9.174e-02   0.947  0.34523   \nNYUR.l3           6.519e-02  4.776e-01   0.136  0.89165   \nTotal_Crimes.l4   2.749e-03  1.700e-03   1.617  0.10832   \ntemp.l4          -1.191e-01  9.204e-02  -1.294  0.19787   \nNYUR.l4           7.078e-02  4.846e-01   0.146  0.88412   \nTotal_Crimes.l5  -7.791e-04  1.690e-03  -0.461  0.64561   \ntemp.l5          -1.102e-01  9.290e-02  -1.186  0.23786   \nNYUR.l5          -3.542e-01  4.863e-01  -0.728  0.46772   \nTotal_Crimes.l6   4.690e-04  1.795e-03   0.261  0.79426   \ntemp.l6          -1.774e-01  9.496e-02  -1.868  0.06407 . \nNYUR.l6           2.062e-01  4.875e-01   0.423  0.67306   \nTotal_Crimes.l7   9.091e-05  1.831e-03   0.050  0.96049   \ntemp.l7          -9.346e-02  9.497e-02  -0.984  0.32690   \nNYUR.l7           2.852e-01  4.893e-01   0.583  0.56094   \nTotal_Crimes.l8  -1.063e-03  1.744e-03  -0.609  0.54340   \ntemp.l8           5.041e-02  9.380e-02   0.537  0.59189   \nNYUR.l8          -5.998e-01  4.885e-01  -1.228  0.22180   \nTotal_Crimes.l9   7.111e-04  1.772e-03   0.401  0.68894   \ntemp.l9          -9.072e-02  9.429e-02  -0.962  0.33779   \nNYUR.l9           3.205e-01  4.851e-01   0.661  0.50993   \nTotal_Crimes.l10 -2.790e-04  1.785e-03  -0.156  0.87607   \ntemp.l10          7.616e-02  9.459e-02   0.805  0.42219   \nNYUR.l10         -5.045e-01  4.755e-01  -1.061  0.29064   \nTotal_Crimes.l11 -1.287e-03  1.672e-03  -0.770  0.44272   \ntemp.l11          2.846e-01  9.007e-02   3.160  0.00196 **\nNYUR.l11          7.950e-01  4.696e-01   1.693  0.09284 . \nTotal_Crimes.l12 -1.967e-03  1.643e-03  -1.197  0.23340   \ntemp.l12          1.325e-01  9.181e-02   1.443  0.15145   \nNYUR.l12         -4.093e-01  3.254e-01  -1.258  0.21071   \nconst             4.280e+01  1.436e+01   2.980  0.00344 **\ntrend            -1.208e-03  6.749e-03  -0.179  0.85819   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.312 on 130 degrees of freedom\nMultiple R-Squared: 0.9659, Adjusted R-squared: 0.9562 \nF-statistic: 99.43 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  -7.066e-04  4.636e-04  -1.524   0.1299    \ntemp.l1           3.188e-02  2.543e-02   1.254   0.2122    \nNYUR.l1           1.060e+00  9.405e-02  11.271   &lt;2e-16 ***\nTotal_Crimes.l2   3.656e-04  4.570e-04   0.800   0.4252    \ntemp.l2           4.501e-03  2.605e-02   0.173   0.8631    \nNYUR.l2          -2.705e-01  1.342e-01  -2.016   0.0459 *  \nTotal_Crimes.l3   9.789e-04  4.790e-04   2.044   0.0430 *  \ntemp.l3           1.284e-02  2.616e-02   0.491   0.6244    \nNYUR.l3           2.765e-01  1.362e-01   2.029   0.0445 *  \nTotal_Crimes.l4  -4.270e-04  4.849e-04  -0.881   0.3801    \ntemp.l4          -1.246e-02  2.625e-02  -0.475   0.6359    \nNYUR.l4          -2.340e-01  1.382e-01  -1.693   0.0928 .  \nTotal_Crimes.l5  -2.622e-04  4.820e-04  -0.544   0.5875    \ntemp.l5          -2.994e-02  2.650e-02  -1.130   0.2605    \nNYUR.l5           9.230e-02  1.387e-01   0.665   0.5070    \nTotal_Crimes.l6  -2.236e-05  5.118e-04  -0.044   0.9652    \ntemp.l6           7.516e-03  2.708e-02   0.277   0.7818    \nNYUR.l6          -1.329e-01  1.390e-01  -0.956   0.3408    \nTotal_Crimes.l7   1.082e-04  5.223e-04   0.207   0.8363    \ntemp.l7           1.662e-02  2.709e-02   0.614   0.5405    \nNYUR.l7           1.870e-01  1.396e-01   1.340   0.1825    \nTotal_Crimes.l8  -2.389e-04  4.975e-04  -0.480   0.6319    \ntemp.l8           1.994e-02  2.675e-02   0.745   0.4573    \nNYUR.l8          -9.392e-02  1.393e-01  -0.674   0.5015    \nTotal_Crimes.l9   4.698e-06  5.055e-04   0.009   0.9926    \ntemp.l9           3.613e-02  2.689e-02   1.343   0.1815    \nNYUR.l9           1.066e-01  1.383e-01   0.770   0.4424    \nTotal_Crimes.l10  7.141e-05  5.092e-04   0.140   0.8887    \ntemp.l10         -2.898e-02  2.698e-02  -1.074   0.2847    \nNYUR.l10         -8.635e-02  1.356e-01  -0.637   0.5254    \nTotal_Crimes.l11  7.388e-04  4.767e-04   1.550   0.1237    \ntemp.l11         -4.162e-02  2.569e-02  -1.620   0.1076    \nNYUR.l11          2.030e-02  1.339e-01   0.152   0.8798    \nTotal_Crimes.l12 -5.791e-04  4.686e-04  -1.236   0.2188    \ntemp.l12          1.823e-02  2.618e-02   0.696   0.4876    \nNYUR.l12         -2.866e-02  9.280e-02  -0.309   0.7580    \nconst            -1.196e+00  4.096e+00  -0.292   0.7707    \ntrend            -2.030e-03  1.925e-03  -1.055   0.2936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9446 on 130 degrees of freedom\nMultiple R-Squared: 0.8631, Adjusted R-squared: 0.8241 \nF-statistic: 22.15 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes     temp     NYUR\nTotal_Crimes     29701.03 221.9391 -60.5674\ntemp               221.94  10.9701  -0.4129\nNYUR               -60.57  -0.4129   0.8923\n\nCorrelation matrix of residuals:\n             Total_Crimes    temp   NYUR\nTotal_Crimes       1.0000  0.3888 -0.372\ntemp               0.3888  1.0000 -0.132\nNYUR              -0.3720 -0.1320  1.000\n\n\n\nmost of the coeffcients values are not significant predictors.\nTemperature and unemployment rate in some lags show significance, with the second lag of unemploymnet rate being a strong predictor.\nR squared of the model is 0.8631 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR12 &lt;- VAR(combined_data[,2:4], p=12, type=\"both\")\nserial.test(VAR12, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR12\nChi-squared = 52.247, df = 0, p-value &lt; 2.2e-16\n\n\nCode\nacf(residuals(VAR12))\n\n\n\n\n\n\nResiduals are normally distributed.\n\n\n\n\nForecasts\n\nVAR(5)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:4)], start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nfit &lt;- VAR(ts_obj, p=5, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       3103.657 2834.700 3372.614 2692.323 3514.991\nFeb 2022       3060.138 2772.459 3347.818 2620.170 3500.106\nMar 2022       3181.174 2870.983 3491.365 2706.778 3655.570\nApr 2022       3257.261 2942.064 3572.459 2775.209 3739.314\nMay 2022       3470.026 3150.259 3789.793 2980.984 3959.067\nJun 2022       3662.001 3329.827 3994.175 3153.984 4170.017\nJul 2022       3761.737 3423.536 4099.938 3244.503 4278.971\nAug 2022       3809.891 3457.215 4162.567 3270.520 4349.262\nSep 2022       3734.276 3375.875 4092.677 3186.149 4282.403\nOct 2022       3578.426 3215.055 3941.798 3022.698 4134.155\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       34.00831 29.37494 38.64168 26.92218 41.09444\nFeb 2022       37.57718 32.21833 42.93603 29.38153 45.77284\nMar 2022       44.65194 38.98766 50.31623 35.98917 53.31472\nApr 2022       53.78903 47.93629 59.64176 44.83804 62.74001\nMay 2022       65.73079 59.84405 71.61753 56.72780 74.73378\nJun 2022       74.35874 68.01711 80.70037 64.66005 84.05743\nJul 2022       77.83947 70.93009 84.74885 67.27249 88.40645\nAug 2022       75.90105 68.50215 83.29994 64.58541 87.21668\nSep 2022       69.37341 61.61778 77.12905 57.51219 81.23463\nOct 2022       59.00884 51.14679 66.87089 46.98488 71.03280\n\nNYUR\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\nJan 2022       4.191674 3.035061 5.348287 2.4227867 5.960561\nFeb 2022       4.378075 2.654950 6.101200 1.7427829 7.013368\nMar 2022       4.472398 2.466663 6.478133 1.4048909 7.539905\nApr 2022       4.557274 2.353268 6.761279 1.1865381 7.928010\nMay 2022       4.699641 2.363008 7.036275 1.1260687 8.273214\nJun 2022       4.819594 2.388499 7.250690 1.1015543 8.537635\nJul 2022       4.789580 2.277456 7.301705 0.9476174 8.631543\nAug 2022       4.729443 2.151139 7.307746 0.7862677 8.672618\nSep 2022       4.641580 2.011852 7.271307 0.6197589 8.663401\nOct 2022       4.507292 1.835800 7.178785 0.4215968 8.592988\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(12)\n\n\nCode\nfit &lt;- VAR(ts_obj, p=12, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       3085.732 2864.870 3306.594 2747.952 3423.512\nFeb 2022       2741.762 2502.073 2981.451 2375.189 3108.335\nMar 2022       2867.565 2617.885 3117.244 2485.713 3249.417\nApr 2022       2830.417 2578.630 3082.204 2445.342 3215.492\nMay 2022       3146.570 2888.337 3404.803 2751.636 3541.504\nJun 2022       3399.356 3129.491 3669.222 2986.633 3812.080\nJul 2022       3770.577 3487.947 4053.207 3338.331 4202.823\nAug 2022       3605.486 3317.579 3893.393 3165.170 4045.802\nSep 2022       3482.601 3194.016 3771.187 3041.248 3923.955\nOct 2022       3431.597 3139.105 3724.088 2984.269 3878.924\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       33.09064 28.84601 37.33528 26.59903 39.58226\nFeb 2022       35.25381 30.78808 39.71955 28.42406 42.08357\nMar 2022       40.50027 36.00971 44.99082 33.63255 47.36798\nApr 2022       50.30555 45.77971 54.83138 43.38388 57.22722\nMay 2022       60.08442 55.54119 64.62765 53.13615 67.03269\nJun 2022       68.82703 64.25990 73.39417 61.84220 75.81187\nJul 2022       75.47951 70.80275 80.15628 68.32701 82.63201\nAug 2022       74.11362 69.29282 78.93442 66.74084 81.48640\nSep 2022       67.95866 63.11526 72.80207 60.55132 75.36601\nOct 2022       59.27962 54.39054 64.16870 51.80242 66.75682\n\nNYUR\n         Point Forecast    Lo 80    Hi 80      Lo 95    Hi 95\nJan 2022       3.815681 2.605107 5.026255  1.9642677 5.667094\nFeb 2022       4.059000 2.258537 5.859462  1.3054298 6.812570\nMar 2022       4.367171 2.267994 6.466347  1.1567577 7.577584\nApr 2022       4.719622 2.372232 7.067013  1.1295982 8.309646\nMay 2022       5.006499 2.493631 7.519367  1.1633986 8.849599\nJun 2022       4.935612 2.307341 7.563883  0.9160188 8.955206\nJul 2022       4.258368 1.557959 6.958777  0.1284485 8.388288\nAug 2022       4.029914 1.263429 6.796398 -0.2010597 8.260887\nSep 2022       4.102393 1.278089 6.926696 -0.2170065 8.421792\nOct 2022       4.132406 1.246105 7.018707 -0.2818106 8.546623\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nEvaluations\nthe historical data shows fluctuations with no clear long-term trend. The forecast suggests a stable outlook with a slight uptick towards the end, within a moderate confidence interval.\n\n\n\n\n\nOverview\n\nIn this model, we employs a Vector Autoregression VAR model to scrutinize the weekly total crime rates in New York City, contextualized by the significant societal disruptions of the COVID-19 pandemic. The model is designed to understand how the pandemic may have altered crime trends, leveraging the robust COVID-19 datasets available from the covidcast package in R. These datasets capture a range of COVID-19 metrics, serving as vital exogenous inputs to our model, thus facilitating a comprehensive examination of the pandemic’s imprint on urban crime patterns.\n\n\n\nCode\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\n#v_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n#w_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \n\ncases_nyc_weekly$total_cases &lt;- cases_nyc_weekly$total_cases +1\n\n# Merge data sets\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\ndata_w &lt;- data_w %&gt;%\n  filter(week &gt;= \"2020-01-19\" & week &lt;= \"2022-12-25\")\n\n\n# Merging the two datasets on the 'date' column\ncombined_data &lt;- merge(data_w, cases_nyc_weekly, by = \"week\")\ncombined_data &lt;- combined_data[, -2] \nhead(combined_data)\n\n\n        week Total_Crimes total_cases\n1 2020-01-19          699           1\n2 2020-01-26          726           1\n3 2020-02-02          727           1\n4 2020-02-09          681           1\n5 2020-02-16          658           1\n6 2020-02-23          676           1\n\n\n\n\nData Visualization\n\n\nCode\nggplot(data_w, aes(x = week, y = Total_Crimes)) +\n  geom_line() +\n  labs(title = \"Weekly Violent Crimes\", x = \"Date\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line() +\n  labs(title = \"Weekly COVID-19 total cases\", x = \"Date\", y = \"Cases\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFitting Model with VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:3)], lag.max=14, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      4      2      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 2.444372e+01 2.389856e+01 2.389264e+01 2.381126e+01 2.382067e+01\nHQ(n)  2.451203e+01 2.400102e+01 2.402926e+01 2.398203e+01 2.402560e+01\nSC(n)  2.461181e+01 2.415070e+01 2.422883e+01 2.423149e+01 2.432495e+01\nFPE(n) 4.128435e+10 2.393633e+10 2.379860e+10 2.194366e+10 2.215909e+10\n                  6            7            8            9           10\nAIC(n) 2.380090e+01 2.381596e+01 2.384493e+01 2.386624e+01 2.389024e+01\nHQ(n)  2.403998e+01 2.408920e+01 2.415232e+01 2.420778e+01 2.426593e+01\nSC(n)  2.438923e+01 2.448834e+01 2.460135e+01 2.470670e+01 2.481475e+01\nFPE(n) 2.173618e+10 2.208071e+10 2.274916e+10 2.326396e+10 2.386044e+10\n                 11           12           13           14\nAIC(n) 2.389903e+01 2.394177e+01 2.394091e+01 2.397983e+01\nHQ(n)  2.430888e+01 2.438577e+01 2.441906e+01 2.449214e+01\nSC(n)  2.490759e+01 2.503438e+01 2.511756e+01 2.524053e+01\nFPE(n) 2.410944e+10 2.520960e+10 2.524353e+10 2.631287e+10\n\n\n\nBased on the results, we have VAR(2), VAR(4) and VAR(6)\n\n\n\nModel Diagnostics\n\nVAR(2)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=2, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 152 \nLog Likelihood: -2231.182 \nRoots of the characteristic polynomial:\n0.9018 0.7848 0.7848 0.3175\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 2, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.582474   0.079733   7.305 1.66e-11 ***\ntotal_cases.l1  -0.002829   0.001470  -1.924 0.056278 .  \nTotal_Crimes.l2  0.285326   0.080174   3.559 0.000503 ***\ntotal_cases.l2   0.002293   0.001481   1.548 0.123780    \nconst           85.147059  33.771947   2.521 0.012768 *  \ntrend            0.221369   0.150777   1.468 0.144204    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.98 on 146 degrees of freedom\nMultiple R-Squared: 0.8065, Adjusted R-squared: 0.7999 \nF-statistic: 121.7 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   -1.34818    3.55002  -0.380    0.705    \ntotal_cases.l1     1.42297    0.06545  21.741   &lt;2e-16 ***\nTotal_Crimes.l2   -0.29171    3.56964  -0.082    0.935    \ntotal_cases.l2    -0.62026    0.06594  -9.407   &lt;2e-16 ***\nconst           1237.92672 1503.65927   0.823    0.412    \ntrend              9.65213    6.71316   1.438    0.153    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2537 on 146 degrees of freedom\nMultiple R-Squared: 0.8693, Adjusted R-squared: 0.8648 \nF-statistic: 194.2 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3247       -3671\ntotal_cases         -3671     6436993\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.02539\ntotal_cases      -0.02539     1.00000\n\n\n\nResiduals\n\n\n\nCode\nVAR2 &lt;- VAR(combined_data[,c(2:3)], p=2, type=\"both\")\nserial.test(VAR2, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR2\nChi-squared = 189.81, df = 200, p-value = 0.6861\n\n\nCode\nacf(residuals(VAR2))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\nVAR(4)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=4, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 150 \nLog Likelihood: -2189.821 \nRoots of the characteristic polynomial:\n0.8861 0.8015 0.8015 0.774 0.6592 0.6592 0.598 0.4795\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 4, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.5739946  0.0835420   6.871 1.93e-10 ***\ntotal_cases.l1  -0.0032288  0.0018585  -1.737  0.08453 .  \nTotal_Crimes.l2  0.2313245  0.0939156   2.463  0.01499 *  \ntotal_cases.l2   0.0039177  0.0031402   1.248  0.21426    \nTotal_Crimes.l3  0.2448559  0.0937532   2.612  0.00999 ** \ntotal_cases.l3  -0.0016973  0.0031546  -0.538  0.59140    \nTotal_Crimes.l4 -0.1829429  0.0838566  -2.182  0.03081 *  \ntotal_cases.l4   0.0004815  0.0018796   0.256  0.79819    \nconst           85.6789208 35.6244882   2.405  0.01748 *  \ntrend            0.2201465  0.1625484   1.354  0.17781    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.39 on 140 degrees of freedom\nMultiple R-Squared: 0.8178, Adjusted R-squared: 0.8061 \nF-statistic: 69.81 on 9 and 140 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   -0.66991    3.61658  -0.185 0.853314    \ntotal_cases.l1     1.40237    0.08046  17.430  &lt; 2e-16 ***\nTotal_Crimes.l2   -3.06501    4.06566  -0.754 0.452189    \ntotal_cases.l2    -0.35208    0.13594  -2.590 0.010615 *  \nTotal_Crimes.l3    6.06912    4.05863   1.495 0.137071    \ntotal_cases.l3    -0.50183    0.13657  -3.675 0.000338 ***\nTotal_Crimes.l4   -2.92659    3.63020  -0.806 0.421507    \ntotal_cases.l4     0.30785    0.08137   3.783 0.000229 ***\nconst            571.35179 1542.20510   0.370 0.711587    \ntrend              5.47566    7.03682   0.778 0.437797    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2441 on 140 degrees of freedom\nMultiple R-Squared: 0.8835, Adjusted R-squared: 0.876 \nF-statistic:   118 on 9 and 140 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3179       -9639\ntotal_cases         -9639     5958430\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.07003\ntotal_cases      -0.07003     1.00000\n\n\n\nBoth models looks good based on the model diagnostics, most coefficients are significant, combined with a high R square of above 0.7.\nResiduals\n\n\n\nCode\nVAR4 &lt;- VAR(combined_data[,c(2:3)], p=4, type=\"both\")\nserial.test(VAR4, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR4\nChi-squared = 157.09, df = 192, p-value = 0.9691\n\n\nCode\nacf(residuals(VAR4))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\nVAR(6)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 148 \nLog Likelihood: -2153.447 \nRoots of the characteristic polynomial:\n0.8565 0.8565 0.8518 0.8219 0.8219 0.7468 0.7468 0.6774 0.6199 0.495 0.495 0.1675\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 6, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + Total_Crimes.l5 + total_cases.l5 + Total_Crimes.l6 + total_cases.l6 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.582113   0.085967   6.771 3.65e-10 ***\ntotal_cases.l1  -0.004696   0.001963  -2.393   0.0181 *  \nTotal_Crimes.l2  0.240793   0.097261   2.476   0.0145 *  \ntotal_cases.l2   0.007350   0.003427   2.144   0.0338 *  \nTotal_Crimes.l3  0.226815   0.098705   2.298   0.0231 *  \ntotal_cases.l3  -0.002131   0.003419  -0.623   0.5341    \nTotal_Crimes.l4 -0.132873   0.098316  -1.351   0.1788    \ntotal_cases.l4  -0.006296   0.003372  -1.867   0.0640 .  \nTotal_Crimes.l5 -0.014488   0.097858  -0.148   0.8825    \ntotal_cases.l5   0.008775   0.003418   2.567   0.0114 *  \nTotal_Crimes.l6 -0.038168   0.085310  -0.447   0.6553    \ntotal_cases.l6  -0.003475   0.001999  -1.739   0.0844 .  \nconst           88.081052  37.622360   2.341   0.0207 *  \ntrend            0.218624   0.173852   1.258   0.2107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.02 on 134 degrees of freedom\nMultiple R-Squared: 0.8265, Adjusted R-squared: 0.8097 \nF-statistic: 49.11 on 13 and 134 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + Total_Crimes.l5 + total_cases.l5 + Total_Crimes.l6 + total_cases.l6 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1    1.65482    3.70941   0.446   0.6562    \ntotal_cases.l1     1.42104    0.08468  16.781  &lt; 2e-16 ***\nTotal_Crimes.l2   -4.86294    4.19676  -1.159   0.2486    \ntotal_cases.l2    -0.31798    0.14789  -2.150   0.0333 *  \nTotal_Crimes.l3    4.78356    4.25904   1.123   0.2634    \ntotal_cases.l3    -0.64459    0.14751  -4.370 2.47e-05 ***\nTotal_Crimes.l4   -4.50211    4.24227  -1.061   0.2905    \ntotal_cases.l4     0.34349    0.14548   2.361   0.0197 *  \nTotal_Crimes.l5    3.21222    4.22251   0.761   0.4482    \ntotal_cases.l5     0.22827    0.14750   1.548   0.1241    \nTotal_Crimes.l6   -1.28240    3.68107  -0.348   0.7281    \ntotal_cases.l6    -0.22308    0.08624  -2.587   0.0108 *  \nconst            835.28512 1623.37950   0.515   0.6077    \ntrend              8.31945    7.50158   1.109   0.2694    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2417 on 134 degrees of freedom\nMultiple R-Squared: 0.8902, Adjusted R-squared: 0.8796 \nF-statistic: 83.58 on 13 and 134 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3138      -12410\ntotal_cases        -12410     5842667\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.09165\ntotal_cases      -0.09165     1.00000\n\n\n\nBoth models looks good based on the model diagnostics, most coefficients are significant, combined with a high R square of above 0.8.\nResiduals\n\n\n\nCode\nVAR6 &lt;- VAR(combined_data[,c(2:3)], p=6, type=\"both\")\nserial.test(VAR6, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR6\nChi-squared = 140.7, df = 184, p-value = 0.9924\n\n\nCode\nacf(residuals(VAR6))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\n\nForecasts\n\nVAR(2)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=2, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       720.5998 647.5726 793.6269 608.9144  832.2851\n2023.030       729.9145 644.7867 815.0424 599.7227  860.1064\n2023.049       746.0172 648.5536 843.4808 596.9595  895.0749\n2023.068       756.9854 651.1908 862.7799 595.1865  918.7842\n2023.088       767.7670 655.4002 880.1337 595.9169  939.6171\n2023.107       777.5903 660.2728 894.9079 598.1687  957.0120\n2023.126       787.1464 665.9966 908.2961 601.8638  972.4290\n2023.145       796.3813 672.2474 920.5153 606.5349  986.2278\n2023.165       805.2947 678.8048 931.7846 611.8451  998.7443\n2023.184       813.7790 685.4162 942.1419 617.4650 1010.0931\n\ntotal_cases\n         Point Forecast       Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3764.515   513.06139  7015.969 -1208.154  8737.185\n2023.030       4693.832  -964.01475 10351.678 -3959.098 13346.761\n2023.049       5903.221 -1382.37518 13188.817 -5239.137 17045.579\n2023.068       7032.947 -1132.21160 15198.106 -5454.586 19520.480\n2023.088       7880.538  -639.56096 16400.637 -5149.829 20910.905\n2023.107       8377.822  -230.70325 16986.347 -4787.781 21543.425\n2023.126       8552.974   -66.80241 17172.750 -4629.836 21735.784\n2023.145       8487.666  -152.42307 17127.756 -4726.210 21701.543\n2023.165       8280.510  -398.79206 16959.812 -4993.337 21554.356\n2023.184       8021.182  -698.09383 16740.457 -5313.800 21356.163\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(4)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=4, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       718.1521 645.8903 790.4138 607.6373  828.6669\n2023.030       729.6403 645.3617 813.9188 600.7474  858.5332\n2023.049       736.3005 642.1673 830.4337 592.3361  880.2649\n2023.068       760.6577 652.4120 868.9034 595.1103  926.2051\n2023.088       770.3121 655.8584 884.7658 595.2703  945.3540\n2023.107       781.1818 661.1142 901.2494 597.5543  964.8094\n2023.126       794.8875 670.1044 919.6706 604.0482  985.7268\n2023.145       803.6789 676.0370 931.3207 608.4675  998.8903\n2023.165       813.5903 683.4608 943.7198 614.5744 1012.6062\n2023.184       823.0851 691.0192 955.1511 621.1077 1025.0625\n\ntotal_cases\n         Point Forecast     Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3237.656   109.402  6365.910 -1546.596  8021.907\n2023.030       3352.171 -2038.886  8743.229 -4892.740 11597.082\n2023.049       3954.929 -3459.054 11368.911 -7383.780 15293.637\n2023.068       4939.053 -3497.901 13376.006 -7964.155 17842.260\n2023.088       5850.568 -2977.765 14678.900 -7651.202 19352.337\n2023.107       6446.281 -2445.007 15337.569 -7151.770 20044.332\n2023.126       6749.394 -2142.575 15641.364 -6849.700 20348.488\n2023.145       6760.553 -2136.883 15657.989 -6846.901 20368.007\n2023.165       6646.433 -2253.220 15546.087 -6964.412 20257.279\n2023.184       6537.003 -2364.094 15438.101 -7076.051 20150.057\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(6)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=6, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       722.4993 650.7087 794.2899 612.7050  832.2935\n2023.030       723.4490 638.4588 808.4392 593.4677  853.4303\n2023.049       731.0963 636.2415 825.9511 586.0285  876.1642\n2023.068       759.0584 650.6196 867.4972 593.2155  924.9013\n2023.088       762.0325 644.9207 879.1442 582.9255  941.1395\n2023.107       778.8229 655.3577 902.2881 589.9992  967.6466\n2023.126       790.3255 661.4272 919.2239 593.1925  987.4586\n2023.145       798.9869 666.9459 931.0279 597.0476 1000.9262\n2023.165       813.5840 678.5537 948.6143 607.0730 1020.0950\n2023.184       821.9354 685.0586 958.8121 612.6005 1031.2702\n\ntotal_cases\n         Point Forecast      Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3456.950   359.2337  6554.666 -1280.598  8194.498\n2023.030       3853.784 -1521.3087  9228.876 -4366.711 12074.279\n2023.049       5094.426 -2424.5477 12613.400 -6404.852 16593.704\n2023.068       6770.689 -1802.0401 15343.418 -6340.169 19881.547\n2023.088       7766.123 -1136.4672 16668.713 -5849.214 21381.459\n2023.107       8337.517  -616.9653 17292.000 -5357.182 22032.217\n2023.126       8367.280  -591.1327 17325.694 -5333.430 22067.991\n2023.145       8214.080  -749.5883 17177.748 -5494.668 21922.827\n2023.165       8180.055  -788.0688 17148.179 -5535.507 21895.617\n2023.184       8154.277  -823.6237 17132.177 -5576.237 21884.790\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\n\nEvaluations\n\nVAR(2) Forecast: The forecast for violent crimes indicates a stable trend with a slight increase toward the end, and the confidence interval is moderate.\nVAR(4) Forecast: The forecast for violent crimes remains stable with a slightly positive trend toward the end, similar to VAR(2), but with a narrower confidence interval, suggesting more certainty in the prediction.\nVAR(6) Forecast: The forecast for violent crimes remains stable with a slightly positive trend toward the end, similar to VAR(2), but with a narrower confidence interval, suggesting more certainty in the prediction.\n\n\n\n\n\nOverview\nIn this model, we focus on analyzing the weekly aggravated violent crimes in New York City using an ARIMAX model. The unprecedented global crisis brought on by the COVID-19 pandemic has had profound impacts on various facets of society, including crime patterns. Recognizing the potential influence of the pandemic on crime dynamics, our model integrates COVID-19 data as a critical exogenous component. This data is extracted using the covidcast package in R, which provides a comprehensive dataset of COVID-19 metrics.\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\nlibrary(covidcast)\nlibrary(zoo)\nlibrary(imputeTS)\nlibrary(vars)\n\n# Read the crime data from a CSV file into a dataframe\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\n\n# Filter the data to include only 'aggravated assault' type offenses\nassault &lt;- data %&gt;%\n  filter(offense_type == \"aggravated assault\")\n# Convert the date to a Date object for easier handling\nassault$date &lt;- as.Date(assault$date_single)\n\n# Aggregate data to count the number of assaults per date\nassault_day &lt;- assault %&gt;%\n  mutate(n = 1) %&gt;%\n  group_by(date) %&gt;%\n  summarize(crime_numbers = sum(n))\n\n# Create a new dataframe with the aggregated data\nassault &lt;- data.frame(\n  date = assault_day$date,\n  crimes = assault_day$crime_numbers\n)\n# Convert the daily crime data into a time series object with daily frequency\nassault_ts &lt;- ts(assault$crimes, start = c(2018, 1), frequency = 365)\n\n# Aggregate the daily data into weekly data\nassault_weekly &lt;- assault %&gt;%\n  mutate(week = lubridate::floor_date(date, \"week\")) %&gt;%  # Rounding down the date to the start of the week\n  group_by(week) %&gt;%\n  summarise(total_crimes = sum(crimes))\n# Convert the weekly aggregated crime data into a time series object with weekly frequency\nts_assault_weekly &lt;- ts(assault_weekly$total_crimes, frequency = 52, start = c(2018, 1))\n\n\n\n\nCOVID-19 data extraction and cleaning\n\nExtract COVID data using covid_cast package in R\n\n\n\nCode\n# Fetching COVID-19 cases data for New York City for the specified timeframe\ncases_nyc &lt;- covidcast_signal(\"jhu-csse\", \"confirmed_incidence_num\",\n                              start_day = \"2020-01-01\", end_day = \"2022-12-31\",\n                              geo_type = \"county\", geo_values = \"36061\") # 36061 is the FIPS code for New York County\n\n# Cleaning and preparing the data\ncases_nyc_cleaned &lt;- cases_nyc %&gt;%\n  dplyr::select(time_value, value) %&gt;%\n  dplyr::arrange(time_value)\n\n# Ensure the date column is in the Date format\ncases_nyc_cleaned$time_value &lt;- as.Date(cases_nyc_cleaned$time_value)\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \nhead(cases_nyc_weekly)\n\n\n# A tibble: 6 × 2\n  week       total_cases\n  &lt;date&gt;           &lt;dbl&gt;\n1 2020-01-19           0\n2 2020-01-26           0\n3 2020-02-02           0\n4 2020-02-09           0\n5 2020-02-16           0\n6 2020-02-23           0\n\n\nCode\n#sum(is.na(cases_nyc_weekly)) = 0 indicates no missing value\n\n\n\n\nCode\n# Visualization of Covid data of NYC\ncovid_plot &lt;- ggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Weekly COVID-19 Cases in New York City\",\n       x = \"Date\",\n       y = \"Cases\") +\n  theme_minimal()\n\n# Convert to interactive plot with plotly\nggplotly(covid_plot, tooltip = \"text\")\n\n\n\n\n\n\n\nFilter weekly assault data and COVID-19 data to have matching time frame\n\n\n\nCode\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\nassault_weekly_filtered &lt;- assault_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Filter cases_nyc_weekly to the same time period\ncases_nyc_weekly_filtered &lt;- cases_nyc_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Merging the two datasets on the 'week' column\ncombined_data &lt;- merge(assault_weekly_filtered, cases_nyc_weekly_filtered, by = \"week\")\nhead(combined_data)\n\n\n        week total_crimes total_cases\n1 2020-01-19          392           0\n2 2020-01-26          366           0\n3 2020-02-02          394           0\n4 2020-02-09          343           0\n5 2020-02-16          358           0\n6 2020-02-23          366           0\n\n\nCode\n# Convert to time series\ncombined_ts &lt;- ts(combined_data, start = c(2020, 3), frequency = 52)\n\n\n\n\nCode\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nWe need a log transform on the COVID-19 data due to a dramatic increase towards the end of the time series.\n\n\n\nCode\n# Apply log plus 1 transformation to the COVID-19 cases data to accommodate for 0s\ncombined_ts[, 3] &lt;- log(combined_ts[, 3] + 1)\n\n\n\n\nCode\n# Plot the transformed data\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and transformed Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nThe transformed COVID-19 series have more stability.\n\n\n\nFit with auto.arima\n\n\nCode\nxreg &lt;- combined_ts[, \"total_cases\"]\n\nfit &lt;- auto.arima(combined_ts[, \"total_crimes\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"total_crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.5098  -12.6470\ns.e.   0.0825    4.3866\n\nsigma^2 = 1418:  log likelihood = -508.94\nAIC=1023.87   AICc=1024.12   BIC=1031.72\n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1\nTraining set 3.227728 37.10015 29.27302 0.17115 7.150118 0.4959847 -0.0275397\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1) errors\nQ* = 18.209, df = 19, p-value = 0.5085\n\nModel df: 1.   Total lags used: 20\n\n\n\n\nModel diagnostics\n\nModel coefficients\n\nThe model computed by auto.arima() is a ARIMAX model with ARIMA(0,1,1), the coefficient for the moving averafe term is -0.5074, which suggests a moderate negative effect from the previous error term.\nThe exogenous coefficient (representing the COVID-19 cases) is -14.4388, implying that as the COVID-19 cases increase, the number of total crimes decreases, holding other factors constant.\nThe negative exogenous coefficient might raise some questions about the relationship between Covid-19 cases and crime rates. It could suggest that an increase in Covid-19 cases leads to a reduction in reported assault crimes in NYC, possibly due to lockdowns or reduced public activity.\n\n\n\nResidual Analysis\n\nThe Ljung-Box test has a p-value of 0.665, which is well above 0.05, suggesting that the residuals are independently distributed and that there is no significant autocorrelation at lags up to 20.\nThe residual plots do not show any obvious patterns or trends, which is good as it suggests that the model has captured the data’s structure adequately. The histogram of residuals, overlaid with a normal distribution, seems to show that residuals are approximately normally distributed.\n\n\n\n\nFitting Manually\n\nLinear Regression\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm( total_crimes ~ total_cases, data=combined_ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = total_crimes ~ total_cases, data = combined_ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.350  -49.895    5.249   53.052  131.863 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  438.386     22.762  19.260   &lt;2e-16 ***\ntotal_cases   -2.375      3.214  -0.739    0.462    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.9 on 100 degrees of freedom\nMultiple R-squared:  0.005433,  Adjusted R-squared:  -0.004513 \nF-statistic: 0.5462 on 1 and 100 DF,  p-value: 0.4616\n\n\n\nWhile the linear regression model does not yield significant coefficients for COVID-19 cases, indicating a weak linear relationship, the variable may still be influential in an ARIMAX model.\n\n\n\nConverting to Time Series Component\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit&lt;-ts(residuals(fit.reg),start=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\n\n\n\n\nResiduals ACF/PACF\n\n\nCode\n############## Then look at the residuals ############\nggAcf(res.fit)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit)\n\n\n\n\n\n\nThe ACF plot of regression models residual has clear correlation, differencing needs to be applied for stationary purpose.\n\n\n\nDifferencing\n\n\nCode\nres.fit %&gt;% diff() %&gt;% ggtsdisplay() \n\n\n\n\n\n\nPotential Model Parameters\n\np = 0, 1, 2\nd = 0, 1\nq = 0, 1, 2\n\n\n\n\nFitting Model Parameters\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=0,1,2 : 3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 0:1)# d=0,1 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1119.359\n1127.234\n1119.604\n\n\n0\n1\n0\n1047.746\n1052.976\n1047.868\n\n\n0\n0\n1\n1077.669\n1088.169\n1078.082\n\n\n0\n1\n1\n1028.672\n1036.518\n1028.920\n\n\n0\n0\n2\n1070.725\n1083.850\n1071.350\n\n\n0\n1\n2\n1030.215\n1040.675\n1030.632\n\n\n1\n0\n0\n1047.058\n1057.558\n1047.470\n\n\n1\n1\n0\n1032.513\n1040.359\n1032.761\n\n\n1\n0\n1\n1038.258\n1051.383\n1038.883\n\n\n1\n1\n1\n1030.391\n1040.851\n1030.808\n\n\n1\n0\n2\n1039.241\n1054.991\n1040.125\n\n\n1\n1\n2\n1032.065\n1045.141\n1032.697\n\n\n2\n0\n0\n1039.652\n1052.777\n1040.277\n\n\n2\n1\n0\n1027.897\n1038.358\n1028.314\n\n\n2\n0\n1\n1039.774\n1055.524\n1040.659\n\n\n2\n1\n1\n1027.902\n1040.977\n1028.533\n\n\n2\n0\n2\n1041.017\n1059.392\n1042.208\n\n\n2\n1\n2\n1029.829\n1045.520\n1030.723\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n4 0 1 1 1028.672 1036.518 1028.92\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n14 2 1 0 1027.897 1038.358 1028.314\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n14 2 1 0 1027.897 1038.358 1028.314\n\n\nWe have two models here, ARIMA(0, 1, 1) is suggested by the lowest BIC, and ARIMA(2, 1, 1) is suggest by lowest AIC and AICc\n\n\nARIMAX Model Diagnostics\n\n\nCode\n# Model diagnostics for ARIMA(0, 1, 1)\nsarima(res.fit, 0, 1, 1)\n\n\ninitial  value 3.748119 \niter   2 value 3.645637\niter   3 value 3.644385\niter   4 value 3.643230\niter   5 value 3.643132\niter   6 value 3.643092\niter   6 value 3.643092\nfinal  value 3.643092 \nconverged\ninitial  value 3.643801 \niter   2 value 3.643798\niter   3 value 3.643798\niter   4 value 3.643797\niter   4 value 3.643797\niter   4 value 3.643797\nfinal  value 3.643797 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.4556    0.6197\ns.e.   0.0825    2.0865\n\nsigma^2 estimated as 1459:  log likelihood = -511.34,  aic = 1028.67\n\n$degrees_of_freedom\n[1] 99\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.4556 0.0825 -5.5238  0.0000\nconstant   0.6197 2.0865  0.2970  0.7671\n\n$AIC\n[1] 10.18488\n\n$AICc\n[1] 10.18609\n\n$BIC\n[1] 10.26255\n\n\n\n\nCode\n# Model diagnostics for ARIMA(2, 1, 1)\nsarima(res.fit, 2, 1, 1)\n\n\ninitial  value 3.754095 \niter   2 value 3.690967\niter   3 value 3.647017\niter   4 value 3.645312\niter   5 value 3.644608\niter   6 value 3.642989\niter   7 value 3.642066\niter   8 value 3.629823\niter   9 value 3.628083\niter  10 value 3.624088\niter  11 value 3.623651\niter  12 value 3.623487\niter  13 value 3.623460\niter  14 value 3.623426\niter  15 value 3.623404\niter  16 value 3.623359\niter  17 value 3.623354\niter  18 value 3.623353\niter  18 value 3.623353\niter  18 value 3.623353\nfinal  value 3.623353 \nconverged\ninitial  value 3.620259 \niter   2 value 3.620234\niter   3 value 3.620217\niter   4 value 3.620210\niter   5 value 3.620195\niter   6 value 3.620180\niter   7 value 3.620178\niter   8 value 3.620178\niter   8 value 3.620178\niter   8 value 3.620178\nfinal  value 3.620178 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ma1  constant\n      -0.9188  -0.4247  0.4619    0.5346\ns.e.   0.2310   0.1066  0.2467    2.3238\n\nsigma^2 estimated as 1390:  log likelihood = -508.95,  aic = 1027.9\n\n$degrees_of_freedom\n[1] 97\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.9188 0.2310 -3.9785  0.0001\nar2       -0.4247 0.1066 -3.9834  0.0001\nma1        0.4619 0.2467  1.8719  0.0642\nconstant   0.5346 2.3238  0.2300  0.8185\n\n$AIC\n[1] 10.17724\n\n$AICc\n[1] 10.18137\n\n$BIC\n[1] 10.30671\n\n\n\nBoth models appear to perform well, with residuals that are normally distributed and Ljung-Box test p-values indicating no significant autocorrelation. Given that the residuals from both models meet these criteria, we will proceed with cross-validation to further compare their predictive performance and select the most appropriate model for our data.\n\n\n\nCross Validation\n\n\nCode\n# Function to forecast\nfarima1 &lt;- function(x, h){forecast(Arima(x, order=c(0,1,1)), h=h)}\nfarima2 &lt;- function(x, h){forecast(Arima(x, order=c(2,1,1)), h=h)}\n\n# 52 steps a head cross validation (a whole year)\nh &lt;- 52\nrmse1 &lt;- numeric(h)\nrmse2 &lt;- numeric(h)\n\nfor (i in 1:h) {\n  e1 &lt;- tsCV(res.fit, farima1, h=i)\n  e2 &lt;- tsCV(res.fit, farima2, h=i)\n  \n  # Calculate RMSE for each horizon\n  rmse1[i] &lt;- sqrt(mean(e1^2, na.rm=TRUE))\n  rmse2[i] &lt;- sqrt(mean(e2^2, na.rm=TRUE))\n}\n\n# Create a data frame for plotting\nrmse &lt;- data.frame(horizon = 1:h, \n                        RMSE1 = rmse1, \n                        RMSE2 = rmse2)\n\n# Plot RMSE vs Horizon\ncv1 &lt;- ggplot(data = rmse) + \n  geom_line(aes(x = horizon, y = RMSE1, colour = \"ARIMA(0,1,1)\")) + \n  geom_line(aes(x = horizon, y = RMSE2, colour = \"ARIMA(2,1,1)\")) +\n  labs(x = \"Forecast Horizon\", y = \"RMSE\", title = \"RMSE vs Forecast Horizon for ARIMA Models\") +\n   geom_point(aes(y=RMSE1,x= horizon)) + \n  geom_point(aes(y=RMSE2,x= horizon)) +\n  theme_minimal()\n\nggplotly(cv1)\n\n\n\n\n\n\nThe cross-validation plot indicates that for short-term forecasts of up to approximately 10 steps ahead, the ARIMA(0,1,1) model outperforms, exhibiting lower RMSE values. However, for longer-term forecasts beyond 10 steps, the ARIMA(2,1,1) model demonstrates improved performance, yielding more accurate predictions as evidenced by its reduced RMSE. Here I will use ARIMA(2, 1, 1).\n\n\n\nModel Equations\n\n\nCode\nfit &lt;- Arima(combined_ts[, \"total_crimes\"],order=c(2,1,1),xreg=xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"total_crimes\"] \nRegression with ARIMA(2,1,1) errors \n\nCoefficients:\n          ar1      ar2     ma1      xreg\n      -0.8941  -0.4250  0.4090  -11.1580\ns.e.   0.2532   0.1148  0.2749    4.6857\n\nsigma^2 = 1400:  log likelihood = -507.3\nAIC=1024.59   AICc=1025.22   BIC=1037.67\n\nTraining set error measures:\n                   ME     RMSE      MAE          MPE     MAPE      MASE\nTraining set 2.325262 36.48714 29.16077 -0.005415194 7.163168 0.4940828\n                    ACF1\nTraining set -0.01284592\n\n\n\nThe model equation for the ARIMAX model is $(1 - 0.9213B - 0.4422B^2)(1 - B)Y_t = (1 + 0.4743B) _t - 12.8691 X_t $\n\n\n\nForecasts\n\nFitting an ARIMA model to the COVID data\n\n\nCode\nfit_c &lt;- auto.arima(combined_ts[, \"total_cases\"]) #fitting an ARIMA model to the Covid data\nsummary(fit_c) \n\n\nSeries: combined_ts[, \"total_cases\"] \nARIMA(0,1,3) \n\nCoefficients:\n         ma1     ma2     ma3\n      0.7728  0.6654  0.2595\ns.e.  0.0928  0.1023  0.0943\n\nsigma^2 = 0.1263:  log likelihood = -37.79\nAIC=83.57   AICc=83.99   BIC=94.04\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03812665 0.3483306 0.2105525 1.441596 3.946224 0.1072581\n                    ACF1\nTraining set 0.007409321\n\n\nCode\n# Obtain the forecasts\nfc &lt;- forecast(fit_c,12)\nfc &lt;- fc$mean\n\n\n\n\nPresenting the Forecasts\n\n\nCode\nfcast &lt;- forecast(fit, xreg=fc,12) \nfcast\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2022.000       419.1293 371.1791 467.0795 345.7958 492.4627\n2022.019       409.1581 355.2247 463.0914 326.6741 491.6420\n2022.038       418.9455 359.4551 478.4358 327.9627 509.9282\n2022.058       415.7438 346.9130 484.5746 310.4761 521.0115\n2022.077       414.7071 341.1517 488.2625 302.2138 527.2003\n2022.096       416.9948 337.6859 496.3038 295.7022 538.2875\n2022.115       415.3900 330.5890 500.1910 285.6981 545.0820\n2022.135       415.8525 326.5126 505.1924 279.2189 552.4861\n2022.154       416.1211 321.9850 510.2572 272.1524 560.0898\n2022.173       415.6844 317.1563 514.2124 264.9987 566.3700\n2022.192       415.9607 313.2691 518.6523 258.9074 573.0140\n2022.212       415.8993 309.1044 522.6941 252.5707 579.2279\n\n\nCode\nautoplot(fcast, main=\"Forecast of Assault Crimes in NYC for the next quarter\") + xlab(\"Year\") +\n  ylab(\"GDP\")\n\n\n\n\n\n\n\n\nEvaluations\nThe results from the ARIMAX model were not as robust as anticipated, possibly because I did not account for seasonality in the data. Despite this, I opted not to use a SARIMAX model due to the limited amount of COVID-19 data available, which spans only approximately two years on a weekly basis. This duration may not suffice for effectively capturing and modeling seasonal patterns. To potentially enhance model performance, I plan to shift to daily data, which will provide a larger dataset that could be more suitable for a SARIMAX model, potentially yielding better results.\n\n\n\n\nOverview\n\nIn this model, I employ a VAR model to analyze the interplay between monthly aggravated assault crimes in New York City and two critical variables: unemployment rates and weather data. The choice of a VAR model stems from its ability to capture the dynamic relationships between multiple time-series datasets. It is particularly suited to understand how changes in economic conditions (reflected by unemployment rates) and environmental factors (illustrated by weather data) collectively impact crime patterns.\nThe integration of unemployment rates is premised on the well-documented correlation between economic conditions and crime. Economic downturns, often signaled by rising unemployment, can influence crime rates through various socio-economic mechanisms. By including unemployment data, the model aims to quantify this relationship and examine how economic fluctuations correlate with changes in crime rates. The unemployment data is from NYUR DATA.\nWeather data, comprising variables such as temperature, humidity, and precipitation, is included based on preliminary exploratory data analysis findings. Our initial examination revealed a notable trend: hotter weather tends to correlate with higher crime rates. This finding aligns with existing criminological theories that suggest weather can significantly impact human behavior, including the propensity to commit crimes. By incorporating weather data, our model is positioned to explore this relationship further, examining how weather patterns influence crime rates over time. The weather data is from NOAA DATA.\n\n\n\nCode\n# Read the crime data from a CSV file into a dataframe\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\n\n# Filter the data to include only 'aggravated assault' type offenses\nassault &lt;- data %&gt;%\n  filter(offense_type == \"aggravated assault\")\n# Convert the date to a Date object for easier handling\nassault$date &lt;- as.Date(assault$date_single)\n\n# Aggregate data to count the number of assaults per day\n#assault_day &lt;- assault %&gt;%\n#  mutate(n = 1) %&gt;%\n#  group_by(date) %&gt;%\n#  summarize(crimes = sum(n))\n\n\n# Aggregate data to count the number of assaults per month\nassault_month &lt;- assault %&gt;%\n  mutate(n = 1) %&gt;%\n  mutate(month = format(date, \"%Y-%m\")) %&gt;%  # Extract year and month\n  group_by(month) %&gt;%\n  summarize(crimes = sum(n))\n\n# Add missing rows for 2012\nmissing_dates &lt;- seq(ymd(\"2012-01-01\"), ymd(\"2012-12-01\"), by=\"1 month\")\nmissing_data &lt;- data.frame(month = format(missing_dates, \"%Y-%m\"), crimes = NA)\n\n# Combine your existing data with the missing data\nassault_month &lt;- bind_rows(assault_month, missing_data) %&gt;%\n  arrange(month)\n\n# Perform moving average interpolation\nimputed_data &lt;- na_ma(assault_month$crimes, k = 12, weighting = \"exponential\")\nassault_month &lt;- data.frame(\n  month = assault_month$month,\n  crimes = imputed_data\n)\n\n# Loading unemployment rate and temperature data\ntemp &lt;- read_csv(\"./dataset/tempdataNYC.csv\")\nune &lt;- read_csv(\"./dataset/NYUR.csv\")\ncolnames(temp)[1] &lt;- \"month\"\ncolnames(une)[1] &lt;- \"month\"\ntemp &lt;- temp[-(1:3), ]\ntemp$month &lt;- paste0(substr(temp$month, 1, 4), \"-\", substr(temp$month, 5, 6))\nune$month &lt;- format(as.Date(une$month), \"%Y-%m\")\n\n# Filter all three data to the overlapping time period (2007-01 to 2022-12)\ntemp &lt;- temp %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\ncolnames(temp)[2] &lt;- \"temp\"\ntemp$temp&lt;- as.numeric(temp$temp)\n\nune &lt;- une %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\n\nassault_month$month &lt;- as.Date(paste0(assault_month$month, \"-01\"), format=\"%Y-%m-%d\")\ntemp$month &lt;- as.Date(paste0(temp$month, \"-01\"), format=\"%Y-%m-%d\")\nune$month &lt;- as.Date(paste0(une$month, \"-01\"), format=\"%Y-%m-%d\")\n\n# Merging all 3 datasets by month\ncombined_data &lt;- merge(assault_month, temp, by = \"month\")\ncombined_data &lt;- merge(combined_data, une, by = \"month\")\n\n\nknitr::kable(head(combined_data))\n\n\n\n\n\nmonth\ncrimes\ntemp\nNYUR\n\n\n\n\n2007-01-01\n1207\n37.7\n4.2\n\n\n2007-02-01\n1037\n28.5\n4.2\n\n\n2007-03-01\n1366\n42.5\n4.2\n\n\n2007-04-01\n1334\n50.6\n4.3\n\n\n2007-05-01\n1665\n65.5\n4.3\n\n\n2007-06-01\n1624\n71.6\n4.4\n\n\n\n\n\nCode\n# Convert to ts\ncombined.ts&lt;-ts(combined_data,start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\n\nData Visualization\n\n\nCode\nggplot(assault_month, aes(x = month, y = crimes)) +\n  geom_line() +\n  labs(title = \"Monthly Assault Crimes\", x = \"Month\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(temp, aes(x = month, y = temp)) +\n  geom_line() +\n  labs(title = \"Monthly Temperature\", x = \"Month\", y = \"Temperature\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(une, aes(x = month, y = NYUR)) +\n  geom_line() +\n  labs(title = \"Monthly Unemployment Rate\", x = \"Month\", y = \"Unemployment Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFitting Model With VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:4)], lag.max=12, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      5      5      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n)     13.59905     12.52283     12.22540     11.94878     11.75518\nHQ(n)      13.70693     12.69544     12.46274     12.25085     12.12199\nSC(n)      13.86513     12.94856     12.81077     12.69380     12.65985\nFPE(n) 805395.32709 274582.03486 203995.77127 154774.39374 127628.08130\n                  6            7            8            9           10\nAIC(n)     11.74172     11.81653     11.87854     11.95327     11.91367\nHQ(n)      12.17325     12.31280     12.43954     12.57900     12.60412\nSC(n)      12.80603     13.04050     13.26216     13.49653     13.61658\nFPE(n) 126055.76717 136045.44180 145022.80109 156649.60926 151012.90362\n                 11           12\nAIC(n)     11.87357     11.78282\nHQ(n)      12.62876     12.60274\nSC(n)      13.73613     13.80503\nFPE(n) 145600.38674 133540.55769\n\n\n\nBased on the results, we have VAR(5) and VAR(6).\n\n\n\nVAR Model Diagnostics\n\nVAR(5)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, temp, NYUR \nDeterministic variables: both \nSample size: 187 \nLog Likelihood: -1836.982 \nRoots of the characteristic polynomial:\n0.9941 0.9941 0.917 0.917 0.7833 0.7833 0.7684 0.7684 0.7154 0.7154 0.6758 0.577 0.577 0.5558 0.5558\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1   0.39580    0.08050   4.917 2.06e-06 ***\ntemp.l1     0.32342    2.22009   0.146  0.88435    \nNYUR.l1     3.19158   10.36868   0.308  0.75860    \ncrimes.l2   0.41667    0.08595   4.848 2.80e-06 ***\ntemp.l2    -4.25760    2.50284  -1.701  0.09075 .  \nNYUR.l2    20.78858   15.23164   1.365  0.17411    \ncrimes.l3  -0.20147    0.09146  -2.203  0.02896 *  \ntemp.l3     4.62587    2.54366   1.819  0.07073 .  \nNYUR.l3   -20.17243   15.38359  -1.311  0.19153    \ncrimes.l4  -0.02354    0.08514  -0.276  0.78252    \ntemp.l4    -2.70790    2.47404  -1.095  0.27527    \nNYUR.l4     8.91772   15.21088   0.586  0.55847    \ncrimes.l5   0.26050    0.07810   3.335  0.00105 ** \ntemp.l5    -8.03288    2.22387  -3.612  0.00040 ***\nNYUR.l5    -6.11807   10.12943  -0.604  0.54666    \nconst     707.99111  127.66747   5.546 1.10e-07 ***\ntrend       0.73872    0.29418   2.511  0.01297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 109.2 on 170 degrees of freedom\nMultiple R-Squared: 0.8631, Adjusted R-squared: 0.8502 \nF-statistic: 66.97 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1  0.004908   0.002638   1.860  0.06459 .  \ntemp.l1    0.532448   0.072764   7.317 9.57e-12 ***\nNYUR.l1   -0.044589   0.339835  -0.131  0.89577    \ncrimes.l2 -0.003698   0.002817  -1.313  0.19103    \ntemp.l2    0.115794   0.082031   1.412  0.15990    \nNYUR.l2    0.511356   0.499220   1.024  0.30714    \ncrimes.l3 -0.004246   0.002998  -1.416  0.15852    \ntemp.l3    0.053238   0.083369   0.639  0.52396    \nNYUR.l3   -0.278591   0.504200  -0.553  0.58130    \ncrimes.l4  0.005518   0.002790   1.978  0.04958 *  \ntemp.l4   -0.246546   0.081087  -3.041  0.00274 ** \nNYUR.l4    0.192930   0.498539   0.387  0.69925    \ncrimes.l5 -0.001763   0.002560  -0.689  0.49200    \ntemp.l5   -0.374941   0.072888  -5.144 7.35e-07 ***\nNYUR.l5   -0.290569   0.331994  -0.875  0.38269    \nconst     49.559462   4.184324  11.844  &lt; 2e-16 ***\ntrend      0.003865   0.009642   0.401  0.68902    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.58 on 170 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.948 \nF-statistic: 212.8 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1 -0.0005777  0.0006425  -0.899  0.36984    \ntemp.l1    0.0037121  0.0177188   0.210  0.83431    \nNYUR.l1    1.0672496  0.0827532  12.897  &lt; 2e-16 ***\ncrimes.l2 -0.0007765  0.0006860  -1.132  0.25922    \ntemp.l2   -0.0041825  0.0199754  -0.209  0.83440    \nNYUR.l2   -0.3703550  0.1215650  -3.047  0.00268 ** \ncrimes.l3  0.0007349  0.0007300   1.007  0.31550    \ntemp.l3    0.0098473  0.0203012   0.485  0.62826    \nNYUR.l3    0.2819647  0.1227777   2.297  0.02286 *  \ncrimes.l4 -0.0003267  0.0006795  -0.481  0.63131    \ntemp.l4   -0.0069616  0.0197455  -0.353  0.72485    \nNYUR.l4   -0.1671504  0.1213993  -1.377  0.17036    \ncrimes.l5  0.0000471  0.0006233   0.076  0.93986    \ntemp.l5   -0.0047919  0.0177489  -0.270  0.78750    \nNYUR.l5    0.0920441  0.0808438   1.139  0.25650    \nconst      2.1165340  1.0189244   2.077  0.03928 *  \ntrend      0.0012321  0.0023479   0.525  0.60044    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8717 on 170 degrees of freedom\nMultiple R-Squared: 0.8612, Adjusted R-squared: 0.8482 \nF-statistic: 65.94 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         crimes    temp     NYUR\ncrimes 11930.07 90.3969 -38.9482\ntemp      90.40 12.8154  -0.5043\nNYUR     -38.95 -0.5043   0.7599\n\nCorrelation matrix of residuals:\n        crimes    temp    NYUR\ncrimes  1.0000  0.2312 -0.4091\ntemp    0.2312  1.0000 -0.1616\nNYUR   -0.4091 -0.1616  1.0000\n\n\n\nThe first and second lags of assaulted crimes are significant predictors, while the third and fifth lags also contribute meaningfully.\nTemperature and unemployment rate in some lags show significance, with the fifth lag of temperature being a strong predictor.\nR squared of the model is 0.861 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR5 &lt;- VAR(combined_data[,c(2:4)], p=5, type=\"both\")\nserial.test(VAR5, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR5\nChi-squared = 86.41, df = 63, p-value = 0.02682\n\n\nCode\nacf(residuals(VAR5))\n\n\n\n\n\n\nThe residuals are normally distributed.\n\n\n\nVAR(6)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, temp, NYUR \nDeterministic variables: both \nSample size: 186 \nLog Likelihood: -1816.59 \nRoots of the characteristic polynomial:\n0.9969 0.9969 0.9163 0.9163 0.7962 0.7962 0.7329 0.7329 0.7172 0.7172 0.6865 0.6865 0.5982 0.5982 0.4988 0.4988 0.3081 0.3081\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 6, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1   0.372958   0.085199   4.378 2.12e-05 ***\ntemp.l1    -1.544602   2.401473  -0.643  0.52099    \nNYUR.l1    -0.632201  10.513349  -0.060  0.95212    \ncrimes.l2   0.419482   0.086201   4.866 2.63e-06 ***\ntemp.l2    -5.089091   2.556283  -1.991  0.04814 *  \nNYUR.l2    22.740606  15.321569   1.484  0.13965    \ncrimes.l3  -0.182148   0.093164  -1.955  0.05225 .  \ntemp.l3     4.530163   2.549612   1.777  0.07743 .  \nNYUR.l3   -19.618072  15.587061  -1.259  0.20994    \ncrimes.l4  -0.030638   0.092913  -0.330  0.74200    \ntemp.l4    -1.934581   2.571100  -0.752  0.45286    \nNYUR.l4    12.290395  15.570026   0.789  0.43103    \ncrimes.l5   0.240700   0.085939   2.801  0.00570 ** \ntemp.l5    -5.703485   2.539730  -2.246  0.02604 *  \nNYUR.l5   -17.020459  15.231031  -1.117  0.26540    \ncrimes.l6  -0.002773   0.083324  -0.033  0.97349    \ntemp.l6    -4.519274   2.451896  -1.843  0.06709 .  \nNYUR.l6     9.422082  10.137986   0.929  0.35404    \nconst     979.420787 182.960937   5.353 2.84e-07 ***\ntrend       0.854254   0.302051   2.828  0.00526 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 108.7 on 166 degrees of freedom\nMultiple R-Squared: 0.8675, Adjusted R-squared: 0.8523 \nF-statistic:  57.2 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1  0.004249   0.002685   1.582  0.11548    \ntemp.l1    0.416667   0.075684   5.505 1.38e-07 ***\nNYUR.l1   -0.230559   0.331335  -0.696  0.48750    \ncrimes.l2 -0.003918   0.002717  -1.442  0.15112    \ntemp.l2    0.066620   0.080563   0.827  0.40946    \nNYUR.l2    0.546802   0.482869   1.132  0.25910    \ncrimes.l3 -0.003238   0.002936  -1.103  0.27165    \ntemp.l3    0.050881   0.080353   0.633  0.52746    \nNYUR.l3   -0.175215   0.491236  -0.357  0.72178    \ncrimes.l4  0.005472   0.002928   1.869  0.06342 .  \ntemp.l4   -0.221615   0.081030  -2.735  0.00692 ** \nNYUR.l4    0.321108   0.490699   0.654  0.51377    \ncrimes.l5 -0.002002   0.002708  -0.739  0.46093    \ntemp.l5   -0.217677   0.080041  -2.720  0.00723 ** \nNYUR.l5   -0.742252   0.480016  -1.546  0.12393    \ncrimes.l6 -0.001812   0.002626  -0.690  0.49113    \ntemp.l6   -0.267250   0.077273  -3.459  0.00069 ***\nNYUR.l6    0.370635   0.319505   1.160  0.24770    \nconst     66.334113   5.766130  11.504  &lt; 2e-16 ***\ntrend      0.010358   0.009519   1.088  0.27815    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.427 on 166 degrees of freedom\nMultiple R-Squared: 0.9572, Adjusted R-squared: 0.9524 \nF-statistic: 195.6 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1 -0.0003366  0.0006856  -0.491  0.62410    \ntemp.l1    0.0146802  0.0193250   0.760  0.44854    \nNYUR.l1    1.0910853  0.0846024  12.897  &lt; 2e-16 ***\ncrimes.l2 -0.0008230  0.0006937  -1.186  0.23712    \ntemp.l2    0.0014627  0.0205708   0.071  0.94340    \nNYUR.l2   -0.3811467  0.1232949  -3.091  0.00234 ** \ncrimes.l3  0.0006020  0.0007497   0.803  0.42315    \ntemp.l3    0.0107992  0.0205171   0.526  0.59935    \nNYUR.l3    0.2699885  0.1254313   2.152  0.03280 *  \ncrimes.l4 -0.0001659  0.0007477  -0.222  0.82469    \ntemp.l4   -0.0140609  0.0206900  -0.680  0.49770    \nNYUR.l4   -0.1716637  0.1252942  -1.370  0.17251    \ncrimes.l5  0.0001281  0.0006916   0.185  0.85322    \ntemp.l5   -0.0175707  0.0204376  -0.860  0.39118    \nNYUR.l5    0.1048116  0.1225663   0.855  0.39371    \ncrimes.l6 -0.0002326  0.0006705  -0.347  0.72913    \ntemp.l6    0.0303244  0.0197308   1.537  0.12622    \nNYUR.l6   -0.0135368  0.0815818  -0.166  0.86841    \nconst      0.4973990  1.4723127   0.338  0.73591    \ntrend      0.0007627  0.0024306   0.314  0.75408    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.875 on 166 degrees of freedom\nMultiple R-Squared: 0.8629, Adjusted R-squared: 0.8472 \nF-statistic: 54.98 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         crimes    temp     NYUR\ncrimes 11821.94 70.1280 -37.8481\ntemp      70.13 11.7420  -0.4049\nNYUR     -37.85 -0.4049   0.7655\n\nCorrelation matrix of residuals:\n        crimes    temp    NYUR\ncrimes  1.0000  0.1882 -0.3978\ntemp    0.1882  1.0000 -0.1351\nNYUR   -0.3978 -0.1351  1.0000\n\n\n\nNone of the assaulted crimes are significant predictors.\nTemperature and unemployment rate in some lags show significance, with the second lag of unemploymnet rate being a strong predictor.\nR squared of the model is 0.8675 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR6 &lt;- VAR(combined_data[,2:4], p=6, type=\"both\")\nserial.test(VAR6, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR6\nChi-squared = 77.916, df = 54, p-value = 0.01826\n\n\nCode\nacf(residuals(VAR6))\n\n\n\n\n\n\nResiduals are normally distributed.\n\n\n\n\nCross Validation\n\n\nCode\nn=length(combined_data$crimes)\nk=24\n\n#n-k=164; 164/4=41;\n\nrmse1 &lt;- matrix(NA, 168,3)\nrmse2 &lt;- matrix(NA, 168,3)\nrmse3 &lt;- matrix(NA,14,12)\nyear&lt;-c()\n\n# Convert data frame to time series object\nts_obj &lt;- ts(combined_data[, c(2:4)], star=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nst &lt;- tsp(ts_obj )[1]+(k-1)/12 \n\n\nfor(i in 1:14)\n{\n  \n  xtrain &lt;- window(ts_obj, end=st + i-1)\n  xtest &lt;- window(ts_obj, start=st + (i-1) + 1/12, end=st + i)\n  \n  ######## first Model ############\n  fit &lt;- VAR(ts_obj, p=5, type='both')\n  fcast &lt;- predict(fit, n.ahead = 12)\n  \n  fc&lt;-fcast$fcst$crimes\n  ft&lt;-fcast$fcst$temp\n  fu&lt;-fcast$fcst$NYUR\n  \n  ff&lt;-data.frame(fc[,1],ft[,1],fu[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff&lt;-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,5, 9,13\n  b= 12*i #4, 8, 12\n  \n  ## it's going from 1978 Q1(1)-Q4(4); 1979 Q1(5)-Q4(8); 1980 Q1(9)-Q4(12)....\n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  &lt;-sqrt((ff-xtest)^2)\n  \n  \n  ######## Second Model ############\n  fit2 &lt;- vars::VAR(ts_obj, p=6, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 12)\n  \n  fc&lt;-fcast2$fcst$crimes\n  ft&lt;-fcast2$fcst$temp\n  fu&lt;-fcast2$fcst$NYUR\n  \n  ff2&lt;-data.frame(fc[,1],ft[,1],fu[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12\n  \n  ff2&lt;-ts(ff2,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i\n  rmse2[c(a:b),]  &lt;-sqrt((ff2-xtest)^2)\n}\n\nyr = rep(c(2009:2022),each =12) #year\nmo = rep(paste0(\"M\",1:12),14) #quarter\n\nrmse1 = data.frame(yr,mo,rmse1)\nnames(rmse1) =c(\"Year\", \"Month\",\"Crimes\",\"Temperature\",\"Unemployment\")\nrmse2 = data.frame(yr,mo,rmse2)\nnames(rmse2) =c(\"Year\", \"Month\",\"Crimes\",\"Temperature\",\"Unemployment\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Crimes),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Crimes),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Assault Crimes\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Temperature),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Temperature),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Temperature\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Unemployment),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Unemployment),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Unemployment rate\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\nThe blue fit which is the VAR(5) model, consistently has a lower RMSE compared with VAR(6) which is the red line, VAR(6) is a better fit.\n\n\n\nForecasts\n\nFrom the cross validation from last part, we have concluded that VAR(5) is a better model, here we are going to forecast using VAR(5)\n\n\n\nCode\nforecast(fit)\n\n\ncrimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2023       1860.974 1720.997 2000.951 1646.897 2075.051\nFeb 2023       1913.238 1763.062 2063.414 1683.564 2142.912\nMar 2023       1982.215 1815.624 2148.806 1727.436 2236.995\nApr 2023       2073.193 1905.060 2241.325 1816.056 2330.329\nMay 2023       2204.415 2033.797 2375.033 1943.477 2465.352\nJun 2023       2280.025 2102.987 2457.062 2009.270 2550.780\nJul 2023       2314.938 2132.891 2496.986 2036.520 2593.356\nAug 2023       2283.671 2094.032 2473.310 1993.643 2573.698\nSep 2023       2209.151 2015.958 2402.344 1913.687 2504.614\nOct 2023       2097.829 1902.323 2293.336 1798.828 2396.830\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2023       34.12223 29.53445 38.71001 27.10582 41.13863\nFeb 2023       37.27250 31.94967 42.59532 29.13193 45.41306\nMar 2023       44.27527 38.64164 49.90890 35.65938 52.89116\nApr 2023       53.66847 47.84361 59.49333 44.76011 62.57682\nMay 2023       65.07287 59.21638 70.92937 56.11614 74.02961\nJun 2023       73.93957 67.65635 80.22278 64.33022 83.54891\nJul 2023       77.42159 70.57498 84.26820 66.95060 87.89257\nAug 2023       75.67472 68.33514 83.01430 64.44981 86.89963\nSep 2023       69.21693 61.52088 76.91297 57.44684 80.98702\nOct 2023       59.05856 51.25563 66.86148 47.12501 70.99211\n\nNYUR\n         Point Forecast    Lo 80    Hi 80       Lo 95    Hi 95\nJan 2023       3.977834 2.860664 5.095004  2.26926943 5.686398\nFeb 2023       3.956187 2.298432 5.613943  1.42086862 6.491506\nMar 2023       4.024649 2.104573 5.944725  1.08814641 6.961152\nApr 2023       4.031821 1.927150 6.136491  0.81300473 7.250636\nMay 2023       4.111567 1.881941 6.341192  0.70164863 7.521485\nJun 2023       4.161144 1.843274 6.479013  0.61626852 7.706019\nJul 2023       4.089418 1.692760 6.486076  0.42404521 7.754791\nAug 2023       4.003606 1.539134 6.468078  0.23452178 7.772691\nSep 2023       3.890928 1.371228 6.410629  0.03737908 7.744477\nOct 2023       3.758231 1.190277 6.326186 -0.16911594 7.685579\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nEvaluations\nthe historical data shows fluctuations with no clear long-term trend. The forecast suggests a stable outlook with a slight uptick towards the end, within a moderate confidence interval.\n\n\n\n\n\nOverview\n\nIn this model, we employs a Vector Autoregression (VAR) model to scrutinize the weekly total crime rates in New York City, contextualized by the significant societal disruptions of the COVID-19 pandemic. The model is designed to understand how the pandemic may have altered crime trends, leveraging the robust COVID-19 datasets available from the covidcast package in R. These datasets capture a range of COVID-19 metrics, serving as vital exogenous inputs to our model, thus facilitating a comprehensive examination of the pandemic’s imprint on urban crime patterns. $Crimes ~ Covid-19 $\n\n\n\nCode\n# Aggregate data to count the number of assaults per week\nassault_week &lt;- assault_day %&gt;%\n  mutate(week = floor_date(as.Date(date), unit = \"week\")) %&gt;%\n  group_by(week) %&gt;%\n  summarize(crimes = sum(crime_numbers), .groups = \"drop\")\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \n\ncases_nyc_weekly$total_cases &lt;- cases_nyc_weekly$total_cases +1\n\n# Merge data sets\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\nassault_week &lt;- assault_week %&gt;%\n  filter(week &gt;= \"2020-01-19\" & week &lt;= \"2022-12-25\")\n\n\n# Merging the two datasets on the 'date' column\ncombined_data &lt;- merge(assault_week, cases_nyc_weekly, by = \"week\")\nhead(combined_data)\n\n\n        week crimes total_cases\n1 2020-01-19    392           1\n2 2020-01-26    366           1\n3 2020-02-02    394           1\n4 2020-02-09    343           1\n5 2020-02-16    358           1\n6 2020-02-23    366           1\n\n\n\n\nData Visualization\n\n\nCode\nggplot(assault_week, aes(x = week, y = crimes)) +\n  geom_line() +\n  labs(title = \"Weekly Assault Crimes\", x = \"Date\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line() +\n  labs(title = \"Weekly Covid-19 total cases\", x = \"Date\", y = \"Cases\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFitting Model with VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:3)], lag.max=14, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     4      4      2      4 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 2.369775e+01 2.315656e+01 2.310645e+01 2.302551e+01 2.306252e+01\nHQ(n)  2.376605e+01 2.325903e+01 2.324306e+01 2.319628e+01 2.326744e+01\nSC(n)  2.386584e+01 2.340871e+01 2.344263e+01 2.344575e+01 2.356680e+01\nFPE(n) 1.958006e+10 1.139761e+10 1.084202e+10 1.000148e+10 1.038223e+10\n                  6            7            8            9           10\nAIC(n) 2.304852e+01 2.307514e+01 2.307862e+01 2.309588e+01 2.313233e+01\nHQ(n)  2.328760e+01 2.334837e+01 2.338601e+01 2.343742e+01 2.350802e+01\nSC(n)  2.363685e+01 2.374751e+01 2.383504e+01 2.393635e+01 2.405684e+01\nFPE(n) 1.024302e+10 1.052630e+10 1.057206e+10 1.076770e+10 1.118209e+10\n                 11           12           13           14\nAIC(n) 2.312695e+01 2.317572e+01 2.321101e+01 2.326620e+01\nHQ(n)  2.353680e+01 2.361973e+01 2.368917e+01 2.377851e+01\nSC(n)  2.413551e+01 2.426833e+01 2.438767e+01 2.452690e+01\nFPE(n) 1.113977e+10 1.171862e+10 1.216641e+10 1.288963e+10\n\n\n\nBased on the results, we have VAR(2) and VAR(3).\n\n\nVAR(2)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=2, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, total_cases \nDeterministic variables: both \nSample size: 152 \nLog Likelihood: -2175.313 \nRoots of the characteristic polynomial:\n0.868 0.7836 0.7836 0.3379\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 2, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + total_cases.l1 + crimes.l2 + total_cases.l2 + const + trend \n\n                Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1       0.523531   0.079728   6.566 8.45e-10 ***\ntotal_cases.l1 -0.001973   0.001019  -1.936 0.054751 .  \ncrimes.l2       0.293939   0.079863   3.681 0.000327 ***\ntotal_cases.l2  0.001517   0.001027   1.477 0.141748    \nconst          70.525157  23.145329   3.047 0.002743 ** \ntrend           0.171422   0.103230   1.661 0.098944 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 39.52 on 146 degrees of freedom\nMultiple R-Squared: 0.7395, Adjusted R-squared: 0.7306 \nF-statistic: 82.89 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = crimes.l1 + total_cases.l1 + crimes.l2 + total_cases.l2 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1        -1.78359    5.10932  -0.349   0.7275    \ntotal_cases.l1    1.42265    0.06531  21.783   &lt;2e-16 ***\ncrimes.l2        -2.17598    5.11800  -0.425   0.6713    \ntotal_cases.l2   -0.62395    0.06581  -9.481   &lt;2e-16 ***\nconst          1672.31419 1483.26091   1.127   0.2614    \ntrend            11.00314    6.61549   1.663   0.0984 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2533 on 146 degrees of freedom\nMultiple R-Squared: 0.8698, Adjusted R-squared: 0.8653 \nF-statistic:   195 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            crimes total_cases\ncrimes        1562       -1663\ntotal_cases  -1663     6414119\n\nCorrelation matrix of residuals:\n              crimes total_cases\ncrimes       1.00000    -0.01662\ntotal_cases -0.01662     1.00000\n\n\n\nResiduals\n\n\n\nCode\nVAR2 &lt;- VAR(combined_data[,c(2:3)], p=2, type=\"both\")\nserial.test(VAR2, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR2\nChi-squared = 203.99, df = 200, p-value = 0.4085\n\n\nCode\nacf(residuals(VAR2))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\nVAR(3)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=3, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, total_cases \nDeterministic variables: both \nSample size: 151 \nLog Likelihood: -2156.034 \nRoots of the characteristic polynomial:\n0.9287 0.8302 0.8302 0.5282 0.5282 0.1334\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 3, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + total_cases.l1 + crimes.l2 + total_cases.l2 + crimes.l3 + total_cases.l3 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1       0.4552498  0.0813176   5.598 1.07e-07 ***\ntotal_cases.l1 -0.0023624  0.0012637  -1.869  0.06360 .  \ncrimes.l2       0.1589059  0.0885834   1.794  0.07495 .  \ntotal_cases.l2  0.0022219  0.0020573   1.080  0.28197    \ncrimes.l3       0.2538277  0.0815577   3.112  0.00224 ** \ntotal_cases.l3 -0.0003402  0.0012809  -0.266  0.79091    \nconst          52.3295381 23.5843610   2.219  0.02807 *  \ntrend           0.1163460  0.1052420   1.106  0.27080    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 38.57 on 143 degrees of freedom\nMultiple R-Squared: 0.7562, Adjusted R-squared: 0.7442 \nF-statistic: 63.35 on 7 and 143 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = crimes.l1 + total_cases.l1 + crimes.l2 + total_cases.l2 + crimes.l3 + total_cases.l3 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1        -1.67958    5.37118  -0.313 0.754962    \ntotal_cases.l1    1.36434    0.08347  16.345  &lt; 2e-16 ***\ncrimes.l2        -3.13685    5.85110  -0.536 0.592714    \ntotal_cases.l2   -0.49033    0.13589  -3.608 0.000425 ***\ncrimes.l3         0.48614    5.38704   0.090 0.928220    \ntotal_cases.l3   -0.09452    0.08461  -1.117 0.265777    \nconst          1825.32786 1557.79246   1.172 0.243250    \ntrend            12.09026    6.95143   1.739 0.084143 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2547 on 143 degrees of freedom\nMultiple R-Squared: 0.8707, Adjusted R-squared: 0.8643 \nF-statistic: 137.5 on 7 and 143 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            crimes total_cases\ncrimes        1487       -2385\ntotal_cases  -2385     6489300\n\nCorrelation matrix of residuals:\n              crimes total_cases\ncrimes       1.00000    -0.02428\ntotal_cases -0.02428     1.00000\n\n\n\nBoth models looks good based on the model diagnostics, most coefficients are significant, combined with a high R square of above 0.7.\nResiduals\n\n\n\nCode\nVAR3 &lt;- VAR(combined_data[,c(2:3)], p=3, type=\"both\")\nserial.test(VAR3, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR3\nChi-squared = 174.27, df = 196, p-value = 0.8659\n\n\nCode\nacf(residuals(VAR3))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\n\nCross Validation\n\nI’m having issues with cross validation for this model currently, code always generate error, will try my best to figure out the cause.\n\n\n\nForecasts\n\nVAR(2)\n\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=2, type='both')\nforecast(fit)\n\n\ncrimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.011       440.9817 390.3351 491.6283 363.5244 518.4390\n2023.030       445.1257 387.5514 502.6999 357.0734 533.1779\n2023.049       456.2213 391.2842 521.1585 356.9085 555.5342\n2023.068       462.4255 392.9244 531.9266 356.1327 568.7183\n2023.088       468.7504 395.7554 541.7455 357.1141 580.3868\n2023.107       474.1915 398.7298 549.6532 358.7828 589.6002\n2023.126       479.4747 402.2120 556.7374 361.3116 597.6378\n2023.145       484.5061 405.9327 563.0794 364.3385 604.6737\n2023.165       489.3174 409.7765 568.8583 367.6701 610.9647\n2023.184       493.8254 413.5680 574.0828 371.0823 616.5685\n\ntotal_cases\n         Point Forecast       Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3814.821   569.14936  7060.493 -1149.005  8778.647\n2023.030       4896.937  -749.05030 10542.924 -3737.855 13531.729\n2023.049       6232.675 -1030.43991 13495.790 -4875.301 17340.650\n2023.068       7439.970  -694.77698 15574.717 -5001.052 19880.992\n2023.088       8299.887  -189.45135 16789.225 -4683.435 21283.209\n2023.107       8756.177   170.79572 17341.558 -4374.031 21886.384\n2023.126       8856.309   248.15359 17464.465 -4308.729 22021.347\n2023.145       8703.801    65.18497 17342.417 -4507.822 21915.425\n2023.165       8414.892  -268.83948 17098.624 -4865.729 21695.514\n2023.184       8090.507  -635.12812 16816.143 -5254.201 21435.215\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\nVAR(3)\n\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=3, type='both')\nforecast(fit)\n\n\ncrimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.011       438.2544 388.8291 487.6797 362.6649 513.8439\n2023.030       445.0616 390.1339 499.9894 361.0569 529.0664\n2023.049       441.1563 382.8150 499.4975 351.9310 530.3815\n2023.068       448.1945 384.3734 512.0155 350.5886 545.8004\n2023.088       452.2342 384.9120 519.5563 349.2738 555.1945\n2023.107       454.3665 384.4263 524.3067 347.4022 561.3309\n2023.126       458.2928 385.9218 530.6637 347.6110 568.9746\n2023.145       462.0882 387.7876 536.3889 348.4552 575.7212\n2023.165       465.6141 389.7417 541.4864 349.5773 581.6508\n2023.184       469.3575 392.1348 546.5802 351.2556 587.4594\n\ntotal_cases\n         Point Forecast      Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3973.129  708.49069  7237.766 -1019.704  8965.961\n2023.030       5119.973 -404.64483 10644.592 -3329.201 13569.148\n2023.049       6494.298 -635.44599 13624.042 -4409.705 17398.301\n2023.068       7785.952 -249.68662 15821.592 -4503.497 20075.402\n2023.088       8781.751  370.79948 17192.702 -4081.689 21645.191\n2023.107       9358.436  853.78750 17863.084 -3648.301 22365.173\n2023.126       9534.122 1008.88758 18059.357 -3504.099 22572.343\n2023.145       9397.693  822.11178 17973.275 -3717.527 22512.914\n2023.165       9065.340  404.26036 17726.420 -4180.638 22311.318\n2023.184       8658.359  -87.27911 17403.996 -4716.940 22033.657\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\nEvaluations\n\nVAR(2) Forecast: The forecast for assault crimes indicates a stable trend with a slight increase toward the end, and the confidence interval is moderate.\n\nVAR(3) Forecast: The forecast for assault crimes remains stable with a slightly positive trend toward the end, similar to VAR(2), but with a narrower confidence interval, suggesting more certainty in the prediction."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#overview",
    "href": "ARIMAX_SARIMAX_VAR.html#overview",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Overview",
    "text": "Overview\nIn this model, we focus on analyzing the weekly total violent crimes in New York City using an ARIMAX model. The unprecedented global crisis brought on by the COVID-19 pandemic has had profound impacts on various facets of society, including crime patterns. Recognizing the potential influence of the pandemic on crime dynamics, our model integrates COVID-19 data as a critical exogenous component. This data is extracted using the covidcast package in R, which provides a comprehensive dataset of COVID-19 metrics."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-auto.arima",
    "href": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-auto.arima",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting Model With Auto.arima()",
    "text": "Fitting Model With Auto.arima()\n\nLoad in violent crime data\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\nlibrary(covidcast)\nlibrary(zoo)\nlibrary(imputeTS)\nlibrary(vars)\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n\n\n\nCOVID-19 data extraction and cleaning\n\nExtract COVID-19 data using covid_cast package in R\n\n\n\nCode\n# Fetching COVID-19 cases data for New York City for the specified timeframe\ncases_nyc &lt;- covidcast_signal(\"jhu-csse\", \"confirmed_incidence_num\",\n                              start_day = \"2020-01-01\", end_day = \"2022-12-31\",\n                              geo_type = \"county\", geo_values = \"36061\") # 36061 is the FIPS code for New York County\n\n# Cleaning and preparing the data\ncases_nyc_cleaned &lt;- cases_nyc %&gt;%\n  dplyr::select(time_value, value) %&gt;%\n  dplyr::arrange(time_value)\n\n# Ensure the date column is in the Date format\ncases_nyc_cleaned$time_value &lt;- as.Date(cases_nyc_cleaned$time_value)\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \nhead(cases_nyc_weekly)\n\n\n# A tibble: 6 × 2\n  week       total_cases\n  &lt;date&gt;           &lt;dbl&gt;\n1 2020-01-19           0\n2 2020-01-26           0\n3 2020-02-02           0\n4 2020-02-09           0\n5 2020-02-16           0\n6 2020-02-23           0\n\n\nCode\n#sum(is.na(cases_nyc_weekly)) = 0 indicates no missing value\n\n\n\n\nCode\n# Visualization of Covid data of NYC\ncovid_plot &lt;- ggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Weekly COVID-19 Cases in New York City\",\n       x = \"Date\",\n       y = \"Cases\") +\n  theme_minimal()\n\n# Convert to interactive plot with plotly\nggplotly(covid_plot, tooltip = \"text\")\n\n\n\n\n\n\n\nFilter weekly crime data and COVID-19 data to have matching time frame\n\n\n\nCode\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\ndata_f &lt;- data_w %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Filter cases_nyc_weekly to the same time period\ncases_nyc_weekly_filtered &lt;- cases_nyc_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Merging the two datasets on the 'week' column\ncombined_data &lt;- merge(data_f, cases_nyc_weekly_filtered, by = \"week\")\n\ncombined_data &lt;- combined_data[, -2] \nhead(combined_data)\n\n\n        week Total_Crimes total_cases\n1 2020-01-19          699           0\n2 2020-01-26          726           0\n3 2020-02-02          727           0\n4 2020-02-09          681           0\n5 2020-02-16          658           0\n6 2020-02-23          676           0\n\n\nCode\n# Convert to time series\ncombined_ts &lt;- ts(combined_data, start = c(2020, 3), frequency = 52)\n\n\n\n\nCode\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nWe need a log transform on the Covid-19 data due to a dramatic increase towards the end of the time series.\n\n\n\nCode\n# Apply log plus 1 transformation to the COVID-19 cases data to accommodate for 0s\ncombined_ts[, 3] &lt;- log(combined_ts[, 3] + 1)\n\n\n\n\nCode\n# Plot the transformed data\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and transformed Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nThe transformed COVID-19 time series have more stability.\n\n\n\nFit the model\n\n\nCode\nxreg &lt;- combined_ts[, \"total_cases\"]\n\nfit &lt;- auto.arima(combined_ts[, \"Total_Crimes\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"Total_Crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.4809  -26.7319\ns.e.   0.0841    6.4813\n\nsigma^2 = 2839:  log likelihood = -543.98\nAIC=1093.96   AICc=1094.2   BIC=1101.8\n\nTraining set error measures:\n                   ME    RMSE      MAE       MPE     MAPE      MASE        ACF1\nTraining set 6.011042 52.4972 41.53063 0.4133057 5.926572 0.3888636 -0.04368721\n\n\n\nModel Residuals\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1) errors\nQ* = 14.12, df = 19, p-value = 0.7767\n\nModel df: 1.   Total lags used: 20\n\n\n\n\nModel coefficients\n\nThe model computed by auto.arima() is a ARIMAX model with ARIMA(0,1,1), the coefficient for the moving average term is -0.4809, which suggests a moderate negative effect from the previous error term.\nThe exogenous coefficient (representing the COVID-19 cases) is -26.7319, implying that as the COVID-19 cases increase, the number of total crimes decreases, holding other factors constant.\nThe negative exogenous coefficient might raise some questions about the relationship between COVID-19 cases and crime rates. It could suggest that an increase in COVID-19 cases leads to a reduction in reported violent crimes in NYC, possibly due to lockdowns or reduced public activity.\n\n\n\nResidual Analysis\n\nThe Ljung-Box test has a p-value of 0.7767, which is well above 0.05, suggesting that the residuals are independently distributed and that there is no significant autocorrelation at lags up to 20.\nThe residual plots do not show any obvious patterns or trends, which is good as it suggests that the model has captured the data’s structure adequately. The histogram of residuals, overlaid with a normal distribution, seems to show that residuals are approximately normally distributed."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fitting-manually",
    "href": "ARIMAX_SARIMAX_VAR.html#fitting-manually",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting Manually",
    "text": "Fitting Manually\n\nLinear Regression\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm( Total_Crimes ~ total_cases, data=combined_ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = Total_Crimes ~ total_cases, data = combined_ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-241.199  -88.522    8.395   86.683  228.474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  781.260     36.680  21.299   &lt;2e-16 ***\ntotal_cases   -9.739      5.179  -1.881   0.0629 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 109.4 on 100 degrees of freedom\nMultiple R-squared:  0.03416,   Adjusted R-squared:  0.0245 \nF-statistic: 3.537 on 1 and 100 DF,  p-value: 0.06292\n\n\n\nWhile the linear regression model does not yield significant coefficients for COVID-19 cases, indicating a weak linear relationship, the variable may still be influential in an ARIMAX model.\n\n\n\nExtract Residuals\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit&lt;-ts(residuals(fit.reg),start=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\n\n\n\n\nResiduals ACF/PACF\n\n\nCode\n############## Then look at the residuals ############\nggAcf(res.fit, 50)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit,50)\n\n\n\n\n\n\nThe ACF plot of regression models residual has clear correlation, differencing needs to be applied for stationary purpose.\n\n\n\nDifferencing\n\n\nCode\nres.fit %&gt;% diff() %&gt;% ggtsdisplay() \n\n\n\n\n\n\nPotential Model Parameters\n\np = 0, 1, 2\nd = 0, 1\nq = 0, 1, 2\n\n\n\n\nFitting Model Parameters\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=0,1,2 : 3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 0:1)# d=0,1 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1206.490\n1214.365\n1206.735\n\n\n0\n1\n0\n1116.052\n1121.282\n1116.175\n\n\n0\n0\n1\n1161.659\n1172.159\n1162.071\n\n\n0\n1\n1\n1099.666\n1107.511\n1099.913\n\n\n0\n0\n2\n1142.726\n1155.851\n1143.351\n\n\n0\n1\n2\n1100.686\n1111.146\n1101.102\n\n\n1\n0\n0\n1118.492\n1128.992\n1118.904\n\n\n1\n1\n0\n1100.204\n1108.050\n1100.452\n\n\n1\n0\n1\n1110.109\n1123.234\n1110.734\n\n\n1\n1\n1\n1100.844\n1111.304\n1101.261\n\n\n1\n0\n2\n1110.436\n1126.186\n1111.321\n\n\n1\n1\n2\n1102.641\n1115.716\n1103.272\n\n\n2\n0\n0\n1109.340\n1122.465\n1109.965\n\n\n2\n1\n0\n1100.485\n1110.945\n1100.901\n\n\n2\n0\n1\n1110.944\n1126.693\n1111.828\n\n\n2\n1\n1\n1101.798\n1114.873\n1102.429\n\n\n2\n0\n2\n1104.548\n1122.923\n1105.739\n\n\n2\n1\n2\n1103.774\n1119.465\n1104.668\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n  p d q      AIC      BIC     AICc\n4 0 1 1 1099.666 1107.511 1099.913\n\n\n\nARIMAX(0,1,1) is the best model with lowest AIC, BIC, AICc.\n\n\n\nModel Diagnostics\n\n\nCode\n# Model diagnostics for ARIMA(0, 1, 1)\nsarima(res.fit, 0, 1, 1)\n\n\ninitial  value 4.086270 \niter   2 value 3.994504\niter   3 value 3.994491\niter   4 value 3.994453\niter   5 value 3.994437\niter   6 value 3.994430\niter   6 value 3.994430\nfinal  value 3.994430 \nconverged\ninitial  value 3.995257 \niter   2 value 3.995251\niter   3 value 3.995249\niter   4 value 3.995249\niter   5 value 3.995248\niter   5 value 3.995248\niter   5 value 3.995248\nfinal  value 3.995248 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.4086    1.3989\ns.e.   0.0824    3.2167\n\nsigma^2 estimated as 2947:  log likelihood = -546.83,  aic = 1099.67\n\n$degrees_of_freedom\n[1] 99\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.4086 0.0824 -4.9591  0.0000\nconstant   1.3989 3.2167  0.4349  0.6646\n\n$AIC\n[1] 10.88778\n\n$AICc\n[1] 10.88899\n\n$BIC\n[1] 10.96546\n\n\n\nThe model appears to perform well, with residuals that are normally distributed and Ljung-Box test p-values indicating no significant autocorrelation. Given that the residuals from both models meet these criteria.\n\n\n\nModel Equations\n\n\nCode\nfit &lt;- Arima(combined_ts[, \"Total_Crimes\"],order=c(0,1,1),xreg=xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"Total_Crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.4809  -26.7319\ns.e.   0.0841    6.4813\n\nsigma^2 = 2839:  log likelihood = -543.98\nAIC=1093.96   AICc=1094.2   BIC=1101.8\n\nTraining set error measures:\n                   ME    RMSE      MAE       MPE     MAPE      MASE        ACF1\nTraining set 6.011042 52.4972 41.53063 0.4133057 5.926572 0.3888636 -0.04368721\n\n\n\n\\((1 - 0B) (1 - 1B) y_t = -26.7319x_t + a_t - 0.4809a_{t-1}\\)\n\n\n\nFitting an ARIMA model to the COVID-19 data\n\n\nCode\nfit_c &lt;- auto.arima(combined_ts[, \"total_cases\"]) #fitting an ARIMA model to the Covid data\nsummary(fit_c) \n\n\nSeries: combined_ts[, \"total_cases\"] \nARIMA(0,1,3) \n\nCoefficients:\n         ma1     ma2     ma3\n      0.7728  0.6654  0.2595\ns.e.  0.0928  0.1023  0.0943\n\nsigma^2 = 0.1263:  log likelihood = -37.79\nAIC=83.57   AICc=83.99   BIC=94.04\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03812665 0.3483306 0.2105525 1.441596 3.946224 0.1072581\n                    ACF1\nTraining set 0.007409321\n\n\nCode\n# Obtain the forecasts\nfc &lt;- forecast(fit_c,12)\nfc &lt;- fc$mean\n\n\n\n\nForecasts\n\n\nCode\nfcast &lt;- forecast(fit, xreg=fc,12) \nfcast\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2022.000       734.1273 665.8376 802.4169 629.6873 838.5672\n2022.019       738.4248 661.4833 815.3664 620.7530 856.0967\n2022.038       739.8946 655.1803 824.6090 610.3352 869.4540\n2022.058       739.8946 648.0630 831.7262 599.4503 880.3389\n2022.077       739.8946 641.4590 838.3302 589.3504 890.4389\n2022.096       739.8946 635.2710 844.5182 579.8867 899.9026\n2022.115       739.8946 629.4292 850.3601 570.9523 908.8370\n2022.135       739.8946 623.8811 855.9082 562.4672 917.3220\n2022.154       739.8946 618.5865 861.2028 554.3698 925.4194\n2022.173       739.8946 613.5135 866.2757 546.6114 933.1779\n2022.192       739.8946 608.6364 871.1528 539.1525 940.6367\n2022.212       739.8946 603.9342 875.8550 531.9611 947.8282\n\n\nCode\nautoplot(fcast, main=\"Forecast of violent Crimes in NYC for the next quarter\") + xlab(\"Year\") +\n  ylab(\"GDP\")\n\n\n\n\n\n\n\nEvaluations\nThe results from the ARIMAX model were not as robust as anticipated, possibly because I did not account for seasonality in the data. Despite this, I opted not to use a SARIMAX model due to the limited amount of COVID-19 data available, which spans only approximately two years on a weekly basis. This duration may not be sufficient for effectively capturing and modeling seasonal patterns. To potentially enhance model performance, I plan to shift to daily data, which will provide a larger data set that could be more suitable for a SARIMAX model, potentially yielding better results."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#overview-1",
    "href": "ARIMAX_SARIMAX_VAR.html#overview-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Overview",
    "text": "Overview\n\nIn this model, I employ a VAR model to analyze the interplay between monthly violent crimes in New York City and two critical variables: unemployment rates and weather data. The choice of a VAR model stems from its ability to capture the dynamic relationships between multiple time-series datasets. It is particularly suited to understand how changes in economic conditions (reflected by unemployment rates) and environmental factors (illustrated by weather data) collectively impact crime patterns.\nThe integration of unemployment rates is premised on the well-documented correlation between economic conditions and crime. Economic downturns, often signaled by rising unemployment, can influence crime rates through various socio-economic mechanisms. By including unemployment data, the model aims to quantify this relationship and examine how economic fluctuations correlate with changes in crime rates. The unemployment data is from NYUR DATA.\nWeather data, comprising variables such as temperature, humidity, and precipitation, is included based on preliminary exploratory data analysis findings. Our initial examination revealed a notable trend: hotter weather tends to correlate with higher crime rates. This finding aligns with existing criminological theories that suggest weather can significantly impact human behavior, including the propensity to commit crimes. By incorporating weather data, our model is positioned to explore this relationship further, examining how weather patterns influence crime rates over time. The weather data is from NOAA DATA."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#load-in-violent-crime-data-1",
    "href": "ARIMAX_SARIMAX_VAR.html#load-in-violent-crime-data-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Load in violent crime data",
    "text": "Load in violent crime data\n\n\nCode\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n\ndata_m &lt;- data_t %&gt;%\n  mutate(month = floor_date(date_single, \"month\")) %&gt;%\n  group_by(month) %&gt;%\n  summarise(Total_Crimes = sum(Total_Crimes))\n\n# Loading unemployment rate and temperature data\ntemp &lt;- read_csv(\"./dataset/tempdataNYC.csv\")\nune &lt;- read_csv(\"./dataset/NYUR.csv\")\ncolnames(temp)[1] &lt;- \"month\"\ncolnames(une)[1] &lt;- \"month\"\ntemp &lt;- temp[-(1:3), ]\ntemp$month &lt;- paste0(substr(temp$month, 1, 4), \"-\", substr(temp$month, 5, 6))\nune$month &lt;- format(as.Date(une$month), \"%Y-%m\")\n\n# Filter all three data to the overlapping time period (2007-01 to 2022-12)\ntemp &lt;- temp %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\ncolnames(temp)[2] &lt;- \"temp\"\ntemp$temp&lt;- as.numeric(temp$temp)\n\nune &lt;- une %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\n\ndata_m$month &lt;- as.Date(paste0(data_m$month, \"-01\"), format=\"%Y-%m-%d\")\ntemp$month &lt;- as.Date(paste0(temp$month, \"-01\"), format=\"%Y-%m-%d\")\nune$month &lt;- as.Date(paste0(une$month, \"-01\"), format=\"%Y-%m-%d\")\n\n# Merging all 3 datasets by month\ncombined_data &lt;- merge(data_m, temp, by = \"month\")\ncombined_data &lt;- merge(combined_data, une, by = \"month\")\n\n\nknitr::kable(head(combined_data))\n\n\n\n\n\nmonth\nTotal_Crimes\ntemp\nNYUR\n\n\n\n\n2007-01-01\n3066\n37.7\n4.2\n\n\n2007-02-01\n2415\n28.5\n4.2\n\n\n2007-03-01\n2910\n42.5\n4.2\n\n\n2007-04-01\n2904\n50.6\n4.3\n\n\n2007-05-01\n3469\n65.5\n4.3\n\n\n2007-06-01\n3469\n71.6\n4.4\n\n\n\n\n\nCode\n# Convert to ts\ncombined.ts&lt;-ts(combined_data,start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#data-visualization",
    "href": "ARIMAX_SARIMAX_VAR.html#data-visualization",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\nCode\nggplot(data_m, aes(x = month, y = Total_Crimes)) +\n  geom_line() +\n  labs(title = \"Monthly violent Crimes\", x = \"Month\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(temp, aes(x = month, y = temp)) +\n  geom_line() +\n  labs(title = \"Monthly Temperature\", x = \"Month\", y = \"Temperature\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(une, aes(x = month, y = NYUR)) +\n  geom_line() +\n  labs(title = \"Monthly Unemployment Rate\", x = \"Month\", y = \"Unemployment Rate\") +\n  theme_minimal()"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-varselect",
    "href": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-varselect",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting Model With VARselect()",
    "text": "Fitting Model With VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:4)], lag.max=12, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    12      5      5     12 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 1.493165e+01 1.392855e+01     13.61708     13.36394     13.09146\nHQ(n)  1.504485e+01 1.410967e+01     13.86612     13.68090     13.47634\nSC(n)  1.521057e+01 1.437483e+01     14.23071     14.14493     14.03981\nFPE(n) 3.053199e+06 1.119919e+06 820475.05442 637366.60205 485796.32954\n                  6            7            8            9           10\nAIC(n)     13.05646     13.14066     13.19271     13.26704     13.25517\nHQ(n)      13.50926     13.66139     13.78136     13.92361     13.97965\nSC(n)      14.17216     14.42372     14.64312     14.88481     15.04029\nFPE(n) 469704.17422 511882.37659 540491.91112 583918.50481 579137.70401\n                 11           12\nAIC(n)     13.17201     12.85490\nHQ(n)      13.96442     13.71523\nSC(n)      15.12448     14.97473\nFPE(n) 535296.35122 391906.49202\n\n\n\nBased on the results, we have VAR(5) and VAR(12)."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-1",
    "href": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nVAR(5)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, temp, NYUR \nDeterministic variables: both \nSample size: 175 \nLog Likelihood: -1834.341 \nRoots of the characteristic polynomial:\n0.9941 0.9941 0.9557 0.907 0.8199 0.8199 0.7798 0.7798 0.7629 0.7629 0.6902 0.5647 0.5647 0.5027 0.5027\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   0.30162    0.08415   3.584 0.000450 ***\ntemp.l1           6.97573    4.41584   1.580 0.116174    \nNYUR.l1          -9.82705   19.92792  -0.493 0.622606    \nTotal_Crimes.l2   0.35402    0.08741   4.050 8.01e-05 ***\ntemp.l2          -6.82080    5.00667  -1.362 0.175029    \nNYUR.l2          28.12641   28.63596   0.982 0.327500    \nTotal_Crimes.l3  -0.12689    0.09391  -1.351 0.178547    \ntemp.l3           7.83481    5.08958   1.539 0.125712    \nNYUR.l3         -19.45212   28.73162  -0.677 0.499378    \nTotal_Crimes.l4   0.02575    0.08685   0.297 0.767230    \ntemp.l4          -0.54410    4.91205  -0.111 0.911941    \nNYUR.l4          22.90749   28.47866   0.804 0.422390    \nTotal_Crimes.l5   0.32350    0.08304   3.896 0.000144 ***\ntemp.l5         -16.94442    4.45045  -3.807 0.000201 ***\nNYUR.l5         -13.17257   19.04017  -0.692 0.490059    \nconst           852.14899  275.51688   3.093 0.002344 ** \ntrend             0.24774    0.33042   0.750 0.454517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 209.9 on 158 degrees of freedom\nMultiple R-Squared: 0.7497, Adjusted R-squared: 0.7243 \nF-statistic: 29.57 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  7.831e-04  1.450e-03   0.540  0.58982    \ntemp.l1          5.464e-01  7.607e-02   7.183 2.53e-11 ***\nNYUR.l1         -2.664e-01  3.433e-01  -0.776  0.43896    \nTotal_Crimes.l2 -1.666e-03  1.506e-03  -1.106  0.27030    \ntemp.l2          1.113e-01  8.625e-02   1.290  0.19894    \nNYUR.l2          7.327e-01  4.933e-01   1.485  0.13946    \nTotal_Crimes.l3 -2.923e-05  1.618e-03  -0.018  0.98561    \ntemp.l3          3.510e-02  8.768e-02   0.400  0.68948    \nNYUR.l3         -1.311e-01  4.950e-01  -0.265  0.79150    \nTotal_Crimes.l4  3.193e-03  1.496e-03   2.134  0.03436 *  \ntemp.l4         -2.639e-01  8.462e-02  -3.119  0.00216 ** \nNYUR.l4          3.109e-03  4.906e-01   0.006  0.99495    \nTotal_Crimes.l5 -2.245e-03  1.431e-03  -1.569  0.11858    \ntemp.l5         -3.657e-01  7.667e-02  -4.770 4.16e-06 ***\nNYUR.l5         -3.122e-01  3.280e-01  -0.952  0.34265    \nconst            5.154e+01  4.746e+00  10.860  &lt; 2e-16 ***\ntrend            8.022e-03  5.692e-03   1.409  0.16072    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.615 on 158 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.9476 \nF-statistic: 197.8 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1 -5.548e-04  3.619e-04  -1.533   0.1272    \ntemp.l1          6.378e-03  1.899e-02   0.336   0.7374    \nNYUR.l1          1.052e+00  8.570e-02  12.271   &lt;2e-16 ***\nTotal_Crimes.l2 -4.145e-06  3.759e-04  -0.011   0.9912    \ntemp.l2         -6.465e-03  2.153e-02  -0.300   0.7644    \nNYUR.l2         -3.063e-01  1.231e-01  -2.488   0.0139 *  \nTotal_Crimes.l3  7.720e-04  4.038e-04   1.912   0.0577 .  \ntemp.l3          9.826e-04  2.189e-02   0.045   0.9642    \nNYUR.l3          2.590e-01  1.236e-01   2.096   0.0377 *  \nTotal_Crimes.l4 -1.037e-04  3.735e-04  -0.278   0.7817    \ntemp.l4         -7.706e-03  2.112e-02  -0.365   0.7157    \nNYUR.l4         -1.658e-01  1.225e-01  -1.354   0.1778    \nTotal_Crimes.l5 -1.182e-04  3.571e-04  -0.331   0.7410    \ntemp.l5         -1.903e-03  1.914e-02  -0.099   0.9209    \nNYUR.l5          7.056e-02  8.188e-02   0.862   0.3901    \nconst            1.228e+00  1.185e+00   1.037   0.3015    \ntrend           -1.489e-03  1.421e-03  -1.048   0.2964    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9025 on 158 degrees of freedom\nMultiple R-Squared: 0.8518, Adjusted R-squared: 0.8368 \nF-statistic: 56.75 on 16 and 158 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes     temp     NYUR\nTotal_Crimes     44044.64 260.1181 -75.6436\ntemp               260.12  13.0714  -0.5906\nNYUR               -75.64  -0.5906   0.8145\n\nCorrelation matrix of residuals:\n             Total_Crimes    temp    NYUR\nTotal_Crimes       1.0000  0.3428 -0.3994\ntemp               0.3428  1.0000 -0.1810\nNYUR              -0.3994 -0.1810  1.0000\n\n\n\nThe first lags of violent crimes are significant predictors, and the third and fifth lags also contribute meaningfully.\nTemperature and unemployment rate in some lags show significance, with the fifth lag of temperature being a strong predictor.\nR squared of the model is 0.8518indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR5 &lt;- VAR(combined_data[,c(2:4)], p=5, type=\"both\")\nserial.test(VAR5, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR5\nChi-squared = 109.43, df = 63, p-value = 0.0002616\n\n\nCode\nacf(residuals(VAR5))\n\n\n\n\n\n\nThe residuals are normally distributed.\n\n\n\nVAR(12)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=12, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, temp, NYUR \nDeterministic variables: both \nSample size: 168 \nLog Likelihood: -1680.956 \nRoots of the characteristic polynomial:\n0.9987 0.9987 0.9797 0.9797 0.9744 0.9744 0.9687 0.9504 0.9504 0.9411 0.9411 0.9037 0.9037 0.8919 0.8919 0.8848 0.8848 0.8497 0.8497 0.8443 0.8443 0.8337 0.8337 0.8192 0.8192 0.7893 0.7893 0.771 0.771 0.7665 0.7665 0.7632 0.7632 0.5073 0.5073 0.5013\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 12, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1    0.31996    0.08458   3.783 0.000236 ***\ntemp.l1            3.68792    4.63894   0.795 0.428068    \nNYUR.l1          -24.56941   17.15873  -1.432 0.154575    \nTotal_Crimes.l2    0.22375    0.08338   2.683 0.008234 ** \ntemp.l2           -6.72250    4.75246  -1.415 0.159596    \nNYUR.l2           37.50651   24.47937   1.532 0.127911    \nTotal_Crimes.l3   -0.05015    0.08739  -0.574 0.567011    \ntemp.l3            3.68732    4.77341   0.772 0.441237    \nNYUR.l3          -23.53034   24.85360  -0.947 0.345519    \nTotal_Crimes.l4    0.17063    0.08846   1.929 0.055930 .  \ntemp.l4            2.84428    4.78909   0.594 0.553605    \nNYUR.l4           43.58852   25.21719   1.729 0.086269 .  \nTotal_Crimes.l5    0.25512    0.08794   2.901 0.004370 ** \ntemp.l5           -9.03397    4.83404  -1.869 0.063898 .  \nNYUR.l5          -29.30841   25.30530  -1.158 0.248909    \nTotal_Crimes.l6    0.13990    0.09338   1.498 0.136539    \ntemp.l6           -8.01680    4.94133  -1.622 0.107141    \nNYUR.l6           29.08540   25.36725   1.147 0.253664    \nTotal_Crimes.l7   -0.10529    0.09529  -1.105 0.271223    \ntemp.l7            1.90565    4.94172   0.386 0.700404    \nNYUR.l7          -44.73522   25.46095  -1.757 0.081270 .  \nTotal_Crimes.l8   -0.16992    0.09077  -1.872 0.063450 .  \ntemp.l8            3.29643    4.88056   0.675 0.500608    \nNYUR.l8           18.74965   25.42074   0.738 0.462103    \nTotal_Crimes.l9   -0.08893    0.09222  -0.964 0.336676    \ntemp.l9           -7.00308    4.90625  -1.427 0.155868    \nNYUR.l9          -42.26845   25.24026  -1.675 0.096408 .  \nTotal_Crimes.l10  -0.08845    0.09290  -0.952 0.342791    \ntemp.l10          10.66517    4.92204   2.167 0.032072 *  \nNYUR.l10          -2.74946   24.74241  -0.111 0.911690    \nTotal_Crimes.l11  -0.34923    0.08698  -4.015 9.98e-05 ***\ntemp.l11          13.52799    4.68640   2.887 0.004561 ** \nNYUR.l11          32.44703   24.43351   1.328 0.186514    \nTotal_Crimes.l12   0.54092    0.08550   6.326 3.73e-09 ***\ntemp.l12          -9.85297    4.77695  -2.063 0.041141 *  \nNYUR.l12          29.48804   16.93136   1.742 0.083940 .  \nconst            532.50378  747.26313   0.713 0.477367    \ntrend              0.22749    0.35115   0.648 0.518240    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 172.3 on 130 degrees of freedom\nMultiple R-Squared: 0.8547, Adjusted R-squared: 0.8133 \nF-statistic: 20.67 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)   \nTotal_Crimes.l1   8.534e-04  1.626e-03   0.525  0.60049   \ntemp.l1           2.762e-01  8.915e-02   3.098  0.00239 **\nNYUR.l1          -3.332e-01  3.298e-01  -1.010  0.31424   \nTotal_Crimes.l2  -1.004e-03  1.602e-03  -0.627  0.53196   \ntemp.l2           4.590e-02  9.133e-02   0.503  0.61610   \nNYUR.l2           4.403e-01  4.705e-01   0.936  0.35106   \nTotal_Crimes.l3  -6.033e-04  1.679e-03  -0.359  0.72002   \ntemp.l3           8.691e-02  9.174e-02   0.947  0.34523   \nNYUR.l3           6.519e-02  4.776e-01   0.136  0.89165   \nTotal_Crimes.l4   2.749e-03  1.700e-03   1.617  0.10832   \ntemp.l4          -1.191e-01  9.204e-02  -1.294  0.19787   \nNYUR.l4           7.078e-02  4.846e-01   0.146  0.88412   \nTotal_Crimes.l5  -7.791e-04  1.690e-03  -0.461  0.64561   \ntemp.l5          -1.102e-01  9.290e-02  -1.186  0.23786   \nNYUR.l5          -3.542e-01  4.863e-01  -0.728  0.46772   \nTotal_Crimes.l6   4.690e-04  1.795e-03   0.261  0.79426   \ntemp.l6          -1.774e-01  9.496e-02  -1.868  0.06407 . \nNYUR.l6           2.062e-01  4.875e-01   0.423  0.67306   \nTotal_Crimes.l7   9.091e-05  1.831e-03   0.050  0.96049   \ntemp.l7          -9.346e-02  9.497e-02  -0.984  0.32690   \nNYUR.l7           2.852e-01  4.893e-01   0.583  0.56094   \nTotal_Crimes.l8  -1.063e-03  1.744e-03  -0.609  0.54340   \ntemp.l8           5.041e-02  9.380e-02   0.537  0.59189   \nNYUR.l8          -5.998e-01  4.885e-01  -1.228  0.22180   \nTotal_Crimes.l9   7.111e-04  1.772e-03   0.401  0.68894   \ntemp.l9          -9.072e-02  9.429e-02  -0.962  0.33779   \nNYUR.l9           3.205e-01  4.851e-01   0.661  0.50993   \nTotal_Crimes.l10 -2.790e-04  1.785e-03  -0.156  0.87607   \ntemp.l10          7.616e-02  9.459e-02   0.805  0.42219   \nNYUR.l10         -5.045e-01  4.755e-01  -1.061  0.29064   \nTotal_Crimes.l11 -1.287e-03  1.672e-03  -0.770  0.44272   \ntemp.l11          2.846e-01  9.007e-02   3.160  0.00196 **\nNYUR.l11          7.950e-01  4.696e-01   1.693  0.09284 . \nTotal_Crimes.l12 -1.967e-03  1.643e-03  -1.197  0.23340   \ntemp.l12          1.325e-01  9.181e-02   1.443  0.15145   \nNYUR.l12         -4.093e-01  3.254e-01  -1.258  0.21071   \nconst             4.280e+01  1.436e+01   2.980  0.00344 **\ntrend            -1.208e-03  6.749e-03  -0.179  0.85819   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.312 on 130 degrees of freedom\nMultiple R-Squared: 0.9659, Adjusted R-squared: 0.9562 \nF-statistic: 99.43 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = Total_Crimes.l1 + temp.l1 + NYUR.l1 + Total_Crimes.l2 + temp.l2 + NYUR.l2 + Total_Crimes.l3 + temp.l3 + NYUR.l3 + Total_Crimes.l4 + temp.l4 + NYUR.l4 + Total_Crimes.l5 + temp.l5 + NYUR.l5 + Total_Crimes.l6 + temp.l6 + NYUR.l6 + Total_Crimes.l7 + temp.l7 + NYUR.l7 + Total_Crimes.l8 + temp.l8 + NYUR.l8 + Total_Crimes.l9 + temp.l9 + NYUR.l9 + Total_Crimes.l10 + temp.l10 + NYUR.l10 + Total_Crimes.l11 + temp.l11 + NYUR.l11 + Total_Crimes.l12 + temp.l12 + NYUR.l12 + const + trend \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  -7.066e-04  4.636e-04  -1.524   0.1299    \ntemp.l1           3.188e-02  2.543e-02   1.254   0.2122    \nNYUR.l1           1.060e+00  9.405e-02  11.271   &lt;2e-16 ***\nTotal_Crimes.l2   3.656e-04  4.570e-04   0.800   0.4252    \ntemp.l2           4.501e-03  2.605e-02   0.173   0.8631    \nNYUR.l2          -2.705e-01  1.342e-01  -2.016   0.0459 *  \nTotal_Crimes.l3   9.789e-04  4.790e-04   2.044   0.0430 *  \ntemp.l3           1.284e-02  2.616e-02   0.491   0.6244    \nNYUR.l3           2.765e-01  1.362e-01   2.029   0.0445 *  \nTotal_Crimes.l4  -4.270e-04  4.849e-04  -0.881   0.3801    \ntemp.l4          -1.246e-02  2.625e-02  -0.475   0.6359    \nNYUR.l4          -2.340e-01  1.382e-01  -1.693   0.0928 .  \nTotal_Crimes.l5  -2.622e-04  4.820e-04  -0.544   0.5875    \ntemp.l5          -2.994e-02  2.650e-02  -1.130   0.2605    \nNYUR.l5           9.230e-02  1.387e-01   0.665   0.5070    \nTotal_Crimes.l6  -2.236e-05  5.118e-04  -0.044   0.9652    \ntemp.l6           7.516e-03  2.708e-02   0.277   0.7818    \nNYUR.l6          -1.329e-01  1.390e-01  -0.956   0.3408    \nTotal_Crimes.l7   1.082e-04  5.223e-04   0.207   0.8363    \ntemp.l7           1.662e-02  2.709e-02   0.614   0.5405    \nNYUR.l7           1.870e-01  1.396e-01   1.340   0.1825    \nTotal_Crimes.l8  -2.389e-04  4.975e-04  -0.480   0.6319    \ntemp.l8           1.994e-02  2.675e-02   0.745   0.4573    \nNYUR.l8          -9.392e-02  1.393e-01  -0.674   0.5015    \nTotal_Crimes.l9   4.698e-06  5.055e-04   0.009   0.9926    \ntemp.l9           3.613e-02  2.689e-02   1.343   0.1815    \nNYUR.l9           1.066e-01  1.383e-01   0.770   0.4424    \nTotal_Crimes.l10  7.141e-05  5.092e-04   0.140   0.8887    \ntemp.l10         -2.898e-02  2.698e-02  -1.074   0.2847    \nNYUR.l10         -8.635e-02  1.356e-01  -0.637   0.5254    \nTotal_Crimes.l11  7.388e-04  4.767e-04   1.550   0.1237    \ntemp.l11         -4.162e-02  2.569e-02  -1.620   0.1076    \nNYUR.l11          2.030e-02  1.339e-01   0.152   0.8798    \nTotal_Crimes.l12 -5.791e-04  4.686e-04  -1.236   0.2188    \ntemp.l12          1.823e-02  2.618e-02   0.696   0.4876    \nNYUR.l12         -2.866e-02  9.280e-02  -0.309   0.7580    \nconst            -1.196e+00  4.096e+00  -0.292   0.7707    \ntrend            -2.030e-03  1.925e-03  -1.055   0.2936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.9446 on 130 degrees of freedom\nMultiple R-Squared: 0.8631, Adjusted R-squared: 0.8241 \nF-statistic: 22.15 on 37 and 130 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes     temp     NYUR\nTotal_Crimes     29701.03 221.9391 -60.5674\ntemp               221.94  10.9701  -0.4129\nNYUR               -60.57  -0.4129   0.8923\n\nCorrelation matrix of residuals:\n             Total_Crimes    temp   NYUR\nTotal_Crimes       1.0000  0.3888 -0.372\ntemp               0.3888  1.0000 -0.132\nNYUR              -0.3720 -0.1320  1.000\n\n\n\nmost of the coeffcients values are not significant predictors.\nTemperature and unemployment rate in some lags show significance, with the second lag of unemploymnet rate being a strong predictor.\nR squared of the model is 0.8631 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR12 &lt;- VAR(combined_data[,2:4], p=12, type=\"both\")\nserial.test(VAR12, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR12\nChi-squared = 52.247, df = 0, p-value &lt; 2.2e-16\n\n\nCode\nacf(residuals(VAR12))\n\n\n\n\n\n\nResiduals are normally distributed."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#forecasts-1",
    "href": "ARIMAX_SARIMAX_VAR.html#forecasts-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasts",
    "text": "Forecasts\n\nVAR(5)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:4)], start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nfit &lt;- VAR(ts_obj, p=5, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       3103.657 2834.700 3372.614 2692.323 3514.991\nFeb 2022       3060.138 2772.459 3347.818 2620.170 3500.106\nMar 2022       3181.174 2870.983 3491.365 2706.778 3655.570\nApr 2022       3257.261 2942.064 3572.459 2775.209 3739.314\nMay 2022       3470.026 3150.259 3789.793 2980.984 3959.067\nJun 2022       3662.001 3329.827 3994.175 3153.984 4170.017\nJul 2022       3761.737 3423.536 4099.938 3244.503 4278.971\nAug 2022       3809.891 3457.215 4162.567 3270.520 4349.262\nSep 2022       3734.276 3375.875 4092.677 3186.149 4282.403\nOct 2022       3578.426 3215.055 3941.798 3022.698 4134.155\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       34.00831 29.37494 38.64168 26.92218 41.09444\nFeb 2022       37.57718 32.21833 42.93603 29.38153 45.77284\nMar 2022       44.65194 38.98766 50.31623 35.98917 53.31472\nApr 2022       53.78903 47.93629 59.64176 44.83804 62.74001\nMay 2022       65.73079 59.84405 71.61753 56.72780 74.73378\nJun 2022       74.35874 68.01711 80.70037 64.66005 84.05743\nJul 2022       77.83947 70.93009 84.74885 67.27249 88.40645\nAug 2022       75.90105 68.50215 83.29994 64.58541 87.21668\nSep 2022       69.37341 61.61778 77.12905 57.51219 81.23463\nOct 2022       59.00884 51.14679 66.87089 46.98488 71.03280\n\nNYUR\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\nJan 2022       4.191674 3.035061 5.348287 2.4227867 5.960561\nFeb 2022       4.378075 2.654950 6.101200 1.7427829 7.013368\nMar 2022       4.472398 2.466663 6.478133 1.4048909 7.539905\nApr 2022       4.557274 2.353268 6.761279 1.1865381 7.928010\nMay 2022       4.699641 2.363008 7.036275 1.1260687 8.273214\nJun 2022       4.819594 2.388499 7.250690 1.1015543 8.537635\nJul 2022       4.789580 2.277456 7.301705 0.9476174 8.631543\nAug 2022       4.729443 2.151139 7.307746 0.7862677 8.672618\nSep 2022       4.641580 2.011852 7.271307 0.6197589 8.663401\nOct 2022       4.507292 1.835800 7.178785 0.4215968 8.592988\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(12)\n\n\nCode\nfit &lt;- VAR(ts_obj, p=12, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       3085.732 2864.870 3306.594 2747.952 3423.512\nFeb 2022       2741.762 2502.073 2981.451 2375.189 3108.335\nMar 2022       2867.565 2617.885 3117.244 2485.713 3249.417\nApr 2022       2830.417 2578.630 3082.204 2445.342 3215.492\nMay 2022       3146.570 2888.337 3404.803 2751.636 3541.504\nJun 2022       3399.356 3129.491 3669.222 2986.633 3812.080\nJul 2022       3770.577 3487.947 4053.207 3338.331 4202.823\nAug 2022       3605.486 3317.579 3893.393 3165.170 4045.802\nSep 2022       3482.601 3194.016 3771.187 3041.248 3923.955\nOct 2022       3431.597 3139.105 3724.088 2984.269 3878.924\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2022       33.09064 28.84601 37.33528 26.59903 39.58226\nFeb 2022       35.25381 30.78808 39.71955 28.42406 42.08357\nMar 2022       40.50027 36.00971 44.99082 33.63255 47.36798\nApr 2022       50.30555 45.77971 54.83138 43.38388 57.22722\nMay 2022       60.08442 55.54119 64.62765 53.13615 67.03269\nJun 2022       68.82703 64.25990 73.39417 61.84220 75.81187\nJul 2022       75.47951 70.80275 80.15628 68.32701 82.63201\nAug 2022       74.11362 69.29282 78.93442 66.74084 81.48640\nSep 2022       67.95866 63.11526 72.80207 60.55132 75.36601\nOct 2022       59.27962 54.39054 64.16870 51.80242 66.75682\n\nNYUR\n         Point Forecast    Lo 80    Hi 80      Lo 95    Hi 95\nJan 2022       3.815681 2.605107 5.026255  1.9642677 5.667094\nFeb 2022       4.059000 2.258537 5.859462  1.3054298 6.812570\nMar 2022       4.367171 2.267994 6.466347  1.1567577 7.577584\nApr 2022       4.719622 2.372232 7.067013  1.1295982 8.309646\nMay 2022       5.006499 2.493631 7.519367  1.1633986 8.849599\nJun 2022       4.935612 2.307341 7.563883  0.9160188 8.955206\nJul 2022       4.258368 1.557959 6.958777  0.1284485 8.388288\nAug 2022       4.029914 1.263429 6.796398 -0.2010597 8.260887\nSep 2022       4.102393 1.278089 6.926696 -0.2170065 8.421792\nOct 2022       4.132406 1.246105 7.018707 -0.2818106 8.546623\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nEvaluations\nthe historical data shows fluctuations with no clear long-term trend. The forecast suggests a stable outlook with a slight uptick towards the end, within a moderate confidence interval."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#overview-2",
    "href": "ARIMAX_SARIMAX_VAR.html#overview-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Overview",
    "text": "Overview\n\nIn this model, we employs a Vector Autoregression VAR model to scrutinize the weekly total crime rates in New York City, contextualized by the significant societal disruptions of the COVID-19 pandemic. The model is designed to understand how the pandemic may have altered crime trends, leveraging the robust COVID-19 datasets available from the covidcast package in R. These datasets capture a range of COVID-19 metrics, serving as vital exogenous inputs to our model, thus facilitating a comprehensive examination of the pandemic’s imprint on urban crime patterns.\n\n\n\nCode\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\n#v_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\n#w_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \n\ncases_nyc_weekly$total_cases &lt;- cases_nyc_weekly$total_cases +1\n\n# Merge data sets\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\ndata_w &lt;- data_w %&gt;%\n  filter(week &gt;= \"2020-01-19\" & week &lt;= \"2022-12-25\")\n\n\n# Merging the two datasets on the 'date' column\ncombined_data &lt;- merge(data_w, cases_nyc_weekly, by = \"week\")\ncombined_data &lt;- combined_data[, -2] \nhead(combined_data)\n\n\n        week Total_Crimes total_cases\n1 2020-01-19          699           1\n2 2020-01-26          726           1\n3 2020-02-02          727           1\n4 2020-02-09          681           1\n5 2020-02-16          658           1\n6 2020-02-23          676           1"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#data-visualization-1",
    "href": "ARIMAX_SARIMAX_VAR.html#data-visualization-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\nCode\nggplot(data_w, aes(x = week, y = Total_Crimes)) +\n  geom_line() +\n  labs(title = \"Weekly Violent Crimes\", x = \"Date\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line() +\n  labs(title = \"Weekly COVID-19 total cases\", x = \"Date\", y = \"Cases\") +\n  theme_minimal()"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-varselect-1",
    "href": "ARIMAX_SARIMAX_VAR.html#fitting-model-with-varselect-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting Model with VARselect()",
    "text": "Fitting Model with VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:3)], lag.max=14, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      4      2      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 2.444372e+01 2.389856e+01 2.389264e+01 2.381126e+01 2.382067e+01\nHQ(n)  2.451203e+01 2.400102e+01 2.402926e+01 2.398203e+01 2.402560e+01\nSC(n)  2.461181e+01 2.415070e+01 2.422883e+01 2.423149e+01 2.432495e+01\nFPE(n) 4.128435e+10 2.393633e+10 2.379860e+10 2.194366e+10 2.215909e+10\n                  6            7            8            9           10\nAIC(n) 2.380090e+01 2.381596e+01 2.384493e+01 2.386624e+01 2.389024e+01\nHQ(n)  2.403998e+01 2.408920e+01 2.415232e+01 2.420778e+01 2.426593e+01\nSC(n)  2.438923e+01 2.448834e+01 2.460135e+01 2.470670e+01 2.481475e+01\nFPE(n) 2.173618e+10 2.208071e+10 2.274916e+10 2.326396e+10 2.386044e+10\n                 11           12           13           14\nAIC(n) 2.389903e+01 2.394177e+01 2.394091e+01 2.397983e+01\nHQ(n)  2.430888e+01 2.438577e+01 2.441906e+01 2.449214e+01\nSC(n)  2.490759e+01 2.503438e+01 2.511756e+01 2.524053e+01\nFPE(n) 2.410944e+10 2.520960e+10 2.524353e+10 2.631287e+10\n\n\n\nBased on the results, we have VAR(2), VAR(4) and VAR(6)"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-2",
    "href": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nVAR(2)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=2, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 152 \nLog Likelihood: -2231.182 \nRoots of the characteristic polynomial:\n0.9018 0.7848 0.7848 0.3175\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 2, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.582474   0.079733   7.305 1.66e-11 ***\ntotal_cases.l1  -0.002829   0.001470  -1.924 0.056278 .  \nTotal_Crimes.l2  0.285326   0.080174   3.559 0.000503 ***\ntotal_cases.l2   0.002293   0.001481   1.548 0.123780    \nconst           85.147059  33.771947   2.521 0.012768 *  \ntrend            0.221369   0.150777   1.468 0.144204    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.98 on 146 degrees of freedom\nMultiple R-Squared: 0.8065, Adjusted R-squared: 0.7999 \nF-statistic: 121.7 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   -1.34818    3.55002  -0.380    0.705    \ntotal_cases.l1     1.42297    0.06545  21.741   &lt;2e-16 ***\nTotal_Crimes.l2   -0.29171    3.56964  -0.082    0.935    \ntotal_cases.l2    -0.62026    0.06594  -9.407   &lt;2e-16 ***\nconst           1237.92672 1503.65927   0.823    0.412    \ntrend              9.65213    6.71316   1.438    0.153    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2537 on 146 degrees of freedom\nMultiple R-Squared: 0.8693, Adjusted R-squared: 0.8648 \nF-statistic: 194.2 on 5 and 146 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3247       -3671\ntotal_cases         -3671     6436993\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.02539\ntotal_cases      -0.02539     1.00000\n\n\n\nResiduals\n\n\n\nCode\nVAR2 &lt;- VAR(combined_data[,c(2:3)], p=2, type=\"both\")\nserial.test(VAR2, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR2\nChi-squared = 189.81, df = 200, p-value = 0.6861\n\n\nCode\nacf(residuals(VAR2))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\nVAR(4)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=4, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 150 \nLog Likelihood: -2189.821 \nRoots of the characteristic polynomial:\n0.8861 0.8015 0.8015 0.774 0.6592 0.6592 0.598 0.4795\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 4, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.5739946  0.0835420   6.871 1.93e-10 ***\ntotal_cases.l1  -0.0032288  0.0018585  -1.737  0.08453 .  \nTotal_Crimes.l2  0.2313245  0.0939156   2.463  0.01499 *  \ntotal_cases.l2   0.0039177  0.0031402   1.248  0.21426    \nTotal_Crimes.l3  0.2448559  0.0937532   2.612  0.00999 ** \ntotal_cases.l3  -0.0016973  0.0031546  -0.538  0.59140    \nTotal_Crimes.l4 -0.1829429  0.0838566  -2.182  0.03081 *  \ntotal_cases.l4   0.0004815  0.0018796   0.256  0.79819    \nconst           85.6789208 35.6244882   2.405  0.01748 *  \ntrend            0.2201465  0.1625484   1.354  0.17781    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.39 on 140 degrees of freedom\nMultiple R-Squared: 0.8178, Adjusted R-squared: 0.8061 \nF-statistic: 69.81 on 9 and 140 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1   -0.66991    3.61658  -0.185 0.853314    \ntotal_cases.l1     1.40237    0.08046  17.430  &lt; 2e-16 ***\nTotal_Crimes.l2   -3.06501    4.06566  -0.754 0.452189    \ntotal_cases.l2    -0.35208    0.13594  -2.590 0.010615 *  \nTotal_Crimes.l3    6.06912    4.05863   1.495 0.137071    \ntotal_cases.l3    -0.50183    0.13657  -3.675 0.000338 ***\nTotal_Crimes.l4   -2.92659    3.63020  -0.806 0.421507    \ntotal_cases.l4     0.30785    0.08137   3.783 0.000229 ***\nconst            571.35179 1542.20510   0.370 0.711587    \ntrend              5.47566    7.03682   0.778 0.437797    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2441 on 140 degrees of freedom\nMultiple R-Squared: 0.8835, Adjusted R-squared: 0.876 \nF-statistic:   118 on 9 and 140 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3179       -9639\ntotal_cases         -9639     5958430\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.07003\ntotal_cases      -0.07003     1.00000\n\n\n\nBoth models looks good based on the model diagnostics, most coefficients are significant, combined with a high R square of above 0.7.\nResiduals\n\n\n\nCode\nVAR4 &lt;- VAR(combined_data[,c(2:3)], p=4, type=\"both\")\nserial.test(VAR4, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR4\nChi-squared = 157.09, df = 192, p-value = 0.9691\n\n\nCode\nacf(residuals(VAR4))\n\n\n\n\n\n\nResiduals are not normally distributed.\n\n\n\nVAR(6)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:3)], p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Total_Crimes, total_cases \nDeterministic variables: both \nSample size: 148 \nLog Likelihood: -2153.447 \nRoots of the characteristic polynomial:\n0.8565 0.8565 0.8518 0.8219 0.8219 0.7468 0.7468 0.6774 0.6199 0.495 0.495 0.1675\nCall:\nvars::VAR(y = combined_data[, c(2:3)], p = 6, type = \"both\")\n\n\nEstimation results for equation Total_Crimes: \n============================================= \nTotal_Crimes = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + Total_Crimes.l5 + total_cases.l5 + Total_Crimes.l6 + total_cases.l6 + const + trend \n\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1  0.582113   0.085967   6.771 3.65e-10 ***\ntotal_cases.l1  -0.004696   0.001963  -2.393   0.0181 *  \nTotal_Crimes.l2  0.240793   0.097261   2.476   0.0145 *  \ntotal_cases.l2   0.007350   0.003427   2.144   0.0338 *  \nTotal_Crimes.l3  0.226815   0.098705   2.298   0.0231 *  \ntotal_cases.l3  -0.002131   0.003419  -0.623   0.5341    \nTotal_Crimes.l4 -0.132873   0.098316  -1.351   0.1788    \ntotal_cases.l4  -0.006296   0.003372  -1.867   0.0640 .  \nTotal_Crimes.l5 -0.014488   0.097858  -0.148   0.8825    \ntotal_cases.l5   0.008775   0.003418   2.567   0.0114 *  \nTotal_Crimes.l6 -0.038168   0.085310  -0.447   0.6553    \ntotal_cases.l6  -0.003475   0.001999  -1.739   0.0844 .  \nconst           88.081052  37.622360   2.341   0.0207 *  \ntrend            0.218624   0.173852   1.258   0.2107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 56.02 on 134 degrees of freedom\nMultiple R-Squared: 0.8265, Adjusted R-squared: 0.8097 \nF-statistic: 49.11 on 13 and 134 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation total_cases: \n============================================ \ntotal_cases = Total_Crimes.l1 + total_cases.l1 + Total_Crimes.l2 + total_cases.l2 + Total_Crimes.l3 + total_cases.l3 + Total_Crimes.l4 + total_cases.l4 + Total_Crimes.l5 + total_cases.l5 + Total_Crimes.l6 + total_cases.l6 + const + trend \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nTotal_Crimes.l1    1.65482    3.70941   0.446   0.6562    \ntotal_cases.l1     1.42104    0.08468  16.781  &lt; 2e-16 ***\nTotal_Crimes.l2   -4.86294    4.19676  -1.159   0.2486    \ntotal_cases.l2    -0.31798    0.14789  -2.150   0.0333 *  \nTotal_Crimes.l3    4.78356    4.25904   1.123   0.2634    \ntotal_cases.l3    -0.64459    0.14751  -4.370 2.47e-05 ***\nTotal_Crimes.l4   -4.50211    4.24227  -1.061   0.2905    \ntotal_cases.l4     0.34349    0.14548   2.361   0.0197 *  \nTotal_Crimes.l5    3.21222    4.22251   0.761   0.4482    \ntotal_cases.l5     0.22827    0.14750   1.548   0.1241    \nTotal_Crimes.l6   -1.28240    3.68107  -0.348   0.7281    \ntotal_cases.l6    -0.22308    0.08624  -2.587   0.0108 *  \nconst            835.28512 1623.37950   0.515   0.6077    \ntrend              8.31945    7.50158   1.109   0.2694    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 2417 on 134 degrees of freedom\nMultiple R-Squared: 0.8902, Adjusted R-squared: 0.8796 \nF-statistic: 83.58 on 13 and 134 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes         3138      -12410\ntotal_cases        -12410     5842667\n\nCorrelation matrix of residuals:\n             Total_Crimes total_cases\nTotal_Crimes      1.00000    -0.09165\ntotal_cases      -0.09165     1.00000\n\n\n\nBoth models looks good based on the model diagnostics, most coefficients are significant, combined with a high R square of above 0.8.\nResiduals\n\n\n\nCode\nVAR6 &lt;- VAR(combined_data[,c(2:3)], p=6, type=\"both\")\nserial.test(VAR6, lags.pt=52, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR6\nChi-squared = 140.7, df = 184, p-value = 0.9924\n\n\nCode\nacf(residuals(VAR6))\n\n\n\n\n\n\nResiduals are not normally distributed."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#forecasts-2",
    "href": "ARIMAX_SARIMAX_VAR.html#forecasts-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasts",
    "text": "Forecasts\n\nVAR(2)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=2, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       720.5998 647.5726 793.6269 608.9144  832.2851\n2023.030       729.9145 644.7867 815.0424 599.7227  860.1064\n2023.049       746.0172 648.5536 843.4808 596.9595  895.0749\n2023.068       756.9854 651.1908 862.7799 595.1865  918.7842\n2023.088       767.7670 655.4002 880.1337 595.9169  939.6171\n2023.107       777.5903 660.2728 894.9079 598.1687  957.0120\n2023.126       787.1464 665.9966 908.2961 601.8638  972.4290\n2023.145       796.3813 672.2474 920.5153 606.5349  986.2278\n2023.165       805.2947 678.8048 931.7846 611.8451  998.7443\n2023.184       813.7790 685.4162 942.1419 617.4650 1010.0931\n\ntotal_cases\n         Point Forecast       Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3764.515   513.06139  7015.969 -1208.154  8737.185\n2023.030       4693.832  -964.01475 10351.678 -3959.098 13346.761\n2023.049       5903.221 -1382.37518 13188.817 -5239.137 17045.579\n2023.068       7032.947 -1132.21160 15198.106 -5454.586 19520.480\n2023.088       7880.538  -639.56096 16400.637 -5149.829 20910.905\n2023.107       8377.822  -230.70325 16986.347 -4787.781 21543.425\n2023.126       8552.974   -66.80241 17172.750 -4629.836 21735.784\n2023.145       8487.666  -152.42307 17127.756 -4726.210 21701.543\n2023.165       8280.510  -398.79206 16959.812 -4993.337 21554.356\n2023.184       8021.182  -698.09383 16740.457 -5313.800 21356.163\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(4)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=4, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       718.1521 645.8903 790.4138 607.6373  828.6669\n2023.030       729.6403 645.3617 813.9188 600.7474  858.5332\n2023.049       736.3005 642.1673 830.4337 592.3361  880.2649\n2023.068       760.6577 652.4120 868.9034 595.1103  926.2051\n2023.088       770.3121 655.8584 884.7658 595.2703  945.3540\n2023.107       781.1818 661.1142 901.2494 597.5543  964.8094\n2023.126       794.8875 670.1044 919.6706 604.0482  985.7268\n2023.145       803.6789 676.0370 931.3207 608.4675  998.8903\n2023.165       813.5903 683.4608 943.7198 614.5744 1012.6062\n2023.184       823.0851 691.0192 955.1511 621.1077 1025.0625\n\ntotal_cases\n         Point Forecast     Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3237.656   109.402  6365.910 -1546.596  8021.907\n2023.030       3352.171 -2038.886  8743.229 -4892.740 11597.082\n2023.049       3954.929 -3459.054 11368.911 -7383.780 15293.637\n2023.068       4939.053 -3497.901 13376.006 -7964.155 17842.260\n2023.088       5850.568 -2977.765 14678.900 -7651.202 19352.337\n2023.107       6446.281 -2445.007 15337.569 -7151.770 20044.332\n2023.126       6749.394 -2142.575 15641.364 -6849.700 20348.488\n2023.145       6760.553 -2136.883 15657.989 -6846.901 20368.007\n2023.165       6646.433 -2253.220 15546.087 -6964.412 20257.279\n2023.184       6537.003 -2364.094 15438.101 -7076.051 20150.057\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nVAR(6)\n\n\nCode\nts_obj &lt;- ts(combined_data[, c(2:3)], star=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\nfit &lt;- VAR(ts_obj, p=6, type='both')\nforecast(fit)\n\n\nTotal_Crimes\n         Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n2023.011       722.4993 650.7087 794.2899 612.7050  832.2935\n2023.030       723.4490 638.4588 808.4392 593.4677  853.4303\n2023.049       731.0963 636.2415 825.9511 586.0285  876.1642\n2023.068       759.0584 650.6196 867.4972 593.2155  924.9013\n2023.088       762.0325 644.9207 879.1442 582.9255  941.1395\n2023.107       778.8229 655.3577 902.2881 589.9992  967.6466\n2023.126       790.3255 661.4272 919.2239 593.1925  987.4586\n2023.145       798.9869 666.9459 931.0279 597.0476 1000.9262\n2023.165       813.5840 678.5537 948.6143 607.0730 1020.0950\n2023.184       821.9354 685.0586 958.8121 612.6005 1031.2702\n\ntotal_cases\n         Point Forecast      Lo 80     Hi 80     Lo 95     Hi 95\n2023.011       3456.950   359.2337  6554.666 -1280.598  8194.498\n2023.030       3853.784 -1521.3087  9228.876 -4366.711 12074.279\n2023.049       5094.426 -2424.5477 12613.400 -6404.852 16593.704\n2023.068       6770.689 -1802.0401 15343.418 -6340.169 19881.547\n2023.088       7766.123 -1136.4672 16668.713 -5849.214 21381.459\n2023.107       8337.517  -616.9653 17292.000 -5357.182 22032.217\n2023.126       8367.280  -591.1327 17325.694 -5333.430 22067.991\n2023.145       8214.080  -749.5883 17177.748 -5494.668 21922.827\n2023.165       8180.055  -788.0688 17148.179 -5535.507 21895.617\n2023.184       8154.277  -823.6237 17132.177 -5576.237 21884.790\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#evaluations-2",
    "href": "ARIMAX_SARIMAX_VAR.html#evaluations-2",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Evaluations",
    "text": "Evaluations\n\nVAR(2) Forecast: The forecast for violent crimes indicates a stable trend with a slight increase toward the end, and the confidence interval is moderate.\nVAR(4) Forecast: The forecast for violent crimes remains stable with a slightly positive trend toward the end, similar to VAR(2), but with a narrower confidence interval, suggesting more certainty in the prediction.\nVAR(6) Forecast: The forecast for violent crimes remains stable with a slightly positive trend toward the end, similar to VAR(2), but with a narrower confidence interval, suggesting more certainty in the prediction."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#covid-19-data-extraction-and-cleaning-1",
    "href": "ARIMAX_SARIMAX_VAR.html#covid-19-data-extraction-and-cleaning-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "COVID-19 data extraction and cleaning",
    "text": "COVID-19 data extraction and cleaning\n\nExtract COVID data using covid_cast package in R\n\n\n\nCode\n# Fetching COVID-19 cases data for New York City for the specified timeframe\ncases_nyc &lt;- covidcast_signal(\"jhu-csse\", \"confirmed_incidence_num\",\n                              start_day = \"2020-01-01\", end_day = \"2022-12-31\",\n                              geo_type = \"county\", geo_values = \"36061\") # 36061 is the FIPS code for New York County\n\n# Cleaning and preparing the data\ncases_nyc_cleaned &lt;- cases_nyc %&gt;%\n  dplyr::select(time_value, value) %&gt;%\n  dplyr::arrange(time_value)\n\n# Ensure the date column is in the Date format\ncases_nyc_cleaned$time_value &lt;- as.Date(cases_nyc_cleaned$time_value)\n\n# Aggregating the daily COVID-19 cases data to weekly\ncases_nyc_weekly &lt;- cases_nyc_cleaned %&gt;%\n  mutate(week = floor_date(time_value, \"week\")) %&gt;%  \n  group_by(week) %&gt;%\n  summarize(total_cases = sum(value, na.rm = TRUE))  \nhead(cases_nyc_weekly)\n\n\n# A tibble: 6 × 2\n  week       total_cases\n  &lt;date&gt;           &lt;dbl&gt;\n1 2020-01-19           0\n2 2020-01-26           0\n3 2020-02-02           0\n4 2020-02-09           0\n5 2020-02-16           0\n6 2020-02-23           0\n\n\nCode\n#sum(is.na(cases_nyc_weekly)) = 0 indicates no missing value\n\n\n\n\nCode\n# Visualization of Covid data of NYC\ncovid_plot &lt;- ggplot(cases_nyc_weekly, aes(x = week, y = total_cases)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Weekly COVID-19 Cases in New York City\",\n       x = \"Date\",\n       y = \"Cases\") +\n  theme_minimal()\n\n# Convert to interactive plot with plotly\nggplotly(covid_plot, tooltip = \"text\")\n\n\n\n\n\n\n\nFilter weekly assault data and COVID-19 data to have matching time frame\n\n\n\nCode\n# Filter assault_weekly to the overlapping time period (2020-01-19 to 2021-12-26)\nassault_weekly_filtered &lt;- assault_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Filter cases_nyc_weekly to the same time period\ncases_nyc_weekly_filtered &lt;- cases_nyc_weekly %&gt;%\n  filter(week &gt;= as.Date(\"2020-01-19\") & week &lt;= as.Date(\"2021-12-26\"))\n\n# Merging the two datasets on the 'week' column\ncombined_data &lt;- merge(assault_weekly_filtered, cases_nyc_weekly_filtered, by = \"week\")\nhead(combined_data)\n\n\n        week total_crimes total_cases\n1 2020-01-19          392           0\n2 2020-01-26          366           0\n3 2020-02-02          394           0\n4 2020-02-09          343           0\n5 2020-02-16          358           0\n6 2020-02-23          366           0\n\n\nCode\n# Convert to time series\ncombined_ts &lt;- ts(combined_data, start = c(2020, 3), frequency = 52)\n\n\n\n\nCode\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nWe need a log transform on the COVID-19 data due to a dramatic increase towards the end of the time series.\n\n\n\nCode\n# Apply log plus 1 transformation to the COVID-19 cases data to accommodate for 0s\ncombined_ts[, 3] &lt;- log(combined_ts[, 3] + 1)\n\n\n\n\nCode\n# Plot the transformed data\nautoplot(combined_ts[,c(2:3)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Assault crimes and transformed Covid-19 cases of NYC from 2020 to 2022\")\n\n\n\n\n\n\nThe transformed COVID-19 series have more stability."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fit-with-auto.arima",
    "href": "ARIMAX_SARIMAX_VAR.html#fit-with-auto.arima",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fit with auto.arima",
    "text": "Fit with auto.arima\n\n\nCode\nxreg &lt;- combined_ts[, \"total_cases\"]\n\nfit &lt;- auto.arima(combined_ts[, \"total_crimes\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"total_crimes\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1      xreg\n      -0.5098  -12.6470\ns.e.   0.0825    4.3866\n\nsigma^2 = 1418:  log likelihood = -508.94\nAIC=1023.87   AICc=1024.12   BIC=1031.72\n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1\nTraining set 3.227728 37.10015 29.27302 0.17115 7.150118 0.4959847 -0.0275397\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1) errors\nQ* = 18.209, df = 19, p-value = 0.5085\n\nModel df: 1.   Total lags used: 20"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-3",
    "href": "ARIMAX_SARIMAX_VAR.html#model-diagnostics-3",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nModel coefficients\n\nThe model computed by auto.arima() is a ARIMAX model with ARIMA(0,1,1), the coefficient for the moving averafe term is -0.5074, which suggests a moderate negative effect from the previous error term.\nThe exogenous coefficient (representing the COVID-19 cases) is -14.4388, implying that as the COVID-19 cases increase, the number of total crimes decreases, holding other factors constant.\nThe negative exogenous coefficient might raise some questions about the relationship between Covid-19 cases and crime rates. It could suggest that an increase in Covid-19 cases leads to a reduction in reported assault crimes in NYC, possibly due to lockdowns or reduced public activity.\n\n\n\nResidual Analysis\n\nThe Ljung-Box test has a p-value of 0.665, which is well above 0.05, suggesting that the residuals are independently distributed and that there is no significant autocorrelation at lags up to 20.\nThe residual plots do not show any obvious patterns or trends, which is good as it suggests that the model has captured the data’s structure adequately. The histogram of residuals, overlaid with a normal distribution, seems to show that residuals are approximately normally distributed."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#fitting-manually-1",
    "href": "ARIMAX_SARIMAX_VAR.html#fitting-manually-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Fitting Manually",
    "text": "Fitting Manually\n\nLinear Regression\n\n\nCode\n############# First fit the linear model##########\nfit.reg &lt;- lm( total_crimes ~ total_cases, data=combined_ts)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = total_crimes ~ total_cases, data = combined_ts)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-168.350  -49.895    5.249   53.052  131.863 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  438.386     22.762  19.260   &lt;2e-16 ***\ntotal_cases   -2.375      3.214  -0.739    0.462    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 67.9 on 100 degrees of freedom\nMultiple R-squared:  0.005433,  Adjusted R-squared:  -0.004513 \nF-statistic: 0.5462 on 1 and 100 DF,  p-value: 0.4616\n\n\n\nWhile the linear regression model does not yield significant coefficients for COVID-19 cases, indicating a weak linear relationship, the variable may still be influential in an ARIMAX model.\n\n\n\nConverting to Time Series Component\n\n\nCode\n########### Converting to Time Series component #######\n\nres.fit&lt;-ts(residuals(fit.reg),start=decimal_date(as.Date(\"2020-01-19\",format = \"%Y-%m-%d\")),frequency = 52)\n\n\n\n\nResiduals ACF/PACF\n\n\nCode\n############## Then look at the residuals ############\nggAcf(res.fit)\n\n\n\n\n\n\n\nCode\nggPacf(res.fit)\n\n\n\n\n\n\nThe ACF plot of regression models residual has clear correlation, differencing needs to be applied for stationary purpose.\n\n\n\nDifferencing\n\n\nCode\nres.fit %&gt;% diff() %&gt;% ggtsdisplay() \n\n\n\n\n\n\nPotential Model Parameters\n\np = 0, 1, 2\nd = 0, 1\nq = 0, 1, 2\n\n\n\n\nFitting Model Parameters\n\n\nCode\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*18),nrow=18) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=0,1,2 : 3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 0:1)# d=0,1 :2\n    {\n      \n      if(p-1+d+q-1&lt;=8) #usual threshold\n      {\n        \n        model&lt;- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1119.359\n1127.234\n1119.604\n\n\n0\n1\n0\n1047.746\n1052.976\n1047.868\n\n\n0\n0\n1\n1077.669\n1088.169\n1078.082\n\n\n0\n1\n1\n1028.672\n1036.518\n1028.920\n\n\n0\n0\n2\n1070.725\n1083.850\n1071.350\n\n\n0\n1\n2\n1030.215\n1040.675\n1030.632\n\n\n1\n0\n0\n1047.058\n1057.558\n1047.470\n\n\n1\n1\n0\n1032.513\n1040.359\n1032.761\n\n\n1\n0\n1\n1038.258\n1051.383\n1038.883\n\n\n1\n1\n1\n1030.391\n1040.851\n1030.808\n\n\n1\n0\n2\n1039.241\n1054.991\n1040.125\n\n\n1\n1\n2\n1032.065\n1045.141\n1032.697\n\n\n2\n0\n0\n1039.652\n1052.777\n1040.277\n\n\n2\n1\n0\n1027.897\n1038.358\n1028.314\n\n\n2\n0\n1\n1039.774\n1055.524\n1040.659\n\n\n2\n1\n1\n1027.902\n1040.977\n1028.533\n\n\n2\n0\n2\n1041.017\n1059.392\n1042.208\n\n\n2\n1\n2\n1029.829\n1045.520\n1030.723\n\n\n\n\n\n\n\nCode\ntemp[which.min(temp$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n4 0 1 1 1028.672 1036.518 1028.92\n\n\n\n\nCode\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n14 2 1 0 1027.897 1038.358 1028.314\n\n\n\n\nCode\ntemp[which.min(temp$AIC),]\n\n\n   p d q      AIC      BIC     AICc\n14 2 1 0 1027.897 1038.358 1028.314\n\n\nWe have two models here, ARIMA(0, 1, 1) is suggested by the lowest BIC, and ARIMA(2, 1, 1) is suggest by lowest AIC and AICc\n\n\nARIMAX Model Diagnostics\n\n\nCode\n# Model diagnostics for ARIMA(0, 1, 1)\nsarima(res.fit, 0, 1, 1)\n\n\ninitial  value 3.748119 \niter   2 value 3.645637\niter   3 value 3.644385\niter   4 value 3.643230\niter   5 value 3.643132\niter   6 value 3.643092\niter   6 value 3.643092\nfinal  value 3.643092 \nconverged\ninitial  value 3.643801 \niter   2 value 3.643798\niter   3 value 3.643798\niter   4 value 3.643797\niter   4 value 3.643797\niter   4 value 3.643797\nfinal  value 3.643797 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.4556    0.6197\ns.e.   0.0825    2.0865\n\nsigma^2 estimated as 1459:  log likelihood = -511.34,  aic = 1028.67\n\n$degrees_of_freedom\n[1] 99\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.4556 0.0825 -5.5238  0.0000\nconstant   0.6197 2.0865  0.2970  0.7671\n\n$AIC\n[1] 10.18488\n\n$AICc\n[1] 10.18609\n\n$BIC\n[1] 10.26255\n\n\n\n\nCode\n# Model diagnostics for ARIMA(2, 1, 1)\nsarima(res.fit, 2, 1, 1)\n\n\ninitial  value 3.754095 \niter   2 value 3.690967\niter   3 value 3.647017\niter   4 value 3.645312\niter   5 value 3.644608\niter   6 value 3.642989\niter   7 value 3.642066\niter   8 value 3.629823\niter   9 value 3.628083\niter  10 value 3.624088\niter  11 value 3.623651\niter  12 value 3.623487\niter  13 value 3.623460\niter  14 value 3.623426\niter  15 value 3.623404\niter  16 value 3.623359\niter  17 value 3.623354\niter  18 value 3.623353\niter  18 value 3.623353\niter  18 value 3.623353\nfinal  value 3.623353 \nconverged\ninitial  value 3.620259 \niter   2 value 3.620234\niter   3 value 3.620217\niter   4 value 3.620210\niter   5 value 3.620195\niter   6 value 3.620180\niter   7 value 3.620178\niter   8 value 3.620178\niter   8 value 3.620178\niter   8 value 3.620178\nfinal  value 3.620178 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ma1  constant\n      -0.9188  -0.4247  0.4619    0.5346\ns.e.   0.2310   0.1066  0.2467    2.3238\n\nsigma^2 estimated as 1390:  log likelihood = -508.95,  aic = 1027.9\n\n$degrees_of_freedom\n[1] 97\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.9188 0.2310 -3.9785  0.0001\nar2       -0.4247 0.1066 -3.9834  0.0001\nma1        0.4619 0.2467  1.8719  0.0642\nconstant   0.5346 2.3238  0.2300  0.8185\n\n$AIC\n[1] 10.17724\n\n$AICc\n[1] 10.18137\n\n$BIC\n[1] 10.30671\n\n\n\nBoth models appear to perform well, with residuals that are normally distributed and Ljung-Box test p-values indicating no significant autocorrelation. Given that the residuals from both models meet these criteria, we will proceed with cross-validation to further compare their predictive performance and select the most appropriate model for our data.\n\n\n\nCross Validation\n\n\nCode\n# Function to forecast\nfarima1 &lt;- function(x, h){forecast(Arima(x, order=c(0,1,1)), h=h)}\nfarima2 &lt;- function(x, h){forecast(Arima(x, order=c(2,1,1)), h=h)}\n\n# 52 steps a head cross validation (a whole year)\nh &lt;- 52\nrmse1 &lt;- numeric(h)\nrmse2 &lt;- numeric(h)\n\nfor (i in 1:h) {\n  e1 &lt;- tsCV(res.fit, farima1, h=i)\n  e2 &lt;- tsCV(res.fit, farima2, h=i)\n  \n  # Calculate RMSE for each horizon\n  rmse1[i] &lt;- sqrt(mean(e1^2, na.rm=TRUE))\n  rmse2[i] &lt;- sqrt(mean(e2^2, na.rm=TRUE))\n}\n\n# Create a data frame for plotting\nrmse &lt;- data.frame(horizon = 1:h, \n                        RMSE1 = rmse1, \n                        RMSE2 = rmse2)\n\n# Plot RMSE vs Horizon\ncv1 &lt;- ggplot(data = rmse) + \n  geom_line(aes(x = horizon, y = RMSE1, colour = \"ARIMA(0,1,1)\")) + \n  geom_line(aes(x = horizon, y = RMSE2, colour = \"ARIMA(2,1,1)\")) +\n  labs(x = \"Forecast Horizon\", y = \"RMSE\", title = \"RMSE vs Forecast Horizon for ARIMA Models\") +\n   geom_point(aes(y=RMSE1,x= horizon)) + \n  geom_point(aes(y=RMSE2,x= horizon)) +\n  theme_minimal()\n\nggplotly(cv1)\n\n\n\n\n\n\nThe cross-validation plot indicates that for short-term forecasts of up to approximately 10 steps ahead, the ARIMA(0,1,1) model outperforms, exhibiting lower RMSE values. However, for longer-term forecasts beyond 10 steps, the ARIMA(2,1,1) model demonstrates improved performance, yielding more accurate predictions as evidenced by its reduced RMSE. Here I will use ARIMA(2, 1, 1)."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#model-equations-1",
    "href": "ARIMAX_SARIMAX_VAR.html#model-equations-1",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Model Equations",
    "text": "Model Equations\n\n\nCode\nfit &lt;- Arima(combined_ts[, \"total_crimes\"],order=c(2,1,1),xreg=xreg)\nsummary(fit)\n\n\nSeries: combined_ts[, \"total_crimes\"] \nRegression with ARIMA(2,1,1) errors \n\nCoefficients:\n          ar1      ar2     ma1      xreg\n      -0.8941  -0.4250  0.4090  -11.1580\ns.e.   0.2532   0.1148  0.2749    4.6857\n\nsigma^2 = 1400:  log likelihood = -507.3\nAIC=1024.59   AICc=1025.22   BIC=1037.67\n\nTraining set error measures:\n                   ME     RMSE      MAE          MPE     MAPE      MASE\nTraining set 2.325262 36.48714 29.16077 -0.005415194 7.163168 0.4940828\n                    ACF1\nTraining set -0.01284592\n\n\n\nThe model equation for the ARIMAX model is $(1 - 0.9213B - 0.4422B^2)(1 - B)Y_t = (1 + 0.4743B) _t - 12.8691 X_t $"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#forecasts-3",
    "href": "ARIMAX_SARIMAX_VAR.html#forecasts-3",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Forecasts",
    "text": "Forecasts\n\nFitting an ARIMA model to the COVID data\n\n\nCode\nfit_c &lt;- auto.arima(combined_ts[, \"total_cases\"]) #fitting an ARIMA model to the Covid data\nsummary(fit_c) \n\n\nSeries: combined_ts[, \"total_cases\"] \nARIMA(0,1,3) \n\nCoefficients:\n         ma1     ma2     ma3\n      0.7728  0.6654  0.2595\ns.e.  0.0928  0.1023  0.0943\n\nsigma^2 = 0.1263:  log likelihood = -37.79\nAIC=83.57   AICc=83.99   BIC=94.04\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03812665 0.3483306 0.2105525 1.441596 3.946224 0.1072581\n                    ACF1\nTraining set 0.007409321\n\n\nCode\n# Obtain the forecasts\nfc &lt;- forecast(fit_c,12)\nfc &lt;- fc$mean\n\n\n\n\nPresenting the Forecasts\n\n\nCode\nfcast &lt;- forecast(fit, xreg=fc,12) \nfcast\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2022.000       419.1293 371.1791 467.0795 345.7958 492.4627\n2022.019       409.1581 355.2247 463.0914 326.6741 491.6420\n2022.038       418.9455 359.4551 478.4358 327.9627 509.9282\n2022.058       415.7438 346.9130 484.5746 310.4761 521.0115\n2022.077       414.7071 341.1517 488.2625 302.2138 527.2003\n2022.096       416.9948 337.6859 496.3038 295.7022 538.2875\n2022.115       415.3900 330.5890 500.1910 285.6981 545.0820\n2022.135       415.8525 326.5126 505.1924 279.2189 552.4861\n2022.154       416.1211 321.9850 510.2572 272.1524 560.0898\n2022.173       415.6844 317.1563 514.2124 264.9987 566.3700\n2022.192       415.9607 313.2691 518.6523 258.9074 573.0140\n2022.212       415.8993 309.1044 522.6941 252.5707 579.2279\n\n\nCode\nautoplot(fcast, main=\"Forecast of Assault Crimes in NYC for the next quarter\") + xlab(\"Year\") +\n  ylab(\"GDP\")"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#evaluations-3",
    "href": "ARIMAX_SARIMAX_VAR.html#evaluations-3",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Evaluations",
    "text": "Evaluations\nThe results from the ARIMAX model were not as robust as anticipated, possibly because I did not account for seasonality in the data. Despite this, I opted not to use a SARIMAX model due to the limited amount of COVID-19 data available, which spans only approximately two years on a weekly basis. This duration may not suffice for effectively capturing and modeling seasonal patterns. To potentially enhance model performance, I plan to shift to daily data, which will provide a larger dataset that could be more suitable for a SARIMAX model, potentially yielding better results."
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Code\nlibrary(crimedata)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\n\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\ndatatable(head(data))\n\n\n\n\n\n\n\n\n\nCode\n# Convert datetime to date by extracting just the date part\ndata$date_single &lt;- as.Date(data$date_single)\n\nviolent_crimes &lt;- c(\"rape (except statutory rape)\", \"personal robbery\", \"aggravated assault\", \n                    \"murder and nonnegligent manslaughter\", \"arson\", \"kidnapping/abduction\")\n\n# Filter the data to include only violent crimes\nviolent_crime &lt;- data[data$offense_type %in% violent_crimes, ]\n\n# Sum up the counts by date\nviolent_crime_total &lt;- violent_crime %&gt;%\n  group_by(date_single) %&gt;%\n  summarise(Count = n())\n\n\nviolent_crime_t &lt;- violent_crime %&gt;%\n  select(offense_type, date_single)\n\n# Sum up the counts by date for each category\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  group_by(date_single, offense_type) %&gt;%\n  summarise(Total_Crimes = n(), .groups = 'drop')\n\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  mutate(offense_type = case_when(\n    offense_type == \"kidnapping/abduction\" ~ \"kidnap\",\n    offense_type == \"rape (except statutory rape)\" ~ \"rape\",\n    offense_type == \"murder and nonnegligent manslaughter\" ~ \"murder\",\n    offense_type == \"personal robbery\" ~ \"robbery\",\n    TRUE ~ offense_type \n  ))\n\nviolent_crime_total$offense_type &lt;- \"total\"\ncolnames(violent_crime_total)[2] &lt;- \"Total_Crimes\"\n\nv_crime &lt;- rbind(violent_crime_t, violent_crime_total)\n\nv_crime &lt;- v_crime %&gt;%\n  arrange(date_single, offense_type)\n\n# Display the result\ndatatable(v_crime)\n\n\n\n\n\n\n\nCode\n# Save the data\n#write.csv(v_crime, file = \"dataset/crimedata_total.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Data_Visualization.html#use-the-crimedata-api-in-r-to-extract-crime-data-of-nyc-from-2018-to-2021.",
    "href": "Data_Visualization.html#use-the-crimedata-api-in-r-to-extract-crime-data-of-nyc-from-2018-to-2021.",
    "title": "Data Visualization",
    "section": "",
    "text": "Code\nlibrary(crimedata)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\n\n#Load the crime data of NYC from 2018 to 2021\n#data &lt;- get_crime_data(\n#  cities = \"New York\", \n#  years = 2018:2021, \n#  type = \"core\",\n#  output = \"sf\"\n#) \n#Save the data\n#write.csv(data, file = \"dataset/crimedata_NYC.csv\", row.names = FALSE)\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\nhead(data)\n\n\n# A tibble: 6 × 15\n       uid city_name offense_code offense_type     offense_group offense_against\n     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;          \n1 17608791 New York  11D          fondling         sex offenses  persons        \n2 17608792 New York  11D          fondling         sex offenses  persons        \n3 17608793 New York  11A          rape (except st… sex offenses  persons        \n4 17608794 New York  11D          fondling         sex offenses  persons        \n5 17608795 New York  26U          other fraud      fraud offens… property       \n6 17608796 New York  11C          sexual assault … sex offenses  persons        \n# ℹ 9 more variables: date_single &lt;dttm&gt;, date_start &lt;dttm&gt;, date_end &lt;dttm&gt;,\n#   longitude &lt;dbl&gt;, latitude &lt;dbl&gt;, location_type &lt;chr&gt;,\n#   location_category &lt;chr&gt;, census_block &lt;dbl&gt;, geometry &lt;chr&gt;\n\n\n\n\nCode\n# Convert datetime to date by extracting just the date part\ndata$date_single &lt;- as.Date(data$date_single)\n\nviolent_crimes &lt;- c(\"rape (except statutory rape)\", \"personal robbery\", \"aggravated assault\", \n                    \"murder and nonnegligent manslaughter\", \"arson\", \"kidnapping/abduction\")\n\n# Filter the data to include only violent crimes\nviolent_crime &lt;- data[data$offense_type %in% violent_crimes, ]\n\n# Sum up the counts by date\nviolent_crime_total &lt;- violent_crime %&gt;%\n  group_by(date_single) %&gt;%\n  summarise(Count = n())\n\n\nviolent_crime_t &lt;- violent_crime %&gt;%\n  select(offense_type, date_single)\n\n# Sum up the counts by date for each category\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  group_by(date_single, offense_type) %&gt;%\n  summarise(Total_Crimes = n(), .groups = 'drop')\n\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  mutate(offense_type = case_when(\n    offense_type == \"kidnapping/abduction\" ~ \"kidnap\",\n    offense_type == \"rape (except statutory rape)\" ~ \"rape\",\n    offense_type == \"murder and nonnegligent manslaughter\" ~ \"murder\",\n    offense_type == \"personal robbery\" ~ \"robbery\",\n    TRUE ~ offense_type \n  ))\n\nviolent_crime_total$offense_type &lt;- \"total\"\ncolnames(violent_crime_total)[2] &lt;- \"Total_Crimes\"\n\nv_crime &lt;- rbind(violent_crime_t, violent_crime_total)\n\nv_crime &lt;- v_crime %&gt;%\n  arrange(date_single, offense_type)\n\n# Display the result\ndatatable(v_crime)\n\n\n\n\n\n\n\nCode\n# Save the data\n#write.csv(v_crime, file = \"dataset/crimedata_total.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Data_Visualization.html#total-violent-crimes-visualization",
    "href": "Data_Visualization.html#total-violent-crimes-visualization",
    "title": "Data Visualization",
    "section": "Total violent crimes visualization",
    "text": "Total violent crimes visualization\n\n\nCode\nt1 &lt;- ggplot(data = v_crime, aes(x = date_single, y = Total_Crimes, color = offense_type)) +\n  geom_line(alpha = 0.5, size = 0.6) +  \n  geom_smooth(se = FALSE, method = \"loess\", span = 0.2) +  \n  labs(title = \"Total Violent Crime Time Series from 2007 to 2022 in NYC\",\n       x = \"Date\",\n       y = \"Number of Crimes\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\",\n        legend.direction = \"horizontal\") +\n  scale_color_manual(values = c(\"aggravated assault\" = \"red\", \"arson\" = \"orange\", \n                                \"kidnap\" = \"green\", \"murder\" = \"blue\", \n                                \"rape\" = \"purple\", \n                                \"robbery\" = \"gray\",\n                                \"total\" = \"black\")) \n\nggplotly(t1)\n\n\n\n\n\n\n\nBased on the interactive visualization, Aggravated assault, indicated as red line plot, appears to be the most common violent crime category among all,Total violent crime is the sum of all violent crimes, and is represented by the line in black. Interestingly, the Robbery trend is in a decline trend indicating drops in this type of criminal activity through out the time frame. Other crimes such as arson, kidnap, murder, and rapeare presented with much less frequency."
  },
  {
    "objectID": "Data_Visualization.html#data-visualization-for-each-sub-category",
    "href": "Data_Visualization.html#data-visualization-for-each-sub-category",
    "title": "Data Visualization",
    "section": "Data visualization for each sub-category",
    "text": "Data visualization for each sub-category\n\n\nCode\ncrime_categories &lt;- c(\"total\",\"robbery\",\"kidnap\", \"aggravated assault\", \"arson\", \"rape\", \"murder\")\n\n# Empty list to store plots\nplot_list &lt;- list()\n\n# Loop through each crime category and create a plot\nfor (crime in crime_categories) {\n  a &lt;- v_crime %&gt;%\n    filter(offense_type == crime)\n\n  plot &lt;- ggplot(data = a, aes(x = date_single, y = Total_Crimes)) +\n    geom_line() +\n    labs(title = paste(crime, \"crime time series from 2007 to 2022\"),\n         x = \"Date\",\n         y = \"Number of Crimes\") +\n    theme_minimal() +\n    geom_line(alpha = 0.5, size = 0.6) +\n    geom_smooth(se = FALSE, method = \"loess\", span = 0.2) \n  plot_list[[crime]] &lt;- ggplotly(plot)\n}\n\n\n\nTotal\n\n\nCode\nplot_list[[\"total\"]]\n\n\n\n\n\n\n\nThis plot illustrates the total violent crime trends in New York City from 2007 to 2022, This blue trend line smooths out the variations. While we can see clear oscillations in crime rates, denoted by periodic rises and falls, the overall trajectory does not display a long-term increase or decrease. The data suggests that despite various short-term fluctuations, New York City’s violent crime rate has maintained relatively consistent through out the time frame.\n\n\n\nRobbery\n\n\nCode\nplot_list[[\"robbery\"]]\n\n\n\n\n\n\n\nThis plot illustrates the robbery crime trends in New York City across from 2007 to 2022, This blue trend line smooths out the variations. While we can see clear oscillations in crime rates, The trend shows a consistently decline until around at the end of 2020, the robbery crimes trend started a upward trend, this is interesting since the timeline matches the outbreak of COVID-19.\n\n\n\nKidnap/Rape/Murder/Arson\n\n\nCode\nplot_list[[\"kidnap\"]]\n\n\n\n\n\n\n\n\nCode\nplot_list[[\"rape\"]]\n\n\n\n\n\n\n\n\nCode\nplot_list[[\"murder\"]]\n\n\n\n\n\n\n\n\nCode\nplot_list[[\"arson\"]]\n\n\n\n\n\n\n\nAll plots above do not illustrate any obvious upward/downward trend.\n\n\n\nAggravated Assault\n\n\nCode\nplot_list[[\"aggravated assault\"]]\n\n\n\n\n\n\n\nThis plot illustrates the aggravated assault crime trends in New York City from 2007 to 2022, This blue trend line smooths out the variations. While we can see clear oscillations in crime rates. The trend shows a steadily increase until around at the end of 2020, the slope of aggravated assault crimes increases, this is interesting since the timeline matches the outbreak of COVID-19."
  },
  {
    "objectID": "Data_Visualization.html#crime-data-map-of-violent-crimes-in-nyc-on-01012008",
    "href": "Data_Visualization.html#crime-data-map-of-violent-crimes-in-nyc-on-01012008",
    "title": "Data Visualization",
    "section": "Crime data map of violent crimes in NYC on 01/01/2008",
    "text": "Crime data map of violent crimes in NYC on 01/01/2008\n\nThis map visualization is generated using the geographical varible obtained from the crime data base.\n\n\n\nCode\ndata_test &lt;- data[data$offense_type %in% violent_crimes, ]\n\ndata_test$date &lt;- as.Date(data_test$date_single)\n\ndata_test &lt;- data_test %&gt;%\n  filter(date == \"2008-01-01\")\n\n# Convert offense_type to a factor\ndata_test$offense_type &lt;- as.factor(data_test$offense_type)\n\n# Create a custom color scale for each offense type\ncolor_palette &lt;- brewer.pal(length(unique(data_test$offense_type)), \"Dark2\")\ncolor_scale &lt;- setNames(color_palette, levels(data_test$offense_type))\n\n# Map offense_type to corresponding color\ndata_test$color &lt;- color_scale[data_test$offense_type]\n\n# Create the plot with the custom color scale\nfig &lt;- plot_ly(\n  data = data_test,\n  type = \"scattermapbox\",\n  mode = \"markers\",\n  lon = ~longitude,\n  lat = ~latitude,\n  text = ~paste(\"Date: \", date, \"&lt;br&gt;Crime Type: \", offense_type),\n  marker = list(\n    size = 10,\n    color = ~color\n  )\n) %&gt;%\n  layout(\n    mapbox = list(\n      center = list(lon = -73.95, lat = 40.75),\n      zoom = 12,\n      style = \"open-street-map\"\n    )\n  )\n\nfig"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "As we can see from the time series plot, all three companies stock is steadily increasing from 2014 to 2021, Netflix stockthe largest increase. But Until the end of 2021, all three stocks dropped significantly, Netflix Stock also has the largest drop.\n\n\n\n\n\n\nWe can tell from this plot that the max temperature of a day in DC is increasing from January to September, which makes sense since the weather is going from Winter to Summer.\n\n\n\n\n\n\nWe can see from the plot that GDP in US in increasing from 1947 to 2023."
  },
  {
    "objectID": "Financial_Time_Series_Models.html",
    "href": "Financial_Time_Series_Models.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "The relationship between crime and the stock performance of companies such as Walmart is an intriguing and multifaceted subject of study. Walmart, as one of the world’s largest retail giants, operates in a diverse range of locations, each with its unique set of challenges related to crime and security. Understanding how crime rates can impact the stock price of a company like Walmart requires a comprehensive analysis that takes into account not only economic and market dynamics but also the local socio-economic environment, security measures, and loss prevention strategies employed by the company. In this context, the application of ARCH/GARCH (Autoregressive Conditional Heteroskedasticity/Generalized Autoregressive Conditional Heteroskedasticity) models becomes a valuable tool for assessing the volatility and risk associated with Walmart’s stock performance, as it allows us to capture and model the time-varying volatility patterns in the stock returns, potentially shedding light on the impact of crime-related events on the company’s stock price."
  },
  {
    "objectID": "Financial_Time_Series_Models.html#introduction",
    "href": "Financial_Time_Series_Models.html#introduction",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "The relationship between crime and the stock performance of companies such as Walmart is an intriguing and multifaceted subject of study. Walmart, as one of the world’s largest retail giants, operates in a diverse range of locations, each with its unique set of challenges related to crime and security. Understanding how crime rates can impact the stock price of a company like Walmart requires a comprehensive analysis that takes into account not only economic and market dynamics but also the local socio-economic environment, security measures, and loss prevention strategies employed by the company. In this context, the application of ARCH/GARCH (Autoregressive Conditional Heteroskedasticity/Generalized Autoregressive Conditional Heteroskedasticity) models becomes a valuable tool for assessing the volatility and risk associated with Walmart’s stock performance, as it allows us to capture and model the time-varying volatility patterns in the stock returns, potentially shedding light on the impact of crime-related events on the company’s stock price."
  },
  {
    "objectID": "Financial_Time_Series_Models.html#archgarch-model",
    "href": "Financial_Time_Series_Models.html#archgarch-model",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "ARCH/GARCH model",
    "text": "ARCH/GARCH model\n\nData Preparation\n\nLoad the Walmart stock prices data\n\n\n\nCode\n# Load the Stock Price data\ndata &lt;- getSymbols(\"WMT\",auto.assign = FALSE, from = \"2016-10-01\",src=\"yahoo\") \nhead(data)\n\n\n           WMT.Open WMT.High WMT.Low WMT.Close WMT.Volume WMT.Adjusted\n2016-10-03    71.83    72.15   71.60     72.01    5857700     62.65071\n2016-10-04    72.00    72.57   71.55     71.75    6569300     62.42453\n2016-10-05    71.87    72.27   71.60     71.67    6464400     62.35490\n2016-10-06    71.67    71.67   68.89     69.36   20110300     60.34513\n2016-10-07    69.26    69.47   68.17     68.70   12653800     59.77094\n2016-10-10    68.75    68.96   67.75     67.98    8988400     59.14450\n\n\n\nClosing price visualization\n\n\n\nCode\nchartSeries(data, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color)\n\n\n\n\n\n\nReturn visualization\n\n\n\nCode\nlibrary(imputeTS)\ndata &lt;- na_ma(data, k = 4, weighting = \"exponential\")\nlog(data$WMT.Adjusted ) %&gt;% diff() %&gt;% chartSeries()\n\n\n\n\n\n\nStationarity: The initial plot illustrates upward trends in Walmart’s stock prices, accompanied by noticeable fluctuations in volatility over time.\nVolatility: The plot shows periods of increased volatility, as evidenced by the taller spikes, which could correspond to specific events or announcements affecting the stock’s price. The clustering of large spikes suggests volatility clustering, a common phenomenon in financial time series where large changes tend to be followed by large changes (of either sign) and small changes tend to be followed by small changes.\nCalculate and plot the returns\n\n\n\nCode\n# Transform data into a time series object\nmts &lt;- ts(data$WMT.Adjusted, start=decimal_date(as.Date(\"2016-10-03\")), frequency = 365.25)\n\n# calculate the returns\nreturns = log(mts) %&gt;% diff()\n\n# Plot the returns\nautoplot(returns) +ggtitle(\"Walmart Returns\")\n\n\n\n\n\n\nACF/PACF plots of the returns\n\n\n\nCode\nggAcf(returns,40)\n\n\n\n\n\n\n\nCode\nggPacf(returns,40)\n\n\n\n\n\n\nWe could see clear stationarity based on the ACF/PACF plots.\n\n\n\nACF plots of absolute values of the returns and squared values\n\n\nCode\nggAcf(abs(returns),40)\n\n\n\n\n\n\n\nCode\nggAcf(returns^2,40)\n\n\n\n\n\n\nWe can see clear correlation in both plots. This correlation is coming from the correlation in conditional variation.\n\n\n\nModel Fitting(ARIMA + GARCH)\n\nArchTest\n\n\n\nCode\nlibrary(FinTS)\nArchTest(returns, lags=1, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 115.54, df = 1, p-value &lt; 2.2e-16\n\n\n\nBecause the p-value is &lt; 0.05, we reject the null hypothesis and conclude the presence of ARCH(1) effects.\nFitting the ARIMA Model\n\n\n\nCode\nggAcf(returns,40)\n\n\n\n\n\n\n\nCode\nggPacf(returns, 40)\n\n\n\n\n\n\nBased on the ACF/PACF plots, we set p = 0 ~ 5, q = 0 ~ 5, d = 0 ~ 2.\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*70),nrow=70)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 1:2)#\n    {\n      \n      if(p+d+q&lt;=7)\n      {\n        \n        model&lt;- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\n\n\n\nCode\noutput &lt;- ARIMA.c(0,5,0,5,data=log(mts))\noutput\n\n\n    p  d  q        AIC        BIC       AICc\n1   0  1  0 -10379.598 -10374.098 -10379.595\n2   0  2  0  -9014.112  -9008.613  -9014.110\n3   0  1  1 -10384.255 -10373.256 -10384.248\n4   0  2  1 -10365.691 -10354.694 -10365.685\n5   0  1  2 -10382.368 -10365.870 -10382.355\n6   0  2  2 -10370.546 -10354.050 -10370.533\n7   0  1  3 -10381.219 -10359.221 -10381.197\n8   0  2  3 -10368.631 -10346.636 -10368.609\n9   0  1  4 -10384.117 -10356.620 -10384.084\n10  0  2  4 -10367.577 -10340.083 -10367.544\n11  0  1  5 -10382.160 -10349.164 -10382.114\n12  0  2  5 -10370.732 -10337.738 -10370.685\n13  1  1  0 -10384.347 -10373.348 -10384.341\n14  1  2  0  -9616.742  -9605.744  -9616.735\n15  1  1  1 -10382.321 -10365.822 -10382.307\n16  1  2  1 -10370.628 -10354.132 -10370.615\n17  1  1  2 -10380.387 -10358.389 -10380.365\n18  1  2  2 -10368.288 -10346.292 -10368.266\n19  1  1  3 -10381.726 -10354.229 -10381.693\n20  1  2  3 -10366.546 -10339.052 -10366.513\n21  1  1  4 -10382.144 -10349.148 -10382.098\n22  1  2  4 -10364.698 -10331.705 -10364.652\n23  1  1  5 -10380.168 -10341.672 -10380.106\n24  2  1  0 -10382.416 -10365.918 -10382.403\n25  2  2  0  -9829.081  -9812.584  -9829.067\n26  2  1  1 -10380.406 -10358.408 -10380.383\n27  2  2  1 -10368.678 -10346.682 -10368.656\n28  2  1  2 -10380.177 -10352.680 -10380.144\n29  2  2  2 -10370.119 -10342.625 -10370.086\n30  2  1  3 -10380.585 -10347.588 -10380.538\n31  2  2  3 -10366.268 -10333.275 -10366.221\n32  2  1  4 -10385.057 -10346.561 -10384.995\n33  3  1  0 -10380.795 -10358.797 -10380.773\n34  3  2  0  -9920.040  -9898.044  -9920.018\n35  3  1  1 -10381.472 -10353.975 -10381.439\n36  3  2  1 -10367.111 -10339.616 -10367.077\n37  3  1  2 -10381.286 -10348.290 -10381.240\n38  3  2  2 -10368.020 -10335.027 -10367.973\n39  3  1  3 -10383.463 -10344.967 -10383.400\n40  4  1  0 -10383.477 -10355.980 -10383.444\n41  4  2  0 -10007.693  -9980.199 -10007.660\n42  4  1  1 -10381.565 -10348.569 -10381.519\n43  4  2  1 -10370.003 -10337.009 -10369.956\n44  4  1  2 -10385.642 -10347.146 -10385.579\n45  5  1  0 -10381.541 -10348.545 -10381.494\n46  5  2  0 -10059.386 -10026.393 -10059.339\n47  5  1  1 -10379.734 -10341.238 -10379.671\n48 NA NA NA         NA         NA         NA\n49 NA NA NA         NA         NA         NA\n50 NA NA NA         NA         NA         NA\n51 NA NA NA         NA         NA         NA\n52 NA NA NA         NA         NA         NA\n53 NA NA NA         NA         NA         NA\n54 NA NA NA         NA         NA         NA\n55 NA NA NA         NA         NA         NA\n56 NA NA NA         NA         NA         NA\n57 NA NA NA         NA         NA         NA\n58 NA NA NA         NA         NA         NA\n59 NA NA NA         NA         NA         NA\n60 NA NA NA         NA         NA         NA\n61 NA NA NA         NA         NA         NA\n62 NA NA NA         NA         NA         NA\n63 NA NA NA         NA         NA         NA\n64 NA NA NA         NA         NA         NA\n65 NA NA NA         NA         NA         NA\n66 NA NA NA         NA         NA         NA\n67 NA NA NA         NA         NA         NA\n68 NA NA NA         NA         NA         NA\n69 NA NA NA         NA         NA         NA\n70 NA NA NA         NA         NA         NA\n\n\n\n\nCode\noutput[which.min(output$AIC),]\n\n\n   p d q       AIC       BIC      AICc\n44 4 1 2 -10385.64 -10347.15 -10385.58\n\n\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 0 1 0 -10379.6 -10374.1 -10379.6\n\n\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n   p d q       AIC       BIC      AICc\n44 4 1 2 -10385.64 -10347.15 -10385.58\n\n\n\nUsing auto.arima()\n\n\n\nCode\nauto.arima(log(mts))\n\n\nSeries: log(mts) \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1  drift\n      -0.0619  5e-04\ns.e.   0.0234  3e-04\n\nsigma^2 = 0.0001865:  log likelihood = 5195.46\nAIC=-10384.92   AICc=-10384.91   BIC=-10368.42\n\n\n\nHere, we have ARIMA(3, 0, 3), ARIMA(0, 1, 0), ARIMA(0, 1, 1).\n\n\n\nModel Diagnostics\n\n\nCode\ndata=log(mts)\nsarima(data, 0,1,0)\n\n\ninitial  value -4.292192 \niter   1 value -4.292192\nfinal  value -4.292192 \nconverged\ninitial  value -4.292192 \niter   1 value -4.292192\nfinal  value -4.292192 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n         5e-04\ns.e.     3e-04\n\nsigma^2 estimated as 0.000187:  log likelihood = 5191.97,  aic = -10379.94\n\n$degrees_of_freedom\n[1] 1806\n\n$ttable\n         Estimate    SE t.value p.value\nconstant    5e-04 3e-04  1.5258  0.1272\n\n$AIC\n[1] -5.744292\n\n$AICc\n[1] -5.744291\n\n$BIC\n[1] -5.738206\n\n\n\n\nCode\nsarima(data, 4,1,2)\n\n\ninitial  value -4.292901 \niter   2 value -4.294456\niter   3 value -4.296489\niter   4 value -4.296499\niter   5 value -4.296504\niter   6 value -4.296522\niter   7 value -4.296620\niter   8 value -4.296741\niter   9 value -4.296851\niter  10 value -4.296917\niter  11 value -4.296969\niter  12 value -4.297008\niter  13 value -4.297089\niter  14 value -4.297125\niter  15 value -4.297138\niter  16 value -4.297141\niter  17 value -4.297144\niter  18 value -4.297152\niter  19 value -4.297207\niter  20 value -4.297311\niter  21 value -4.297545\niter  22 value -4.297612\niter  23 value -4.297837\niter  24 value -4.297963\niter  25 value -4.298036\niter  26 value -4.298163\niter  27 value -4.298241\niter  28 value -4.298323\niter  29 value -4.298382\niter  30 value -4.298454\niter  31 value -4.298455\niter  32 value -4.298455\niter  32 value -4.298455\niter  32 value -4.298455\nfinal  value -4.298455 \nconverged\ninitial  value -4.297419 \niter   2 value -4.297423\niter   3 value -4.297425\niter   4 value -4.297426\niter   5 value -4.297427\niter   6 value -4.297429\niter   7 value -4.297430\niter   8 value -4.297430\niter   9 value -4.297430\niter  10 value -4.297431\niter  11 value -4.297431\niter  12 value -4.297432\niter  13 value -4.297432\niter  13 value -4.297432\niter  13 value -4.297432\nfinal  value -4.297432 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1      ma2  constant\n      -0.3563  0.4355  0.0101  -0.0697  0.2957  -0.4527     5e-04\ns.e.   0.2512  0.2370  0.0318   0.0240  0.2515   0.2226     3e-04\n\nsigma^2 estimated as 0.000185:  log likelihood = 5201.44,  aic = -10386.88\n\n$degrees_of_freedom\n[1] 1800\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3563 0.2512 -1.4179  0.1564\nar2        0.4355 0.2370  1.8375  0.0663\nar3        0.0101 0.0318  0.3179  0.7506\nar4       -0.0697 0.0240 -2.9041  0.0037\nma1        0.2957 0.2515  1.1758  0.2398\nma2       -0.4527 0.2226 -2.0333  0.0422\nconstant   0.0005 0.0003  1.7953  0.0728\n\n$AIC\n[1] -5.748133\n\n$AICc\n[1] -5.748098\n\n$BIC\n[1] -5.723786\n\n\n\n\nCode\nsarima(data, 0,1,1)\n\n\ninitial  value -4.292192 \niter   2 value -4.294125\niter   3 value -4.294126\niter   3 value -4.294126\niter   3 value -4.294126\nfinal  value -4.294126 \nconverged\ninitial  value -4.294125 \niter   1 value -4.294125\nfinal  value -4.294125 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.0619     5e-04\ns.e.   0.0234     3e-04\n\nsigma^2 estimated as 0.0001863:  log likelihood = 5195.46,  aic = -10384.92\n\n$degrees_of_freedom\n[1] 1805\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.0619 0.0234 -2.6464  0.0082\nconstant   0.0005 0.0003  1.6292  0.1034\n\n$AIC\n[1] -5.747052\n\n$AICc\n[1] -5.747048\n\n$BIC\n[1] -5.737921\n\n\n\nI opted for an ARIMA(0, 1, 0) model due to the lack of significance in the coefficients of the other two models. However, it’s evident that further modeling is warranted, primarily because the standard residuals do not conform to an ideal normal distribution. As a next step, I plan to conduct GARCH modeling to better capture and analyze the volatility patterns in the data.\n\n\n\nFit the GARCH Model\n\nfit the ARIMA model and fitting a GARCH model to the residuals of the ARIMA model\n\n\n\nCode\narima.fit&lt;-Arima(data,order=c(0,1,0),include.drift = TRUE)\narima.res&lt;-arima.fit$residuals\n\nacf(arima.res)\n\n\n\n\n\n\n\nCode\nggAcf(arima.res^2, 40)\n\n\n\n\n\n\n\nCode\nggPacf(arima.res^2, 40)\n\n\n\n\n\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:7) {\n  for (q in 1:7) {\n  \nmodel[[cc]] &lt;- garch(arima.res,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 2\n\n\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2  \n3.778e-05  2.246e-01  9.000e-02  5.003e-01  \n\n\n\n\nCode\nsummary(garchFit(~garch(1,1), arima.res,trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x00000235364889b8&gt;\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n2.2922e-06  2.5793e-05  1.4189e-01  7.2697e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     2.292e-06   2.777e-04    0.008    0.993    \nomega  2.579e-05   6.085e-06    4.239 2.24e-05 ***\nalpha1 1.419e-01   3.009e-02    4.715 2.41e-06 ***\nbeta1  7.270e-01   5.159e-02   14.091  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5335.594    normalized:  2.951103 \n\nDescription:\n Fri Dec  8 02:31:16 2023 by user: 22426 \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2.531831e+04 0.0000000\n Shapiro-Wilk Test  R    W      8.689852e-01 0.0000000\n Ljung-Box Test     R    Q(10)  8.399467e+00 0.5898789\n Ljung-Box Test     R    Q(15)  1.027491e+01 0.8021004\n Ljung-Box Test     R    Q(20)  1.621943e+01 0.7029211\n Ljung-Box Test     R^2  Q(10)  1.533055e+00 0.9988287\n Ljung-Box Test     R^2  Q(15)  2.138105e+00 0.9999539\n Ljung-Box Test     R^2  Q(20)  2.390255e+00 0.9999994\n LM Arch Test       R    TR^2   1.627614e+00 0.9997981\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.897781 -5.885613 -5.897791 -5.893291 \n\n\n\nAll Coefficients are significant, also the Ljung-box test no obvious auto-correlation within the residuals, indicating a good overall fit to the data.\n\n\n\nFinal Model\n\n\nCode\nsummary(arima.fit&lt;-Arima(data,order=c(0,1,0),include.drift = TRUE))\n\n\nSeries: data \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      5e-04\ns.e.  3e-04\n\nsigma^2 = 0.0001871:  log likelihood = 5191.97\nAIC=-10379.94   AICc=-10379.93   BIC=-10368.94\n\nTraining set error measures:\n                       ME       RMSE         MAE           MPE      MAPE\nTraining set 2.288208e-06 0.01367149 0.008777026 -0.0001569746 0.1879001\n                   MASE        ACF1\nTraining set 0.04514855 -0.06250935\n\n\n\n\nCode\nsummary(final.fit &lt;- garchFit(~garch(1,1), arima.res,trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x00000235353ddda0&gt;\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n2.2922e-06  2.5793e-05  1.4189e-01  7.2697e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     2.292e-06   2.777e-04    0.008    0.993    \nomega  2.579e-05   6.085e-06    4.239 2.24e-05 ***\nalpha1 1.419e-01   3.009e-02    4.715 2.41e-06 ***\nbeta1  7.270e-01   5.159e-02   14.091  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5335.594    normalized:  2.951103 \n\nDescription:\n Fri Dec  8 02:31:16 2023 by user: 22426 \n\n\nStandardised Residuals Tests:\n                                   Statistic   p-Value\n Jarque-Bera Test   R    Chi^2  2.531831e+04 0.0000000\n Shapiro-Wilk Test  R    W      8.689852e-01 0.0000000\n Ljung-Box Test     R    Q(10)  8.399467e+00 0.5898789\n Ljung-Box Test     R    Q(15)  1.027491e+01 0.8021004\n Ljung-Box Test     R    Q(20)  1.621943e+01 0.7029211\n Ljung-Box Test     R^2  Q(10)  1.533055e+00 0.9988287\n Ljung-Box Test     R^2  Q(15)  2.138105e+00 0.9999539\n Ljung-Box Test     R^2  Q(20)  2.390255e+00 0.9999994\n LM Arch Test       R    TR^2   1.627614e+00 0.9997981\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-5.897781 -5.885613 -5.897791 -5.893291 \n\n\n\n\nModel Equation\n\nBased on the model diagnostics, the equations of the final model is given by:\n\n\\(x_t = 0.0005 + \\varepsilon_t\\)\n\\(z_t = \\sigma_t \\varepsilon_t\\)\n\\(\\sigma^2_t = 0.000024167 + 0.1379 z^2_{t-1} + 0.74066 \\sigma^2_{t-1}\\)"
  },
  {
    "objectID": "quiz1.html",
    "href": "quiz1.html",
    "title": "5600 quiz1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(tidyquant)  \n\n\nLoading required package: PerformanceAnalytics\nLoading required package: xts\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nAttaching package: 'PerformanceAnalytics'\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\nLoading required package: quantmod\nLoading required package: TTR\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nORCL &lt;- getSymbols(\"ORCL\",auto.assign = FALSE, from = \"2022-01-01\",src=\"yahoo\")\nORCL=data.frame(ORCL)\nORCL &lt;- data.frame(ORCL,rownames(ORCL))\ncolnames(ORCL)[7] = \"date\"\nORCL$date &lt;- as.Date(ORCL$date,\"%Y-%m-%d\")\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(autoplotly)\nlibrary(forecast)\n\n\nRegistered S3 methods overwritten by 'forecast':\n  method                 from     \n  autoplot.Arima         ggfortify\n  autoplot.acf           ggfortify\n  autoplot.ar            ggfortify\n  autoplot.bats          ggfortify\n  autoplot.decomposed.ts ggfortify\n  autoplot.ets           ggfortify\n  autoplot.forecast      ggfortify\n  autoplot.stl           ggfortify\n  autoplot.ts            ggfortify\n  fitted.ar              ggfortify\n  fortify.ts             ggfortify\n  residuals.ar           ggfortify\n\n\nCode\ns &lt;- ggplot(ORCL, aes(y=ORCL.Adjusted,x=date))+\n  labs(\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\",\n    title = \"Oracle stock prices from 2022 until now\")+\n  geom_candlestick(aes(\n    open = ORCL.Open, high = ORCL.High, low = ORCL.Low, close = ORCL.Close)\n    )+\n    theme_tq()+\n   geom_ma(ma_fun = SMA, n = 20, linetype = 5, size = 1.25) +\n    geom_ma(ma_fun = SMA, n = 50, color = \"red\", size = 1.25)\ns\n\n\nWarning: The following aesthetics were dropped during statistical transformation: open,\nhigh, low, close, y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: The following aesthetics were dropped during statistical transformation: x,\nopen, high, low, close, y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nCode\nggplotly(s)\n\n\nWarning: The following aesthetics were dropped during statistical transformation: open,\nhigh, low, close, y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: The following aesthetics were dropped during statistical transformation: x,\nopen, high, low, close, y\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning in geom2trace.default(dots[[1L]][[1L]], dots[[2L]][[1L]], dots[[3L]][[1L]]): geom_GeomLinerangeBC() has yet to be implemented in plotly.\n  If you'd like to see this geom implemented,\n  Please open an issue with your example code at\n  https://github.com/ropensci/plotly/issues\n\n\nWarning in geom2trace.default(dots[[1L]][[1L]], dots[[2L]][[1L]], dots[[3L]][[1L]]): geom_GeomRectCS() has yet to be implemented in plotly.\n  If you'd like to see this geom implemented,\n  Please open an issue with your example code at\n  https://github.com/ropensci/plotly/issues\n\n\nWarning in geom2trace.default(dots[[1L]][[1L]], dots[[2L]][[1L]], dots[[3L]][[1L]]): geom_GeomMA() has yet to be implemented in plotly.\n  If you'd like to see this geom implemented,\n  Please open an issue with your example code at\n  https://github.com/ropensci/plotly/issues\n\nWarning in geom2trace.default(dots[[1L]][[1L]], dots[[2L]][[1L]], dots[[3L]][[1L]]): geom_GeomMA() has yet to be implemented in plotly.\n  If you'd like to see this geom implemented,\n  Please open an issue with your example code at\n  https://github.com/ropensci/plotly/issues\n\n\n\n\n\n\n\n\nCode\nsma20 &lt;- ma(ORCL, 20)\nsma50 &lt;-ma(ORCL, 50)\nsma20 &lt;- data.frame(sma20)\nsma20 &lt;- sma20 %&gt;%\n  select(X4)\nORCL &lt;- cbind(ORCL, sma20)\ncolnames(ORCL)[8]&lt;- \"sma20\"\n\nsma50 &lt;- data.frame(sma50)\nsma50 &lt;- sma50 %&gt;%\n  select(X4)\nORCL &lt;- cbind(ORCL, sma50)\ncolnames(ORCL)[9]&lt;- \"sma50\"\n\nfigc &lt;- ORCL %&gt;% plot_ly(x = ~date, type=\"candlestick\",\n          open = ~ORCL.Open, close = ~ORCL.Close,\n          high = ~ORCL.High, low = ~ORCL.Low, name = \"Candlesticks\") \nfigc &lt;- figc %&gt;% layout(title = \"Oracle stock prices from 2022 until now\")\n\nfigc &lt;- figc %&gt;% add_lines(x = ~date, y = ~sma20, line = list(color= \"orange\", widthh=0.5), name = \"sma20\") %&gt;%\n  add_lines(x = ~date, y = ~sma50, line = list(color= \"blue\", widthh=0.5), name = \"sma50\")\n\nfigc\n\n\nWarning: 'scatter' objects don't have these attributes: 'open', 'close', 'high', 'low'\nValid attributes include:\n'cliponaxis', 'connectgaps', 'customdata', 'customdatasrc', 'dx', 'dy', 'error_x', 'error_y', 'fill', 'fillcolor', 'fillpattern', 'groupnorm', 'hoverinfo', 'hoverinfosrc', 'hoverlabel', 'hoveron', 'hovertemplate', 'hovertemplatesrc', 'hovertext', 'hovertextsrc', 'ids', 'idssrc', 'legendgroup', 'legendgrouptitle', 'legendrank', 'line', 'marker', 'meta', 'metasrc', 'mode', 'name', 'opacity', 'orientation', 'selected', 'selectedpoints', 'showlegend', 'stackgaps', 'stackgroup', 'stream', 'text', 'textfont', 'textposition', 'textpositionsrc', 'textsrc', 'texttemplate', 'texttemplatesrc', 'transforms', 'type', 'uid', 'uirevision', 'unselected', 'visible', 'x', 'x0', 'xaxis', 'xcalendar', 'xhoverformat', 'xperiod', 'xperiod0', 'xperiodalignment', 'xsrc', 'y', 'y0', 'yaxis', 'ycalendar', 'yhoverformat', 'yperiod', 'yperiod0', 'yperiodalignment', 'ysrc', 'key', 'set', 'frame', 'transforms', '_isNestedKey', '_isSimpleKey', '_isGraticule', '_bbox'\n\nWarning: 'scatter' objects don't have these attributes: 'open', 'close', 'high', 'low'\nValid attributes include:\n'cliponaxis', 'connectgaps', 'customdata', 'customdatasrc', 'dx', 'dy', 'error_x', 'error_y', 'fill', 'fillcolor', 'fillpattern', 'groupnorm', 'hoverinfo', 'hoverinfosrc', 'hoverlabel', 'hoveron', 'hovertemplate', 'hovertemplatesrc', 'hovertext', 'hovertextsrc', 'ids', 'idssrc', 'legendgroup', 'legendgrouptitle', 'legendrank', 'line', 'marker', 'meta', 'metasrc', 'mode', 'name', 'opacity', 'orientation', 'selected', 'selectedpoints', 'showlegend', 'stackgaps', 'stackgroup', 'stream', 'text', 'textfont', 'textposition', 'textpositionsrc', 'textsrc', 'texttemplate', 'texttemplatesrc', 'transforms', 'type', 'uid', 'uirevision', 'unselected', 'visible', 'x', 'x0', 'xaxis', 'xcalendar', 'xhoverformat', 'xperiod', 'xperiod0', 'xperiodalignment', 'xsrc', 'y', 'y0', 'yaxis', 'ycalendar', 'yhoverformat', 'yperiod', 'yperiod0', 'yperiodalignment', 'ysrc', 'key', 'set', 'frame', 'transforms', '_isNestedKey', '_isSimpleKey', '_isGraticule', '_bbox'"
  },
  {
    "objectID": "Introduction.html#time-series-analysis-and-forecasting-of-violent-crime-data-in-new-york-city",
    "href": "Introduction.html#time-series-analysis-and-forecasting-of-violent-crime-data-in-new-york-city",
    "title": "Introduction",
    "section": "Time series analysis and forecasting of violent crime data in New York City",
    "text": "Time series analysis and forecasting of violent crime data in New York City\n\nUnderstanding Crime Trends in NYC:\n\nAfter the outbreak of COVID-19 in 2021, violent crimes in New York City (NYC) have undergone significant fluctuations. These criminal activities threaten the safety and well-being of NYC’s residents and communities. one thing worth nothing that, the city has witnessed changes in various aspects of violent crimes, with some categories of violent crimes experiencing increases while others have decreased during the pandemic. For example, the number of murders and other serious violent crimes has raised significant concerns, while there has also been a decline in certain minor offenses. \nAnother perspective of criminal activities during COVID-19 pandemic era is that according to this article Boman and Gallupe (2020), the study has found that crime decreased compared with 2019. But this decrease in crime rates was mostly contributed to minor crimes that happened when people aren’t staying home. More serious crimes like murders, has stayed the same and even got worse."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "About Michael",
    "section": "",
    "text": "GUID: xx133\nEmail: xx133@georgetown.edu\nPhone: 571-220-3128"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "About Michael",
    "section": "Background",
    "text": "Background\n\nHello, I’m Xinzhe Xie, and I prefer to be called Michael. Originally from Chongqing, China, I graduated in 2021 with a degree in Statistics and Actuarial Science from the University of Massachusetts Amherst. After graduation, I embarked on a career as a data analyst in China, where my work primarily focused on developing river water quality forecast models.\nIn my role, I applied mechanical modeling methods using R, built interactive data visualizations using R Shiny, and deployed applications using Shiny Proxy and Docker. I also had the opportunity to work with neural networks and SARIMA time series modeling. My professional interests lie in the fields of machine learning and time series modeling, particularly within the realms of environmental and healthcare applications.\nBeyond my professional pursuits, I have a strong passion for weightlifting, specifically weighted calisthenics and powerlifting. I also have a background as a competitive swimmer, which has contributed to my discipline and determination."
  },
  {
    "objectID": "index.html#academic-journey",
    "href": "index.html#academic-journey",
    "title": "About Michael",
    "section": "Academic Journey",
    "text": "Academic Journey\n\nMy academic journey has been marked by a deep fascination with time series analysis and machine learning. During my previous job, I gained extensive experience in handling large time series datasets. I applied various models to these datasets to discern underlying trends and patterns. This experience motivated me to enroll in DSAN 5600 this semester, where I hope to further develop my expertise in time series analysis.\nI’m particularly enthusiastic about machine learning, especially deep learning methods such as neural network modeling. These methods have broad applications, including forecasting in domains like stocks, healthcare, and weather. I believe that incorporating deep learning techniques into time series modeling can significantly enhance predictive accuracy."
  },
  {
    "objectID": "index.html#professional-interest",
    "href": "index.html#professional-interest",
    "title": "About Michael",
    "section": "Professional interest",
    "text": "Professional interest\nMy professional interests encompass the following areas:\n\nAdvanced programming in R and Python\nTime series forecasting models, including ARIMA, SARIMA, Bayesian structural time series, and more\nDeep learning methods, particularly neural networks, for forecasting in diverse fields such as stocks, healthcare, and weather\nSoftware development, including building and deploying applications\n\n\nEducation\n\nMaster of Science(MS) in Data Science and Analytics, Georgetown University(2025)\nBachelor of Science (BS) in Mathematics, University of Massachusetts Amherst (2021)"
  },
  {
    "objectID": "Data_Visualization.html#use-the-crimedata-api-in-r-to-extract-crime-data-of-nyc-from-2007-to-2022.",
    "href": "Data_Visualization.html#use-the-crimedata-api-in-r-to-extract-crime-data-of-nyc-from-2007-to-2022.",
    "title": "Data Visualization",
    "section": "",
    "text": "Code\nlibrary(crimedata)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(DT)\nlibrary(RColorBrewer)\n\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\ndatatable(head(data))\n\n\n\n\n\n\n\n\n\nCode\n# Convert datetime to date by extracting just the date part\ndata$date_single &lt;- as.Date(data$date_single)\n\nviolent_crimes &lt;- c(\"rape (except statutory rape)\", \"personal robbery\", \"aggravated assault\", \n                    \"murder and nonnegligent manslaughter\", \"arson\", \"kidnapping/abduction\")\n\n# Filter the data to include only violent crimes\nviolent_crime &lt;- data[data$offense_type %in% violent_crimes, ]\n\n# Sum up the counts by date\nviolent_crime_total &lt;- violent_crime %&gt;%\n  group_by(date_single) %&gt;%\n  summarise(Count = n())\n\n\nviolent_crime_t &lt;- violent_crime %&gt;%\n  select(offense_type, date_single)\n\n# Sum up the counts by date for each category\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  group_by(date_single, offense_type) %&gt;%\n  summarise(Total_Crimes = n(), .groups = 'drop')\n\nviolent_crime_t &lt;- violent_crime_t %&gt;%\n  mutate(offense_type = case_when(\n    offense_type == \"kidnapping/abduction\" ~ \"kidnap\",\n    offense_type == \"rape (except statutory rape)\" ~ \"rape\",\n    offense_type == \"murder and nonnegligent manslaughter\" ~ \"murder\",\n    offense_type == \"personal robbery\" ~ \"robbery\",\n    TRUE ~ offense_type \n  ))\n\nviolent_crime_total$offense_type &lt;- \"total\"\ncolnames(violent_crime_total)[2] &lt;- \"Total_Crimes\"\n\nv_crime &lt;- rbind(violent_crime_t, violent_crime_total)\n\nv_crime &lt;- v_crime %&gt;%\n  arrange(date_single, offense_type)\n\n# Display the result\ndatatable(v_crime)\n\n\n\n\n\n\n\nCode\n# Save the data\n#write.csv(v_crime, file = \"dataset/crimedata_total.csv\", row.names = FALSE)"
  },
  {
    "objectID": "Data_Visualization.html#crime-data-map-of-violent-crimes-in-nyc-on-082021",
    "href": "Data_Visualization.html#crime-data-map-of-violent-crimes-in-nyc-on-082021",
    "title": "Data Visualization",
    "section": "Crime data map of violent crimes in NYC on 08/2021",
    "text": "Crime data map of violent crimes in NYC on 08/2021\n\nThis map visualization is generated using the geographical varible obtained from the crime data base.\n\n\n\nCode\ndata_test &lt;- data[data$offense_type %in% violent_crimes, ]\n\ndata_test$date &lt;- as.Date(data_test$date_single)\n\ndata_test &lt;- data_test %&gt;%\n  filter(date &gt;= \"2021-08-01\" & date &lt;= \"2021-09-01\")\n\n# Convert offense_type to a factor\ndata_test$offense_type &lt;- as.factor(data_test$offense_type)\n\n# Create a custom color scale for each offense type\ncolor_palette &lt;- brewer.pal(length(unique(data_test$offense_type)), \"Dark2\")\ncolor_scale &lt;- setNames(color_palette, levels(data_test$offense_type))\n\n# Map offense_type to corresponding color\ndata_test$color &lt;- color_scale[data_test$offense_type]\n\n# Create the plot with the custom color scale\nfig &lt;- plot_ly(\n  data = data_test,\n  type = \"scattermapbox\",\n  mode = \"markers\",\n  lon = ~longitude,\n  lat = ~latitude,\n  text = ~paste(\"Date: \", date, \"&lt;br&gt;Crime Type: \", offense_type),\n  marker = list(\n    size = 10,\n    color = ~color\n  )\n) %&gt;%\n  layout(\n    mapbox = list(\n      center = list(lon = -73.95, lat = 40.75),\n      zoom = 12,\n      style = \"open-street-map\"\n    )\n  )\n\nfig"
  },
  {
    "objectID": "Conclusions.html#overview",
    "href": "Conclusions.html#overview",
    "title": "Conclusions",
    "section": "Overview",
    "text": "Overview\n\n\n\nConclusion Diagram\n\n\n\nData Gethering\n\nNow, We have come to the conclusion part of this time series analysis on violent crimes in NYC. Step by step, we gathered comprehensive violent crimes data on NYC including major categories including Total violent crime, Aggravated Assault, and Robbery, also, gathered data on related external factors including COVID-19 total cases, unemployment rates and climate/weather data.\n\n\n\nEDA Process\n\nAfter that, multiple data visualizations were applied to the data, to have a better understanding of the trends, and also provide the geographical plots for a better view of crimes in NYC. Then we proceed to the exploratory data analysis (EDA), where we examine lag plots and check for ACF/PACF plots to prepare our insights for further statistical modeling parts.\n\n\n\nARIMA/SARIMA Models\n\nWe then proceeded to the statistical modeling parts where ARIMA/SARIMA models were employed for all three types of time series data, performing model diagnostics and obtaining forecasts for violent crime trends.\n\n\n\n\nSARIMA model forcast for total violent crimes\n\n\n\n\n\nSARIMA model forcast for total violent crimes\n\n\n\n\n\nSARIMA model forcast for total violent crimes\n\n\n\nWith these forecasts in hand, we’ve equipped ourselves with a powerful tool for investigating complex patterns of various types of violent crimes in New York City. Our analysis is not just statistical exploration, it offers invaluable insights into crimes evolution over time. This deep dive into the data not only unravels the crime dynamics but also paves the way for a more informed and proactive approach to crime management.\nBy providing timely and accurate forecast reports to the NYPD, the work takes on a practical and impactful dimension. These forecasts are more than just numbers and graphs; they are a crucial aid in the hands of law enforcement, empowering them with the foresight needed for swift and effective decision-making. In the bustling streets of New York City, where every moment counts, having access to such predictive insights can be a game-changer for the NYPD.\n\n\n\nARIMAX/VAR\n\nThe next step is to incorporate ARIMAX and VAR models into the crime data with external factors, by doing this, we acquired a more comprehensive modeling forecast to our statistical approach.\n\n\n\n\nARIMAX model forcast for total violent crimes\n\n\n\n\n\nVAR model forcast for total violent crimes\n\n\n\nImplementing ARIMAX and VAR models in our analysis significantly enhances the depth and breadth of our understanding of New York City’s violent crime patterns. These models bring a new level of sophistication to our study, allowing us to incorporate and measure the impact of external factors that might influence crime rates.\n\n\n\nDeep learning\n\nIn the final stage of our project, we applied sophisticated deep learning models like Recurrent Neural Networks (RNN), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM) networks to our dataset of total violent crimes in New York City.\nOur findings from this part of analsyis were illuminating. Among the deep learning models we tested, the GRU model emerged as the most accurate method with the relatively lower RMSE, surpassing both RNN and LSTM in accuracy and reliability.\nFurthermore, when we compared the results of these deep learning models with those obtained from the traditional ARIMA model, a clear distinction was evident. The deep learning models demonstrated a significantly lower RMSE. This lower RMSE from the deep learning models suggests a higher level of precision in predicting violent crime trends, underscoring the advanced capabilities of AI-driven approaches.\n\n\n\n\nARIMAX model forcast for total violent crimes"
  },
  {
    "objectID": "Conclusions.html#conclusion-and-limitations",
    "href": "Conclusions.html#conclusion-and-limitations",
    "title": "Conclusions",
    "section": "Conclusion and limitations",
    "text": "Conclusion and limitations\n\nStarting this project to investigate violent crime data in New York City has been an incredible experience. It’s shown us the complex and dynamic nature of crime.\nTo improve this project, adding more crimes categories and external factors is a cruial part, this will help us counter the forever-changing crimes. This is not all about collecting more data, it is about getting a better understanding of the criminal catrgoeis in the city of New York and finding patterns or connections that might have missed before.\nThe updates to this project won’t just involve accumulating more data or running more models. They will be about infusing fresh ideas and insights I will be learning afterwards into my work, ensuring that this analysis stays relevant and reflective of the city’s current state. It’s an ongoing process that requires adaptability, creativity, and a deep commitment to understanding the complexities of urban life."
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#overview-3",
    "href": "ARIMAX_SARIMAX_VAR.html#overview-3",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Overview",
    "text": "Overview\nIn this model, we focus on analyzing the weekly aggravated violent crimes in New York City using an ARIMAX model. The unprecedented global crisis brought on by the COVID-19 pandemic has had profound impacts on various facets of society, including crime patterns. Recognizing the potential influence of the pandemic on crime dynamics, our model integrates COVID-19 data as a critical exogenous component. This data is extracted using the covidcast package in R, which provides a comprehensive dataset of COVID-19 metrics.\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\nlibrary(covidcast)\nlibrary(zoo)\nlibrary(imputeTS)\nlibrary(vars)\n\n# Read the crime data from a CSV file into a dataframe\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\n\n# Filter the data to include only 'aggravated assault' type offenses\nassault &lt;- data %&gt;%\n  filter(offense_type == \"aggravated assault\")\n# Convert the date to a Date object for easier handling\nassault$date &lt;- as.Date(assault$date_single)\n\n# Aggregate data to count the number of assaults per date\nassault_day &lt;- assault %&gt;%\n  mutate(n = 1) %&gt;%\n  group_by(date) %&gt;%\n  summarize(crime_numbers = sum(n))\n\n# Create a new dataframe with the aggregated data\nassault &lt;- data.frame(\n  date = assault_day$date,\n  crimes = assault_day$crime_numbers\n)\n# Convert the daily crime data into a time series object with daily frequency\nassault_ts &lt;- ts(assault$crimes, start = c(2018, 1), frequency = 365)\n\n# Aggregate the daily data into weekly data\nassault_weekly &lt;- assault %&gt;%\n  mutate(week = lubridate::floor_date(date, \"week\")) %&gt;%  # Rounding down the date to the start of the week\n  group_by(week) %&gt;%\n  summarise(total_crimes = sum(crimes))\n# Convert the weekly aggregated crime data into a time series object with weekly frequency\nts_assault_weekly &lt;- ts(assault_weekly$total_crimes, frequency = 52, start = c(2018, 1))"
  },
  {
    "objectID": "ARIMAX_SARIMAX_VAR.html#overview-4",
    "href": "ARIMAX_SARIMAX_VAR.html#overview-4",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Overview",
    "text": "Overview\n\nIn this model, I employ a VAR model to analyze the interplay between monthly aggravated assault crimes in New York City and two critical variables: unemployment rates and weather data. The choice of a VAR model stems from its ability to capture the dynamic relationships between multiple time-series datasets. It is particularly suited to understand how changes in economic conditions (reflected by unemployment rates) and environmental factors (illustrated by weather data) collectively impact crime patterns.\nThe integration of unemployment rates is premised on the well-documented correlation between economic conditions and crime. Economic downturns, often signaled by rising unemployment, can influence crime rates through various socio-economic mechanisms. By including unemployment data, the model aims to quantify this relationship and examine how economic fluctuations correlate with changes in crime rates. The unemployment data is from NYUR DATA.\nWeather data, comprising variables such as temperature, humidity, and precipitation, is included based on preliminary exploratory data analysis findings. Our initial examination revealed a notable trend: hotter weather tends to correlate with higher crime rates. This finding aligns with existing criminological theories that suggest weather can significantly impact human behavior, including the propensity to commit crimes. By incorporating weather data, our model is positioned to explore this relationship further, examining how weather patterns influence crime rates over time. The weather data is from NOAA DATA.\n\n\n\nCode\n# Read the crime data from a CSV file into a dataframe\ndata &lt;- read_csv(\"./dataset/crimedata_NYC.csv\")\n\n# Filter the data to include only 'aggravated assault' type offenses\nassault &lt;- data %&gt;%\n  filter(offense_type == \"aggravated assault\")\n# Convert the date to a Date object for easier handling\nassault$date &lt;- as.Date(assault$date_single)\n\n# Aggregate data to count the number of assaults per day\n#assault_day &lt;- assault %&gt;%\n#  mutate(n = 1) %&gt;%\n#  group_by(date) %&gt;%\n#  summarize(crimes = sum(n))\n\n\n# Aggregate data to count the number of assaults per month\nassault_month &lt;- assault %&gt;%\n  mutate(n = 1) %&gt;%\n  mutate(month = format(date, \"%Y-%m\")) %&gt;%  # Extract year and month\n  group_by(month) %&gt;%\n  summarize(crimes = sum(n))\n\n# Add missing rows for 2012\nmissing_dates &lt;- seq(ymd(\"2012-01-01\"), ymd(\"2012-12-01\"), by=\"1 month\")\nmissing_data &lt;- data.frame(month = format(missing_dates, \"%Y-%m\"), crimes = NA)\n\n# Combine your existing data with the missing data\nassault_month &lt;- bind_rows(assault_month, missing_data) %&gt;%\n  arrange(month)\n\n# Perform moving average interpolation\nimputed_data &lt;- na_ma(assault_month$crimes, k = 12, weighting = \"exponential\")\nassault_month &lt;- data.frame(\n  month = assault_month$month,\n  crimes = imputed_data\n)\n\n# Loading unemployment rate and temperature data\ntemp &lt;- read_csv(\"./dataset/tempdataNYC.csv\")\nune &lt;- read_csv(\"./dataset/NYUR.csv\")\ncolnames(temp)[1] &lt;- \"month\"\ncolnames(une)[1] &lt;- \"month\"\ntemp &lt;- temp[-(1:3), ]\ntemp$month &lt;- paste0(substr(temp$month, 1, 4), \"-\", substr(temp$month, 5, 6))\nune$month &lt;- format(as.Date(une$month), \"%Y-%m\")\n\n# Filter all three data to the overlapping time period (2007-01 to 2022-12)\ntemp &lt;- temp %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\ncolnames(temp)[2] &lt;- \"temp\"\ntemp$temp&lt;- as.numeric(temp$temp)\n\nune &lt;- une %&gt;%\n  filter(month &gt;= \"2007-01\" & month &lt;= \"2022-12\")\n\nassault_month$month &lt;- as.Date(paste0(assault_month$month, \"-01\"), format=\"%Y-%m-%d\")\ntemp$month &lt;- as.Date(paste0(temp$month, \"-01\"), format=\"%Y-%m-%d\")\nune$month &lt;- as.Date(paste0(une$month, \"-01\"), format=\"%Y-%m-%d\")\n\n# Merging all 3 datasets by month\ncombined_data &lt;- merge(assault_month, temp, by = \"month\")\ncombined_data &lt;- merge(combined_data, une, by = \"month\")\n\n\nknitr::kable(head(combined_data))\n\n\n\n\n\nmonth\ncrimes\ntemp\nNYUR\n\n\n\n\n2007-01-01\n1207\n37.7\n4.2\n\n\n2007-02-01\n1037\n28.5\n4.2\n\n\n2007-03-01\n1366\n42.5\n4.2\n\n\n2007-04-01\n1334\n50.6\n4.3\n\n\n2007-05-01\n1665\n65.5\n4.3\n\n\n2007-06-01\n1624\n71.6\n4.4\n\n\n\n\n\nCode\n# Convert to ts\ncombined.ts&lt;-ts(combined_data,start=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n\n\nData Visualization\n\n\nCode\nggplot(assault_month, aes(x = month, y = crimes)) +\n  geom_line() +\n  labs(title = \"Monthly Assault Crimes\", x = \"Month\", y = \"Number of Crimes\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(temp, aes(x = month, y = temp)) +\n  geom_line() +\n  labs(title = \"Monthly Temperature\", x = \"Month\", y = \"Temperature\") +\n  theme_minimal()\n\n\n\n\n\nCode\nggplot(une, aes(x = month, y = NYUR)) +\n  geom_line() +\n  labs(title = \"Monthly Unemployment Rate\", x = \"Month\", y = \"Unemployment Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFitting Model With VARselect()\n\n\nCode\nVARselect(combined_data[, c(2:4)], lag.max=12, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     6      5      5      6 \n\n$criteria\n                  1            2            3            4            5\nAIC(n)     13.59905     12.52283     12.22540     11.94878     11.75518\nHQ(n)      13.70693     12.69544     12.46274     12.25085     12.12199\nSC(n)      13.86513     12.94856     12.81077     12.69380     12.65985\nFPE(n) 805395.32709 274582.03486 203995.77127 154774.39374 127628.08130\n                  6            7            8            9           10\nAIC(n)     11.74172     11.81653     11.87854     11.95327     11.91367\nHQ(n)      12.17325     12.31280     12.43954     12.57900     12.60412\nSC(n)      12.80603     13.04050     13.26216     13.49653     13.61658\nFPE(n) 126055.76717 136045.44180 145022.80109 156649.60926 151012.90362\n                 11           12\nAIC(n)     11.87357     11.78282\nHQ(n)      12.62876     12.60274\nSC(n)      13.73613     13.80503\nFPE(n) 145600.38674 133540.55769\n\n\n\nBased on the results, we have VAR(5) and VAR(6).\n\n\n\nVAR Model Diagnostics\n\nVAR(5)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, temp, NYUR \nDeterministic variables: both \nSample size: 187 \nLog Likelihood: -1836.982 \nRoots of the characteristic polynomial:\n0.9941 0.9941 0.917 0.917 0.7833 0.7833 0.7684 0.7684 0.7154 0.7154 0.6758 0.577 0.577 0.5558 0.5558\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1   0.39580    0.08050   4.917 2.06e-06 ***\ntemp.l1     0.32342    2.22009   0.146  0.88435    \nNYUR.l1     3.19158   10.36868   0.308  0.75860    \ncrimes.l2   0.41667    0.08595   4.848 2.80e-06 ***\ntemp.l2    -4.25760    2.50284  -1.701  0.09075 .  \nNYUR.l2    20.78858   15.23164   1.365  0.17411    \ncrimes.l3  -0.20147    0.09146  -2.203  0.02896 *  \ntemp.l3     4.62587    2.54366   1.819  0.07073 .  \nNYUR.l3   -20.17243   15.38359  -1.311  0.19153    \ncrimes.l4  -0.02354    0.08514  -0.276  0.78252    \ntemp.l4    -2.70790    2.47404  -1.095  0.27527    \nNYUR.l4     8.91772   15.21088   0.586  0.55847    \ncrimes.l5   0.26050    0.07810   3.335  0.00105 ** \ntemp.l5    -8.03288    2.22387  -3.612  0.00040 ***\nNYUR.l5    -6.11807   10.12943  -0.604  0.54666    \nconst     707.99111  127.66747   5.546 1.10e-07 ***\ntrend       0.73872    0.29418   2.511  0.01297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 109.2 on 170 degrees of freedom\nMultiple R-Squared: 0.8631, Adjusted R-squared: 0.8502 \nF-statistic: 66.97 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1  0.004908   0.002638   1.860  0.06459 .  \ntemp.l1    0.532448   0.072764   7.317 9.57e-12 ***\nNYUR.l1   -0.044589   0.339835  -0.131  0.89577    \ncrimes.l2 -0.003698   0.002817  -1.313  0.19103    \ntemp.l2    0.115794   0.082031   1.412  0.15990    \nNYUR.l2    0.511356   0.499220   1.024  0.30714    \ncrimes.l3 -0.004246   0.002998  -1.416  0.15852    \ntemp.l3    0.053238   0.083369   0.639  0.52396    \nNYUR.l3   -0.278591   0.504200  -0.553  0.58130    \ncrimes.l4  0.005518   0.002790   1.978  0.04958 *  \ntemp.l4   -0.246546   0.081087  -3.041  0.00274 ** \nNYUR.l4    0.192930   0.498539   0.387  0.69925    \ncrimes.l5 -0.001763   0.002560  -0.689  0.49200    \ntemp.l5   -0.374941   0.072888  -5.144 7.35e-07 ***\nNYUR.l5   -0.290569   0.331994  -0.875  0.38269    \nconst     49.559462   4.184324  11.844  &lt; 2e-16 ***\ntrend      0.003865   0.009642   0.401  0.68902    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.58 on 170 degrees of freedom\nMultiple R-Squared: 0.9525, Adjusted R-squared: 0.948 \nF-statistic: 212.8 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1 -0.0005777  0.0006425  -0.899  0.36984    \ntemp.l1    0.0037121  0.0177188   0.210  0.83431    \nNYUR.l1    1.0672496  0.0827532  12.897  &lt; 2e-16 ***\ncrimes.l2 -0.0007765  0.0006860  -1.132  0.25922    \ntemp.l2   -0.0041825  0.0199754  -0.209  0.83440    \nNYUR.l2   -0.3703550  0.1215650  -3.047  0.00268 ** \ncrimes.l3  0.0007349  0.0007300   1.007  0.31550    \ntemp.l3    0.0098473  0.0203012   0.485  0.62826    \nNYUR.l3    0.2819647  0.1227777   2.297  0.02286 *  \ncrimes.l4 -0.0003267  0.0006795  -0.481  0.63131    \ntemp.l4   -0.0069616  0.0197455  -0.353  0.72485    \nNYUR.l4   -0.1671504  0.1213993  -1.377  0.17036    \ncrimes.l5  0.0000471  0.0006233   0.076  0.93986    \ntemp.l5   -0.0047919  0.0177489  -0.270  0.78750    \nNYUR.l5    0.0920441  0.0808438   1.139  0.25650    \nconst      2.1165340  1.0189244   2.077  0.03928 *  \ntrend      0.0012321  0.0023479   0.525  0.60044    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8717 on 170 degrees of freedom\nMultiple R-Squared: 0.8612, Adjusted R-squared: 0.8482 \nF-statistic: 65.94 on 16 and 170 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         crimes    temp     NYUR\ncrimes 11930.07 90.3969 -38.9482\ntemp      90.40 12.8154  -0.5043\nNYUR     -38.95 -0.5043   0.7599\n\nCorrelation matrix of residuals:\n        crimes    temp    NYUR\ncrimes  1.0000  0.2312 -0.4091\ntemp    0.2312  1.0000 -0.1616\nNYUR   -0.4091 -0.1616  1.0000\n\n\n\nThe first and second lags of assaulted crimes are significant predictors, while the third and fifth lags also contribute meaningfully.\nTemperature and unemployment rate in some lags show significance, with the fifth lag of temperature being a strong predictor.\nR squared of the model is 0.861 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR5 &lt;- VAR(combined_data[,c(2:4)], p=5, type=\"both\")\nserial.test(VAR5, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR5\nChi-squared = 86.41, df = 63, p-value = 0.02682\n\n\nCode\nacf(residuals(VAR5))\n\n\n\n\n\n\nThe residuals are normally distributed.\n\n\n\nVAR(6)\n\n\nCode\nsummary(vars::VAR(combined_data[, c(2:4)], p=6, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: crimes, temp, NYUR \nDeterministic variables: both \nSample size: 186 \nLog Likelihood: -1816.59 \nRoots of the characteristic polynomial:\n0.9969 0.9969 0.9163 0.9163 0.7962 0.7962 0.7329 0.7329 0.7172 0.7172 0.6865 0.6865 0.5982 0.5982 0.4988 0.4988 0.3081 0.3081\nCall:\nvars::VAR(y = combined_data[, c(2:4)], p = 6, type = \"both\")\n\n\nEstimation results for equation crimes: \n======================================= \ncrimes = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1   0.372958   0.085199   4.378 2.12e-05 ***\ntemp.l1    -1.544602   2.401473  -0.643  0.52099    \nNYUR.l1    -0.632201  10.513349  -0.060  0.95212    \ncrimes.l2   0.419482   0.086201   4.866 2.63e-06 ***\ntemp.l2    -5.089091   2.556283  -1.991  0.04814 *  \nNYUR.l2    22.740606  15.321569   1.484  0.13965    \ncrimes.l3  -0.182148   0.093164  -1.955  0.05225 .  \ntemp.l3     4.530163   2.549612   1.777  0.07743 .  \nNYUR.l3   -19.618072  15.587061  -1.259  0.20994    \ncrimes.l4  -0.030638   0.092913  -0.330  0.74200    \ntemp.l4    -1.934581   2.571100  -0.752  0.45286    \nNYUR.l4    12.290395  15.570026   0.789  0.43103    \ncrimes.l5   0.240700   0.085939   2.801  0.00570 ** \ntemp.l5    -5.703485   2.539730  -2.246  0.02604 *  \nNYUR.l5   -17.020459  15.231031  -1.117  0.26540    \ncrimes.l6  -0.002773   0.083324  -0.033  0.97349    \ntemp.l6    -4.519274   2.451896  -1.843  0.06709 .  \nNYUR.l6     9.422082  10.137986   0.929  0.35404    \nconst     979.420787 182.960937   5.353 2.84e-07 ***\ntrend       0.854254   0.302051   2.828  0.00526 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 108.7 on 166 degrees of freedom\nMultiple R-Squared: 0.8675, Adjusted R-squared: 0.8523 \nF-statistic:  57.2 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation temp: \n===================================== \ntemp = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1  0.004249   0.002685   1.582  0.11548    \ntemp.l1    0.416667   0.075684   5.505 1.38e-07 ***\nNYUR.l1   -0.230559   0.331335  -0.696  0.48750    \ncrimes.l2 -0.003918   0.002717  -1.442  0.15112    \ntemp.l2    0.066620   0.080563   0.827  0.40946    \nNYUR.l2    0.546802   0.482869   1.132  0.25910    \ncrimes.l3 -0.003238   0.002936  -1.103  0.27165    \ntemp.l3    0.050881   0.080353   0.633  0.52746    \nNYUR.l3   -0.175215   0.491236  -0.357  0.72178    \ncrimes.l4  0.005472   0.002928   1.869  0.06342 .  \ntemp.l4   -0.221615   0.081030  -2.735  0.00692 ** \nNYUR.l4    0.321108   0.490699   0.654  0.51377    \ncrimes.l5 -0.002002   0.002708  -0.739  0.46093    \ntemp.l5   -0.217677   0.080041  -2.720  0.00723 ** \nNYUR.l5   -0.742252   0.480016  -1.546  0.12393    \ncrimes.l6 -0.001812   0.002626  -0.690  0.49113    \ntemp.l6   -0.267250   0.077273  -3.459  0.00069 ***\nNYUR.l6    0.370635   0.319505   1.160  0.24770    \nconst     66.334113   5.766130  11.504  &lt; 2e-16 ***\ntrend      0.010358   0.009519   1.088  0.27815    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 3.427 on 166 degrees of freedom\nMultiple R-Squared: 0.9572, Adjusted R-squared: 0.9524 \nF-statistic: 195.6 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation NYUR: \n===================================== \nNYUR = crimes.l1 + temp.l1 + NYUR.l1 + crimes.l2 + temp.l2 + NYUR.l2 + crimes.l3 + temp.l3 + NYUR.l3 + crimes.l4 + temp.l4 + NYUR.l4 + crimes.l5 + temp.l5 + NYUR.l5 + crimes.l6 + temp.l6 + NYUR.l6 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \ncrimes.l1 -0.0003366  0.0006856  -0.491  0.62410    \ntemp.l1    0.0146802  0.0193250   0.760  0.44854    \nNYUR.l1    1.0910853  0.0846024  12.897  &lt; 2e-16 ***\ncrimes.l2 -0.0008230  0.0006937  -1.186  0.23712    \ntemp.l2    0.0014627  0.0205708   0.071  0.94340    \nNYUR.l2   -0.3811467  0.1232949  -3.091  0.00234 ** \ncrimes.l3  0.0006020  0.0007497   0.803  0.42315    \ntemp.l3    0.0107992  0.0205171   0.526  0.59935    \nNYUR.l3    0.2699885  0.1254313   2.152  0.03280 *  \ncrimes.l4 -0.0001659  0.0007477  -0.222  0.82469    \ntemp.l4   -0.0140609  0.0206900  -0.680  0.49770    \nNYUR.l4   -0.1716637  0.1252942  -1.370  0.17251    \ncrimes.l5  0.0001281  0.0006916   0.185  0.85322    \ntemp.l5   -0.0175707  0.0204376  -0.860  0.39118    \nNYUR.l5    0.1048116  0.1225663   0.855  0.39371    \ncrimes.l6 -0.0002326  0.0006705  -0.347  0.72913    \ntemp.l6    0.0303244  0.0197308   1.537  0.12622    \nNYUR.l6   -0.0135368  0.0815818  -0.166  0.86841    \nconst      0.4973990  1.4723127   0.338  0.73591    \ntrend      0.0007627  0.0024306   0.314  0.75408    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.875 on 166 degrees of freedom\nMultiple R-Squared: 0.8629, Adjusted R-squared: 0.8472 \nF-statistic: 54.98 on 19 and 166 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         crimes    temp     NYUR\ncrimes 11821.94 70.1280 -37.8481\ntemp      70.13 11.7420  -0.4049\nNYUR     -37.85 -0.4049   0.7655\n\nCorrelation matrix of residuals:\n        crimes    temp    NYUR\ncrimes  1.0000  0.1882 -0.3978\ntemp    0.1882  1.0000 -0.1351\nNYUR   -0.3978 -0.1351  1.0000\n\n\n\nNone of the assaulted crimes are significant predictors.\nTemperature and unemployment rate in some lags show significance, with the second lag of unemploymnet rate being a strong predictor.\nR squared of the model is 0.8675 indicating a strong fit.\nResiduals\n\n\n\nCode\nVAR6 &lt;- VAR(combined_data[,2:4], p=6, type=\"both\")\nserial.test(VAR6, lags.pt=12, type=\"PT.asymptotic\")\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object VAR6\nChi-squared = 77.916, df = 54, p-value = 0.01826\n\n\nCode\nacf(residuals(VAR6))\n\n\n\n\n\n\nResiduals are normally distributed.\n\n\n\n\nCross Validation\n\n\nCode\nn=length(combined_data$crimes)\nk=24\n\n#n-k=164; 164/4=41;\n\nrmse1 &lt;- matrix(NA, 168,3)\nrmse2 &lt;- matrix(NA, 168,3)\nrmse3 &lt;- matrix(NA,14,12)\nyear&lt;-c()\n\n# Convert data frame to time series object\nts_obj &lt;- ts(combined_data[, c(2:4)], star=decimal_date(as.Date(\"2007-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\nst &lt;- tsp(ts_obj )[1]+(k-1)/12 \n\n\nfor(i in 1:14)\n{\n  \n  xtrain &lt;- window(ts_obj, end=st + i-1)\n  xtest &lt;- window(ts_obj, start=st + (i-1) + 1/12, end=st + i)\n  \n  ######## first Model ############\n  fit &lt;- VAR(ts_obj, p=5, type='both')\n  fcast &lt;- predict(fit, n.ahead = 12)\n  \n  fc&lt;-fcast$fcst$crimes\n  ft&lt;-fcast$fcst$temp\n  fu&lt;-fcast$fcst$NYUR\n  \n  ff&lt;-data.frame(fc[,1],ft[,1],fu[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff&lt;-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11 #going from 1,5, 9,13\n  b= 12*i #4, 8, 12\n  \n  ## it's going from 1978 Q1(1)-Q4(4); 1979 Q1(5)-Q4(8); 1980 Q1(9)-Q4(12)....\n  \n  ##### collecting errors ######\n  rmse1[c(a:b),]  &lt;-sqrt((ff-xtest)^2)\n  \n  \n  ######## Second Model ############\n  fit2 &lt;- vars::VAR(ts_obj, p=6, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 12)\n  \n  fc&lt;-fcast2$fcst$crimes\n  ft&lt;-fcast2$fcst$temp\n  fu&lt;-fcast2$fcst$NYUR\n  \n  ff2&lt;-data.frame(fc[,1],ft[,1],fu[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12\n  \n  ff2&lt;-ts(ff2,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i\n  rmse2[c(a:b),]  &lt;-sqrt((ff2-xtest)^2)\n}\n\nyr = rep(c(2009:2022),each =12) #year\nmo = rep(paste0(\"M\",1:12),14) #quarter\n\nrmse1 = data.frame(yr,mo,rmse1)\nnames(rmse1) =c(\"Year\", \"Month\",\"Crimes\",\"Temperature\",\"Unemployment\")\nrmse2 = data.frame(yr,mo,rmse2)\nnames(rmse2) =c(\"Year\", \"Month\",\"Crimes\",\"Temperature\",\"Unemployment\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Crimes),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Crimes),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Assault Crimes\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Temperature),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Temperature),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Temperature\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Unemployment),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Unemployment),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Unemployment rate\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\nThe blue fit which is the VAR(5) model, consistently has a lower RMSE compared with VAR(6) which is the red line, VAR(6) is a better fit.\n\n\n\nForecasts\n\nFrom the cross validation from last part, we have concluded that VAR(5) is a better model, here we are going to forecast using VAR(5)\n\n\n\nCode\nforecast(fit)\n\n\ncrimes\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2023       1860.974 1720.997 2000.951 1646.897 2075.051\nFeb 2023       1913.238 1763.062 2063.414 1683.564 2142.912\nMar 2023       1982.215 1815.624 2148.806 1727.436 2236.995\nApr 2023       2073.193 1905.060 2241.325 1816.056 2330.329\nMay 2023       2204.415 2033.797 2375.033 1943.477 2465.352\nJun 2023       2280.025 2102.987 2457.062 2009.270 2550.780\nJul 2023       2314.938 2132.891 2496.986 2036.520 2593.356\nAug 2023       2283.671 2094.032 2473.310 1993.643 2573.698\nSep 2023       2209.151 2015.958 2402.344 1913.687 2504.614\nOct 2023       2097.829 1902.323 2293.336 1798.828 2396.830\n\ntemp\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2023       34.12223 29.53445 38.71001 27.10582 41.13863\nFeb 2023       37.27250 31.94967 42.59532 29.13193 45.41306\nMar 2023       44.27527 38.64164 49.90890 35.65938 52.89116\nApr 2023       53.66847 47.84361 59.49333 44.76011 62.57682\nMay 2023       65.07287 59.21638 70.92937 56.11614 74.02961\nJun 2023       73.93957 67.65635 80.22278 64.33022 83.54891\nJul 2023       77.42159 70.57498 84.26820 66.95060 87.89257\nAug 2023       75.67472 68.33514 83.01430 64.44981 86.89963\nSep 2023       69.21693 61.52088 76.91297 57.44684 80.98702\nOct 2023       59.05856 51.25563 66.86148 47.12501 70.99211\n\nNYUR\n         Point Forecast    Lo 80    Hi 80       Lo 95    Hi 95\nJan 2023       3.977834 2.860664 5.095004  2.26926943 5.686398\nFeb 2023       3.956187 2.298432 5.613943  1.42086862 6.491506\nMar 2023       4.024649 2.104573 5.944725  1.08814641 6.961152\nApr 2023       4.031821 1.927150 6.136491  0.81300473 7.250636\nMay 2023       4.111567 1.881941 6.341192  0.70164863 7.521485\nJun 2023       4.161144 1.843274 6.479013  0.61626852 7.706019\nJul 2023       4.089418 1.692760 6.486076  0.42404521 7.754791\nAug 2023       4.003606 1.539134 6.468078  0.23452178 7.772691\nSep 2023       3.890928 1.371228 6.410629  0.03737908 7.744477\nOct 2023       3.758231 1.190277 6.326186 -0.16911594 7.685579\n\n\nCode\nforecast(fit) %&gt;% autoplot() + xlab(\"Year\")\n\n\n\n\n\n\n\nEvaluations\nthe historical data shows fluctuations with no clear long-term trend. The forecast suggests a stable outlook with a slight uptick towards the end, within a moderate confidence interval."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA_Models.html#rualts-from-eda-process",
    "href": "ARMA_ARIMA_SARIMA_Models.html#rualts-from-eda-process",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "During the EDA process, I already differenced the data and used ADF test to check for stationarity.\n\n\n\nCode\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(TTR)\nlibrary(astsa)\n\n# Load the clean violent crime data\ndata &lt;- read_csv(\"./dataset/crimedata_total.csv\")\ndata_w &lt;- read_csv(\"./dataset/crimedata_week.csv\")\n\ndata_t &lt;- data %&gt;%\n  filter(offense_type == \"total\")\n\nv_ts &lt;- ts(data_t$Total_Crimes, start = c(2007, 1), frequency = 365.25)\nw_ts &lt;- ts(data_w$Total_Crimes, frequency = 52, start = c(2007, 1))"
  }
]