{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Deep Learning for TS\"\n",
        "format:\n",
        "  html:\n",
        "    page-layout: full\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "    code-overflow: wrap\n",
        "bibliography: references.bib\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Traditional statistical methods, such as `ARIMA` and `SARIMA` models, have been extensively used for time series forecasting. However, the complex and often nonlinear nature of crime data makes it challenging for these traditional models to capture the underlying patterns accurately. This is where deep learning models come into play, offering a robust alternative with their ability to learn complex patterns from large datasets. In this study, we focus on employing deep learning techniques to analyze and predict violent crime time series data in NYC. Specifically, we explore three different types of neural network architectures: Recurrent Neural Networks `(RNN)`, Gated Recurrent Units `(GRU)`, and Long Short-Term Memory `(LSTM)` networks.\n",
        "\n",
        "**Recurrent Neural Networks (RNN):** `RNNs` are a class of neural networks that are particularly well-suited for sequential data, like time series. They can capture temporal dynamics, which is essential for understanding how crime trends evolve over time.\n",
        "\n",
        "**Gated Recurrent Units (GRU):** `GRUs` are a variation of RNNs that aim to solve the vanishing gradient problem often encountered in traditional RNNs. They are designed to retain information over extended periods, making them suitable for analyzing time series data where long-term dependencies are crucial.\n",
        "\n",
        "**Long Short-Term Memory (LSTM):** `LSTMs` are another advanced variant of `RNNs`, known for their effectiveness in learning long-term dependencies. They are particularly adept at handling the complexity and variability inherent in crime data, making them a promising tool for predicting violent crime trends.\n"
      ],
      "id": "2883219a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,mean_absolute_error\n",
        "from math import sqrt"
      ],
      "id": "ff450e7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n",
        "\n",
        "For the deep learning model, we will be using the total violent crimes data we used in ARIMA/SARIMA model section.\n",
        "\n",
        "### Visualization of raw data"
      ],
      "id": "3cead3b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"./dataset/crimedata_week.csv\")\n",
        "df.drop('offense_type', axis = 1)\n",
        "print(df.head())\n",
        "\n",
        "df = df.rename(columns={\"week\": \"t\", \"Total_Crimes\": \"y\"})\n",
        "df = df[[\"t\",\"y\"]]\n",
        "\n",
        "t=np.array([*range(0,df.shape[0])])\n",
        "x=np.array(df['y']).reshape(t.shape[0],1)\n",
        "feature_columns=[0] # columns to use as features\n",
        "target_columns=[0]  # columns to use as targets\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(0,x.shape[1]):\n",
        "    ax.plot(t, x[:,i],'o',alpha = 0.5)\n",
        "    ax.plot(t, x[:,i],\"-\")\n",
        "ax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \n",
        "plt.show()"
      ],
      "id": "d1950267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization of normalized data"
      ],
      "id": "04c87da7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalize the data\n",
        "x=(x-np.mean(x,axis=0))/np.std(x,axis=0)\n",
        "\n",
        "# visualize normalized data \n",
        "fig, ax = plt.subplots()\n",
        "for i in range(0,x.shape[1]):\n",
        "    ax.plot(t, x[:,i],'o')\n",
        "    ax.plot(t, x[:,i],\"-\")\n",
        "ax.plot(t, 0*x[:,0],\"-\") # add baseline for reference \n",
        "plt.show()"
      ],
      "id": "8d6769f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split data into train/test sets\n"
      ],
      "id": "f7f46dc6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "split_fraction=0.8\n",
        "cut=int(split_fraction*x.shape[0]) \n",
        "tt=t[0:cut]; xt=x[0:cut]\n",
        "tv=t[cut:]; xv=x[cut:]\n",
        "\n",
        "# visualize normalized data \n",
        "fig, ax = plt.subplots()\n",
        "for i in range(0,x.shape[1]):\n",
        "    ax.plot(tt, xt[:,i],'ro',alpha=0.25)\n",
        "    ax.plot(tt, xt[:,i],\"g-\")\n",
        "for i in range(0,x.shape[1]):\n",
        "    ax.plot(tv, xv[:,i],'bo',alpha=0.25)\n",
        "    ax.plot(tv, xv[:,i],\"g-\")\n",
        "plt.show()"
      ],
      "id": "c5939814",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Form mini-batches for training/testing data"
      ],
      "id": "c70b2797"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time series mini-batch function\n",
        "def form_arrays(x,lookback=3,delay=1,step=1,feature_columns=[0],target_columns=[0],unique=False,verbose=False):\n",
        "    # verbose=True --> report and plot for debugging\n",
        "    # unique=True --> don't re-sample: \n",
        "    # x1,x2,x3 --> x4 then x4,x5,x6 --> x7 instead of x2,x3,x4 --> x5\n",
        "\n",
        "    # initialize \n",
        "    i_start=0; count=0; \n",
        "    \n",
        "    # initialize output arrays with samples \n",
        "    x_out=[]\n",
        "    y_out=[]\n",
        "    \n",
        "    # sequentially build mini-batch samples\n",
        "    while i_start+lookback+delay< x.shape[0]:\n",
        "        \n",
        "        # define index bounds\n",
        "        i_stop=i_start+lookback\n",
        "        i_pred=i_stop+delay\n",
        "        \n",
        "        # report if desired \n",
        "        if verbose and count<2: print(\"indice range:\",i_start,i_stop,\"-->\",i_pred)\n",
        "\n",
        "        # define arrays: \n",
        "        # method-1: buggy due to indexing from left \n",
        "        # numpy's slicing --> start:stop:step\n",
        "        # xtmp=x[i_start:i_stop+1:steps]\n",
        "        \n",
        "        # method-2: non-vectorized but cleaner\n",
        "        indices_to_keep=[]; j=i_stop\n",
        "        while  j>=i_start:\n",
        "            indices_to_keep.append(j)\n",
        "            j=j-step\n",
        "\n",
        "        # create mini-batch sample\n",
        "        xtmp=x[indices_to_keep,:]    # isolate relevant indices\n",
        "        xtmp=xtmp[:,feature_columns] # isolate desire features\n",
        "        ytmp=x[i_pred,target_columns]\n",
        "        x_out.append(xtmp); y_out.append(ytmp); \n",
        "        \n",
        "        # report if desired \n",
        "        if verbose and count<2: print(xtmp, \"-->\",ytmp)\n",
        "        if verbose and count<2: print(\"shape:\",xtmp.shape, \"-->\",ytmp.shape)\n",
        "\n",
        "        # PLOT FIRST SAMPLE IF DESIRED FOR DEBUGGING    \n",
        "        if verbose and count<2:\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.plot(x,'b-')\n",
        "            ax.plot(x,'bx')\n",
        "            ax.plot(indices_to_keep,xtmp,'go')\n",
        "            ax.plot(i_pred*np.ones(len(target_columns)),ytmp,'ro')\n",
        "            plt.show()\n",
        "            \n",
        "        # UPDATE START POINT \n",
        "        if unique: i_start+=lookback \n",
        "        i_start+=1; count+=1\n",
        "        \n",
        "    return np.array(x_out),np.array(y_out)\n",
        "\n",
        "# training\n",
        "L=25; S=1; D=1\n",
        "Xt,Yt=form_arrays(xt,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)\n",
        "\n",
        "# validation\n",
        "Xv,Yv=form_arrays(xv,lookback=L,delay=D,step=S,feature_columns=feature_columns,target_columns=target_columns,unique=False,verbose=True)"
      ],
      "id": "36901706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the plot function and utility function for accuracy metrics\n"
      ],
      "id": "be51b171"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def history_plot(history):\n",
        "    FS=18   #FONT SIZE\n",
        "    # PLOTTING THE TRAINING AND VALIDATION LOSS \n",
        "    history_dict = history.history\n",
        "    loss_values = history_dict[\"loss\"]\n",
        "    val_loss_values = history_dict[\"val_loss\"]\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "    plt.title(\"Training and validation loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# UTILITY FUNCTION\n",
        "def regression_report(yt,ytp,yv,yvp):\n",
        "    print(\"---------- Regression report ----------\")\n",
        "    \n",
        "    print(\"TRAINING:\")\n",
        "    print(\" MSE:\",mean_squared_error(yt,ytp))\n",
        "    print(\" MAE:\",mean_absolute_error(yt,ytp))\n",
        "    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n",
        "    \n",
        "    # PARITY PLOT\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(yt,ytp,'ro')\n",
        "    ax.plot(yt,yt,'b-')\n",
        "    ax.set(xlabel='y_data', ylabel='y_predicted',\n",
        "        title='Training data parity plot (line y=x represents a perfect fit)')\n",
        "    plt.show()\n",
        "    \n",
        "    # PLOT PART OF THE PREDICTED TIME-SERIES\n",
        "    frac_plot=1.0\n",
        "    upper=int(frac_plot*yt.shape[0]); \n",
        "    # print(int(0.5*yt.shape[0]))\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(yt[0:upper],'b-')\n",
        "    ax.plot(ytp[0:upper],'r-',alpha=0.5)\n",
        "    ax.plot(ytp[0:upper],'ro',alpha=0.25)\n",
        "    ax.set(xlabel='index', ylabel='y(t (blue=actual & red=prediction)', title='Training: Time-series prediction')\n",
        "    plt.show()\n",
        "\n",
        "      \n",
        "    print(\"VALIDATION:\")\n",
        "    print(\" MSE:\",mean_squared_error(yv,yvp))\n",
        "    print(\" MAE:\",mean_absolute_error(yv,yvp))\n",
        "    # print(\" MAPE:\",mean_absolute_percentage_error(Yt,Ytp))\n",
        "    \n",
        "    # PARITY PLOT \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(yv,yvp,'ro')\n",
        "    ax.plot(yv,yv,'b-')\n",
        "    ax.set(xlabel='y_data', ylabel='y_predicted',\n",
        "        title='Validation data parity plot (line y=x represents a perfect fit)')\n",
        "    plt.show()\n",
        "    \n",
        "    # PLOT PART OF THE PREDICTED TIME-SERIES\n",
        "    upper=int(frac_plot*yv.shape[0])\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(yv[0:upper],'b-')\n",
        "    ax.plot(yvp[0:upper],'r-',alpha=0.5)\n",
        "    ax.plot(yvp[0:upper],'ro',alpha=0.25)\n",
        "    ax.set(xlabel='index', ylabel='y(t) (blue=actual & red=prediction)', title='Validation: Time-series prediction')\n",
        "    plt.show()"
      ],
      "id": "3fd88a02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "::: {.panel-tabset}\n",
        "\n",
        "# RNN\n",
        "\n",
        "## Without regularization"
      ],
      "id": "93669a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RESHAPE INTO A DATA FRAME \n",
        "Xt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\n",
        "Xv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n",
        "\n",
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "#model.add(LSTM(\n",
        "#model.add(GRU(\n",
        "model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp)  \n",
        "RNN_e = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "2ca1cc51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With regularization\n"
      ],
      "id": "84335e7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "#model.add(LSTM(\n",
        "#model.add(GRU(\n",
        "model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "recurrent_regularizer=regularizers.L2(L2),\n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp) \n",
        "\n",
        "RNN_e_r = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "52ea5e9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- According to the model results, `RNN` model with regularization has larger `MSE` and `MAE` values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data \"too well\", and enhance generalization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GRUS\n",
        "\n",
        "## Without regularization\n"
      ],
      "id": "416a46f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RESHAPE INTO A DATA FRAME \n",
        "Xt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\n",
        "Xv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n",
        "\n",
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "#model.add(LSTM(\n",
        "model.add(GRU(\n",
        "#model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp)  \n",
        "GRU_e = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "7c9c5e3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With regularization\n"
      ],
      "id": "a1a2e3ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "#model.add(LSTM(\n",
        "model.add(GRU(\n",
        "#model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "recurrent_regularizer=regularizers.L2(L2),\n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp) \n",
        "GRU_e_r = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "8a5f82d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- According to the model results, `GRU` model with regularization has larger `MSE` and `MAE` values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data \"too well\", and enhance generalization.\n",
        "\n",
        "\n",
        "# LSTM\n",
        "\n",
        "## Without regularization\n"
      ],
      "id": "9cdeaf60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RESHAPE INTO A DATA FRAME \n",
        "Xt1=Xt.reshape(Xt.shape[0],Xt.shape[1]*Xt.shape[2])\n",
        "Xv1=Xv.reshape(Xv.shape[0],Xv.shape[1]*Xv.shape[2])\n",
        "\n",
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "model.add(LSTM(\n",
        "#model.add(GRU(\n",
        "#model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp)  \n",
        "LSTM_e = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "0a6291e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With regularization\n"
      ],
      "id": "7bf5c580"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# # HYPERPARAMETERS \n",
        "optimizer=\"rmsprop\"\n",
        "loss_function=\"MeanSquaredError\" \n",
        "learning_rate=0.001\n",
        "numbers_epochs=200 #100\n",
        "L2=0 #1e-4\n",
        "input_shape=(Xt.shape[1],Xt.shape[2])\n",
        "\n",
        "# # batch_size=1                       # stocastic training\n",
        "# # batch_size=int(len(x_train)/2.)    # mini-batch training\n",
        "batch_size=len(Xt1)              # batch training\n",
        "\n",
        "# BUILD MODEL\n",
        "recurrent_hidden_units=32\n",
        "\n",
        "# CREATE MODEL\n",
        "model = keras.Sequential()\n",
        "\n",
        "# ADD RECURRENT LAYER\n",
        "\n",
        "# #COMMENT/UNCOMMENT TO USE RNN, LSTM,GRU\n",
        "model.add(LSTM(\n",
        "#model.add(GRU(\n",
        "#model.add(SimpleRNN(\n",
        "units=recurrent_hidden_units,\n",
        "return_sequences=False,\n",
        "input_shape=input_shape, \n",
        "recurrent_regularizer=regularizers.L2(L2),\n",
        "# recurrent_dropout=0.8,\n",
        "activation='relu')\n",
        "          ) \n",
        "     \n",
        "# NEED TO TAKE THE OUTPUT RNN AND CONVERT TO SCALAR \n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# MODEL SUMMARY\n",
        "print(model.summary()); #print(x_train.shape,y_train.shape)\n",
        "# # print(\"initial parameters:\", model.get_weights())\n",
        "\n",
        "# # COMPILING THE MODEL \n",
        "opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "model.compile(optimizer=opt, loss=loss_function)\n",
        "\n",
        "# TRAINING YOUR MODEL\n",
        "history = model.fit(Xt,\n",
        "                    Yt,\n",
        "                    epochs=numbers_epochs,\n",
        "                    batch_size=batch_size, verbose=False,\n",
        "                    validation_data=(Xv, Yv))\n",
        "# History plot\n",
        "history_plot(history)\n",
        "\n",
        "# Predictions \n",
        "Ytp=model.predict(Xt)\n",
        "Yvp=model.predict(Xv) \n",
        "\n",
        "# REPORT\n",
        "regression_report(Yt,Ytp,Yv,Yvp) \n",
        "LSTM_e_r = sqrt(mean_squared_error(Yt, Ytp))"
      ],
      "id": "7312a9c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- According to the model results, `LSTM` model with regularization has larger `MSE` and `MAE` values for both traning and test sets. This makes sense since the concept of adding regularization to deep learning model is to add a penalty to the loss function, precenting the model to learn the data \"too well\", and enhance generalization.\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "- Comparison of `RMSE` of all three models"
      ],
      "id": "ce21d6ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "# Creating a DataFrame to hold the data\n",
        "data = {\n",
        "    'Model': ['RNN', 'RNN with Regularization', 'GRU', 'GRU with Regularization', 'LSTM', 'LSTM with Regularization'],\n",
        "    'RMSE': [RNN_e, RNN_e_r, GRU_e, GRU_e_r, LSTM_e, LSTM_e_r]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Set the palette to have different colors for each bar\n",
        "palette = sns.color_palette(\"hsv\", len(df))\n",
        "\n",
        "# Plotting using seaborn\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(y='RMSE', x='Model', data=df, palette=palette)\n",
        "plt.xlabel('RMSE')\n",
        "plt.ylabel('Model')\n",
        "plt.title('Comparison of RMSE for RNN, GRU and LSTM with and without regularization')\n",
        "plt.show()"
      ],
      "id": "e310970a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Based on the model diagnostics with `RMSE`, we can see that the all three deep learning model have similar accuracy, with `LSTM` model outperforms the other two with a slight lower `RMSE` on both with and without regularization. \n",
        "\n",
        "- Also, from the forcasting plots, we can see all three deep learning models can accurately forcast up to approximately 80 index, for my data set, this indicates accurate predictions up to `80` weeks of violent crimes.\n",
        "\n",
        "\n",
        "## Comparison with traditional TS models\n",
        "\n",
        "- In order to compare with the `SARIMA(0,2,2)(0,1,0)52`, we need to denormalize the `RMSE` result from deep learning models to the normal scale.\n",
        "\n",
        "![Forcast plots for SARIMA(0,2,2)(0,1,0)52](images/sarima.png)"
      ],
      "id": "dd7453ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv(\"./dataset/crimedata_week.csv\")\n",
        "data.drop('offense_type', axis = 1)\n",
        "std = np.std(data['Total_Crimes'])\n",
        "\n",
        "# Denormalizing the RMSE for the single time series\n",
        "denormalized_rmse = df['RMSE'] * std\n",
        "\n",
        "# Update the DataFrame with denormalized RMSE\n",
        "df['Denormalized RMSE'] = denormalized_rmse\n",
        "df"
      ],
      "id": "e058bfb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add a row for SARIMA model for plotting\n",
        "s_rmse = 67.19183\n",
        "sa = {'Model': 'SARIMA', 'RMSE': s_rmse, 'Denormalized RMSE': s_rmse}\n",
        "\n",
        "sa = pd.DataFrame([sa])\n",
        "\n",
        "# Concatenate the new row to the existing DataFrame\n",
        "df = pd.concat([df, sa], ignore_index=True)\n",
        "\n",
        "# Plotting using seaborn\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(y='Denormalized RMSE', x='Model', data=df, palette=palette)\n",
        "plt.xlabel('RMSE')\n",
        "plt.ylabel('Model')\n",
        "plt.title('Comparison of RMSE for RNN, GRU, LSTM with and without regularization and SARIMA model')\n",
        "plt.show()"
      ],
      "id": "8ade5085",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We can clearly see that the `RMSE` for `SARIMA(0,2,2)(0,1,0)52` is significantly higher than that of the deep learning modes, indicating that deep learning models have a higher accuracy than traditional time series model for my data set."
      ],
      "id": "077ef0ad"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}